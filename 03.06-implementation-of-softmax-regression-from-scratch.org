#+PROPERTY: header-args    :tangle src/clj_d2l/softmax_from_scratch.clj
* Implementation of Softmax Regression from Scratch

#+begin_src clojure :results silent
(ns clj-d2l.softmax-from-scratch
  (:require [clojure.java.io :as io]
            [clj-djl.ndarray :as nd]
            [clj-djl.device :as device]
            [clj-djl.engine :as engine]
            [clj-djl.training.dataset :as ds]
            [clj-djl.model :as model]
            [clj-djl.nn :as nn]
            [clj-djl.training.loss :as loss]
            [clj-djl.training.tracker :as tracker]
            [clj-djl.training.optimizer :as optimizer]
            [clj-djl.training :as training]
            [clj-djl.training.listener :as listener]
            [clj-d2l.core :as d2l])
  (:import [ai.djl.ndarray.types DataType]
           [ai.djl.basicdataset FashionMnist]
           [ai.djl.training.dataset Dataset$Usage]
           [java.nio.file Paths]))
#+end_src

#+begin_src emacs-lisp :tangle no
(setq org-babel-clojure-sync-nrepl-timeout 1000)
#+end_src

#+RESULTS:
: 1000

#+begin_src clojure :results output :exports both
(def batch-size 256)
(def random-shuffle true)

(def mnist-train (-> (FashionMnist/builder)
                     (ds/opt-usage Dataset$Usage/TRAIN)
                     (ds/set-sampling batch-size random-shuffle)
                     (ds/build)
                     (ds/prepare)))

(def mnist-test (-> (FashionMnist/builder)
                    (ds/opt-usage Dataset$Usage/TEST)
                    (ds/set-sampling batch-size random-shuffle)
                    (ds/build)
                    (ds/prepare)))

(println "train dataset size: "(nd/size mnist-train))
(println "test dataset size: " (nd/size mnist-test))
#+end_src

#+RESULTS:
: train dataset size:  60000
: test dataset size:  10000

** Initializing Model Parameters

#+begin_src clojure :results silent :exports both
(def num-inputs 784)
(def num-outputs 10)
(def manager (nd/base-manager))
(def W (nd/random-normal manager 0 0.01 [num-inputs num-outputs] :float32 (device/default-device)))
(def b (nd/zeros manager [num-outputs] :float32))
#+end_src

** The Softmax

#+begin_src clojure :results output :exports both
(def X (nd/create manager [[1 2 3] [4 5 6]]))
(d2l/ps (nd/sum X [0] true))
(d2l/ps (nd/sum X [1] true))
(d2l/ps (nd/sum X [0 1] true))
(d2l/ps (nd/sum X [0 1] false))
(d2l/ps (nd/sum X))
#+end_src

#+RESULTS:
#+begin_example
ND: (1, 3) cpu() int64
[[ 5,  7,  9],
]
ND: (2, 1) cpu() int64
[[ 6],
 [15],
]
ND: (1, 1) cpu() int64
[[21],
]
ND: () cpu() int64
21
ND: () cpu() int64
21
#+end_example


#+begin_src clojure :results output :exports both
(defn softmax [ndarray]
  (let [Xexp (nd/exp ndarray)
        partition (nd/sum Xexp [1] true)]
    (nd// Xexp partition)))

(def X (nd/random-normal manager [2 5]))
(d2l/ps (softmax X))
(d2l/ps (nd/sum (softmax X) [1]))
#+end_src

#+RESULTS:
: ND: (2, 5) cpu() float32
: [[0.3466, 0.1384, 0.3952, 0.0768, 0.043 ],
:  [0.2207, 0.1179, 0.022 , 0.2144, 0.4251],
: ]
: ND: (2) cpu() float32
: [1., 1.]

** The Model

#+begin_src clojure :results silent :exports both
(defn net [ndarray]
  (let [current-W W
        current-b b]
    (-> ndarray
        (nd/reshape [-1 num-inputs])
        (nd/dot current-W)
        (nd/+ current-b)
        softmax)))
#+end_src

** The Loss Function

#+begin_src clojure :results output :exports both
(def y-hat (nd/create manager [[0.1 0.3 0.6][0.3 0.2 0.5]]))
(d2l/ps (nd/get y-hat ":,{}" (nd/create manager [0 2])))
#+end_src

#+RESULTS:
: ND: (2, 1) cpu() float64
: [[0.1],
:  [0.5],
: ]

#+begin_src clojure :results output :exports both
(defn cross-entropy [y-hat y]
  (-> (nd/get y-hat ":, {}" (.toType y DataType/INT32 false))
      (.log)
      (.neg)))

(d2l/ps (cross-entropy y-hat (nd/create manager [0 2])))
#+end_src

#+RESULTS:
: ND: (2, 1) cpu() float64
: [[2.3026],
:  [0.6931],
: ]

** Classification Accuracy

#+begin_src clojure :results silent :exports both
(defn accuracy [y-hat y]
  (if (> (nd/size (nd/get-shape y-hat)) 1)
    (-> (.argMax y-hat 1)
        (nd/to-type :int64 false)
        (nd/= (nd/to-type y :int64 false))
        (nd/sum)
        (nd/to-type :float32 false)
        (nd/get-element))
    (-> (nd/= y-hat (nd/to-type y :int64 false))
        (nd/sum)
        (nd/to-type :float32 false)
        (nd/get-element))))

(defn evaluate-accuracy [net data-iter]
  (let [acc (atom [0 0])]
    (doseq [batch (training/iter-seq data-iter)]
      (let [X (nd/head (ds/get-batch-data batch))
            y (nd/head (ds/get-batch-labels batch))]
        (swap! acc update 0 + (accuracy (net X) y))
        (swap! acc update 1 + (nd/size y))
        (ds/close-batch batch)))
    (reduce / @acc)))
#+end_src


#+begin_src clojure :results value :exports both
(evaluate-accuracy net (ds/get-data mnist-test manager))
#+end_src

#+RESULTS:
: 0.0675


** Model Training

#+begin_src clojure :results silent :exports both
(defn accumulate [atom x y z]
  (swap! atom update 0 + x)
  (swap! atom update 1 + y)
  (swap! atom update 2 + z))

(defn sgd [params lr batch-size]
  (doseq [param params]
    (nd/-! param (nd// (nd/* (nd/get-gradient param) lr) batch-size))))
#+end_src



#+begin_src clojure :results silent :exports both
(defn train-epoch-ch3 [net train-iter lr loss updater]
  (let [acc (atom [0 0 0])]
    (doseq [param [W b]]
      (nd/attach-gradient param))
    (doseq [batch (training/iter-seq train-iter)]
      (let [X (-> batch ds/get-batch-data nd/head (nd/reshape [-1 num-inputs]))
            y (-> batch ds/get-batch-labels nd/head)
            ]
        (with-open [gc (-> (engine/get-instance) (engine/new-gradient-collector))]
          (let [y-hat (net X)
                l (loss y-hat y)]
            (training/backward gc l)
            (accumulate acc (nd/get-element (nd/sum l)) (accuracy y-hat y) (nd/size y)))))
      (sgd [W b] lr batch-size)
      (ds/close-batch batch))
    [(/ (@acc 0) (@acc 2)) (/ (@acc 1) (@acc 2))]))
#+end_src

#+begin_src clojure :results silent :exports both
(defn train-ch3 [net train-ds test-ds lr loss num-epochs updater]
  (doseq [i (range num-epochs)]
    (let [train-metrics (train-epoch-ch3 net (ds/get-data train-ds manager) lr loss updater)
          accuracy (evaluate-accuracy net (ds/get-data test-ds manager))
          train-accuracy (get train-metrics 1)
          train-loss (get train-metrics 0)]
      (println "Epoch " i ": Test Accuracy: " accuracy)
      (println "Train Accuracy: " train-accuracy)
      (println "Train Loss: "train-loss))))
#+end_src


#+begin_src clojure :results output :exports both
(def num-epochs 3)
(def lr 0.1)
(train-ch3 net mnist-train mnist-test lr cross-entropy num-epochs sgd)
#+end_src

#+RESULTS:
: Epoch  0 : Test Accuracy:  0.7923
: Train Accuracy:  0.74795
: Train Loss:  0.7867562699635824
: Epoch  1 : Test Accuracy:  0.8123
: Train Accuracy:  0.81455
: Train Loss:  0.5689848224639893
: Epoch  2 : Test Accuracy:  0.8187
: Train Accuracy:  0.8252166666666667
: Train Loss:  0.5255329947153727


** Prediction

#+begin_src clojure :results output :exports both
(defn predict-ch3 [net dataset ndmanager]
  (let [batch (.next (ds/get-data dataset ndmanager))
        X (nd/head (ds/get-batch-data batch))
        y-hat (nd/argmax (net X) 1)
        y (nd/head (ds/get-batch-labels batch))]
    [y-hat y]))

(def prediction (predict-ch3 net mnist-test manager))
(println "Prediction:   " (take 20 (nd/to-vec (prediction 0))))
(println "Actual label: "(take 20 (map int (nd/to-vec (prediction 1)))))
#+end_src

#+RESULTS:
: Prediction:    (3 8 0 9 2 3 4 0 3 3 0 7 9 6 8 1 6 2 3 2)
: Actual label:  (0 8 0 9 2 4 4 6 3 6 3 5 9 6 8 1 3 2 0 2)
