#+PROPERTY: header-args    :tangle src/clj_d2l/linreg.clj
* Linear Regression Implementation from Scratch

#+begin_src clojure :results silent
(ns clj-d2l.linreg
  (:require [clj-djl.ndarray :as nd]
            [clj-djl.device :as device]
            [clj-djl.engine :as engine]
            [clj-djl.training :as training]
            [clj-djl.training.dataset :as ds]
            [clj-d2l.core :as d2l])
  (:import [ai.djl.ndarray.types DataType]))
#+end_src

** Generating the Dataset

#+begin_src clojure :results output :exports both
(defn synthetic-data [ndm w b num]
  (let [X (nd/random-normal ndm [num (nd/size w)])
        y (nd/+ (nd/dot X w) b)
        noise (nd/random-normal ndm 0 0.01 (nd/get-shape y) DataType/FLOAT32)]
    [X (nd/+ y noise)]))

(def ndm (nd/new-base-manager))
(def true-w (nd/create ndm (float-array [2 -3.4])))
(def true-b 4.2)
(def dp (synthetic-data ndm true-w true-b 1000))
(def features (get dp 0))
(def labels (get dp 1))
(println "features(0): "(nd/to-vec (nd/get features [0])))
(println "labels(0): " (nd/get-element labels [0]))
#+end_src

#+RESULTS:
: features(0):  [0.2726572 0.046338152]
: labels(0):  4.5826263


Now we can show the data with plotly:

#+begin_src clojure :results silent :exports both
(let [x (nd/to-vec (nd/get features ":, 1"))
      y (nd/to-vec labels)]
  (d2l/plot-line
   "figure/synthetic_data.svg"
   "data"
   x
   y))
#+end_src

#+RESULTS:
[[./figure/synthetic_data.svg]]


** Reading the Dataset

#+begin_src clojure :results silent :exports both
(def batch-size 10)
(def dataset (-> (ds/new-array-dataset-builder)
                 (ds/set-data features)
                 (ds/opt-labels labels)
                 (ds/set-sampling batch-size false)
                 (ds/build)))
#+end_src

#+begin_src clojure :results output :exports both
(let [batch (.next (ds/get-data dataset ndm))
      X (-> (ds/get-batch-data batch)
            (nd/head))
      y (-> (ds/get-batch-labels batch)
            (nd/head))]
  (println (str X))
  (println (nd/to-vec (nd/+ (nd/dot X true-w) true-b)))
  (println (nd/to-vec y))
  (ds/close-batch batch))
#+end_src

#+RESULTS:
#+begin_example
ND: (10, 2) cpu() float32
[[ 0.2727,  0.0463],
 [ 0.8032,  1.3645],
 [ 0.0895,  0.6368],
 [-0.435 , -1.8171],
 [ 1.6563, -1.197 ],
 [ 0.5207, -1.0564],
 [ 0.1503,  1.0269],
 [ 0.5365,  0.6632],
 [-0.6452,  1.2165],
 [ 2.0424,  0.8562],
]

[4.5877643 1.1672001 2.213755 9.508151 11.582605 8.833304 1.0091412 3.0179806 -1.2264853 5.3737807]
[4.5826263 1.178563 2.2149844 9.506688 11.571146 8.845152 1.0214792 3.0197468 -1.2324206 5.3641534]
#+end_example

** Initializing Model Parameters

#+begin_src clojure :results output :exports both
(def w (nd/random-normal ndm 0 0.01 [2 1] DataType/FLOAT32 (device/default-device)))
(def b (nd/zeros ndm [1]))
(println (nd/to-vec w))
(println (nd/to-vec b))
#+end_src

#+RESULTS:
: [8.1814115E-4 -0.016517017]
: [0.0]

** Defining the Model

#+begin_src clojure :results silent :export both
(defn linreg [X w b]
  (nd/+ (nd/dot X w) b))
#+end_src

** Defining the Loss Function

#+begin_src clojure :results silent :export both
(defn squared-loss [y-hat y]
  (nd// (nd/* (nd/- y-hat (nd/reshape y (nd/get-shape y-hat)))
              (nd/- y-hat (nd/reshape y (nd/get-shape y-hat))))
        2))
#+end_src

** Defining the Optimization Algorithm

stochastic gradient descent (SGD):

#+begin_src clojure :results silent :export both
(defn sgd [params lr batch-size]
  (doseq [param params]
    ;; param = param - param.gradient * lr / batchSize
    (nd/-! param (nd// (nd/* (nd/get-gradient param) lr) batch-size))))
#+end_src

** Training

#+begin_src clojure :results output :exports both
(def lr 0.03)
(def epochs 3)

(map #(nd/attach-gradient %) [w b])

(doseq [epoch (range epochs)]
  (doseq [batch (training/iter-seq (ds/get-data dataset ndm))]
    (let [X (-> (ds/get-batch-data batch)
                (nd/head))
          y (-> (ds/get-batch-labels batch)
                (nd/head))]
      (with-open [gc (-> (engine/get-instance) (engine/new-gradient-collector))]
        (let [l (-> (linreg X w b) (squared-loss y))]
          (.backward gc l)))
      (sgd [w b] lr batch-size)
      (ds/close-batch batch)))
  (let [train-loss (squared-loss (linreg features w b) labels)]
    (println "epoch" (inc epoch) ", loss " (nd/get-element (.mean train-loss)))))
#+end_src

#+RESULTS:
: epoch 1 , loss  0.037740294
: epoch 2 , loss  1.4411E-4
: epoch 3 , loss  4.955114E-5

#+begin_src clojure :results output :exports both
(println (nd/to-vec w))
(println (nd/to-vec true-w))
(def w-error (nd/to-vec (nd/- true-w (nd/reshape w (nd/get-shape true-w)))))
(println "Error in estimating w:" (vec w-error))
(println "Error in estimating w:" (- true-b (nd/get-element b)))
#+end_src

#+RESULTS:
: [2.0000813 -3.3988967]
: [2.0 -3.4]
: Error in estimating w: [-8.1300735E-5 -0.0011034012]
: Error in estimating w: 6.200790405275214E-4
