* Linear Regression Implementation from Scratch

#+begin_src clojure :results silent
(ns clj-d2l.linreg
  (:require [clj-djl.ndarray :as nd]
            [clj-djl.device :as device]
            [clj-djl.engine :as engine]
            [clj-djl.training :as training]
            [clj-djl.training.dataset :as ds])
  (:import [ai.djl.ndarray.types DataType]
           [tech.tablesaw.api Table FloatColumn]
           [tech.tablesaw.plotly.api ScatterPlot]
           [tech.tablesaw.plotly Plot]))
#+end_src

** Generating the Dataset

#+begin_src clojure :results output :exports both
(defn synthetic-data [ndm w b num]
  (let [X (nd/random-normal ndm [num (nd/size w)])
        y (nd/+ (nd/dot X w) b)
        noise (nd/random-normal ndm 0 0.01 (nd/get-shape y) DataType/FLOAT32)]
    [X (nd/+ y noise)]))

(def ndm (nd/new-base-manager))
(def true-w (nd/create ndm (float-array [2 -3.4])))
(def true-b 4.2)
(def dp (synthetic-data ndm true-w true-b 1000))
(def features (get dp 0))
(def labels (get dp 1))
(println "features(0): "(nd/to-vec (nd/get features [0])))
(println "labels(0): " (nd/get-element labels [0]))
#+end_src

#+RESULTS:
: features(0):  [0.40681973 -1.4101993]
: labels(0):  9.818751


Now we can show the data with plotly:

#+begin_src clojure :results silent
(def X (nd/to-array (nd/get features ":,1")))
(def y (nd/to-array labels))
(def data (-> (Table/create "data")
              (.addColumns (into-array
                            [(FloatColumn/create "X" X)
                             (FloatColumn/create "y" y)]))))
(Plot/show (ScatterPlot/create "Synthetic Data" data "X" "y"))
#+end_src

** Reading the Dataset

#+begin_src clojure :results silent :exports both
(def batch-size 10)
(def dataset (-> (ds/new-array-dataset-builder)
                 (ds/set-data features)
                 (ds/opt-labels labels)
                 (ds/set-sampling batch-size false)
                 (ds/build)))
#+end_src

#+begin_src clojure :results output :exports both
(let [batch (.next (ds/get-data dataset ndm))
      X (-> (ds/get-batch-data batch)
            (nd/head))
      y (-> (ds/get-batch-labels batch)
            (nd/head))]
  (println (str X))
  (println (nd/to-vec (nd/+ (nd/dot X true-w) true-b)))
  (println (nd/to-vec y))
  (ds/close-batch batch))
#+end_src

#+RESULTS:
#+begin_example
ND: (10, 2) cpu() float32
[[ 0.4068, -1.4102],
 [ 0.329 ,  0.9445],
 [ 1.2594, -1.0093],
 [ 1.6118, -0.34  ],
 [ 1.5747, -1.4185],
 [ 0.3785,  1.8297],
 [-0.7906,  0.0745],
 [ 0.8816,  1.7102],
 [-1.543 , -0.6171],
 [-0.4345, -1.2346],
]

[9.808317 1.6467907 10.150299 8.579533 12.172226 -1.263792 2.365392 0.14853334 3.2121205 7.5286875]
[9.818751 1.6490054 10.1547785 8.577532 12.174219 -1.258281 2.3434823 0.15108518 3.1960368 7.5158978]
#+end_example

** Initializing Model Parameters

#+begin_src clojure :results output :exports both
(def w (nd/random-normal ndm 0 0.01 [2 1] DataType/FLOAT32 (device/default-device)))
(def b (nd/zeros ndm [1]))
(println (nd/to-vec w))
(println (nd/to-vec b))
#+end_src

#+RESULTS:
: [-0.017967535 -0.015705079]
: [0.0]

** Defining the Model

#+begin_src clojure :results silent :export both
(defn linreg [X w b]
  (nd/+ (nd/dot X w) b))
#+end_src

** Defining the Loss Function

#+begin_src clojure :results silent :export both
(defn squared-loss [y-hat y]
  (nd// (nd/* (nd/- y-hat (nd/reshape y (nd/get-shape y-hat)))
              (nd/- y-hat (nd/reshape y (nd/get-shape y-hat))))
        2))
#+end_src

** Defining the Optimization Algorithm

stochastic gradient descent (SGD):

#+begin_src clojure :results silent :export both
(defn sgd [params lr batch-size]
  (doseq [param params]
    ;; param = param - param.gradient * lr / batchSize
    (nd/-! param (nd// (nd/* (nd/get-gradient param) lr) batch-size))))
#+end_src

** Training

#+begin_src clojure :results output :exports both
(def lr 0.03)
(def epochs 3)

(map #(nd/attach-gradient %) [w b])

(doseq [epoch (range epochs)]
  (doseq [batch (training/iter-seq (ds/get-data dataset ndm))]
    (let [X (-> (ds/get-batch-data batch)
                (nd/head))
          y (-> (ds/get-batch-labels batch)
                (nd/head))]
      (with-open [gc (-> (engine/get-instance) (engine/new-gradient-collector))]
        (let [l (-> (linreg X w b) (squared-loss y))]
          (.backward gc l)))
      (sgd [w b] lr batch-size)
      (ds/close-batch batch)))
  (let [train-loss (squared-loss (linreg features w b) labels)]
    (println "epoch" (inc epoch) ", loss " (nd/get-element (.mean train-loss)))))
#+end_src

#+RESULTS:
: epoch 1 , loss  0.036502816
: epoch 2 , loss  1.3316181E-4
: epoch 3 , loss  5.017259E-5

#+begin_src clojure :results output :exports both
(println (nd/to-vec w))
(println (nd/to-vec true-w))
(def w-error (nd/to-vec (nd/- true-w (nd/reshape w (nd/get-shape true-w)))))
(println "Error in estimating w:" (vec w-error))
(println "Error in estimating w:" (- true-b (nd/get-element b)))
#+end_src

#+RESULTS:
: [2.0001454 -3.3996136]
: [2.0 -3.4]
: Error in estimating w: [-1.4543533E-4 -3.8647652E-4]
: Error in estimating w: 4.560470581056464E-4
