* Linear Regression Implementation from Scratch

#+begin_src clojure :results silent
(ns clj-d2l.linreg
  (:require [clj-djl.ndarray :as nd]
            [clj-djl.device :as device]
            [clj-djl.engine :as engine]
            [clj-djl.training.dataset :as ds])
  (:import [ai.djl.ndarray.types DataType]
           [tech.tablesaw.api Table FloatColumn]
           [tech.tablesaw.plotly.api ScatterPlot]
           [tech.tablesaw.plotly Plot]))
#+end_src

** Generating the Dataset

#+begin_src clojure :results output :exports both
(defn synthetic-data [ndm w b num]
  (let [X (nd/random-normal ndm [num (nd/size w)])
        y (nd/+ (nd/dot X w) b)
        noise (nd/random-normal ndm 0 0.01 (nd/get-shape y) DataType/FLOAT32)]
    [X (nd/+ y noise)]))

(def ndm (nd/new-base-manager))
(def true-w (nd/create ndm (float-array [2 -3.4])))
(def true-b 4.2)
(def dp (synthetic-data ndm true-w true-b 1000))
(def features (get dp 0))
(def labels (get dp 1))
(println "features(0): "(nd/to-vec (nd/get features [0])))
(println "labels(0): " (nd/get-element labels [0]))
#+end_src

#+RESULTS:
: features(0):  [-0.92293715 0.38278535]
: labels(0):  1.0584149


Now we can show the data with plotly:

#+begin_src clojure :results silent
(def X (nd/to-array (nd/get features ":,1")))
(def y (nd/to-array labels))
(def data (-> (Table/create "data")
              (.addColumns (into-array
                            [(FloatColumn/create "X" X)
                             (FloatColumn/create "y" y)]))))
(Plot/show (ScatterPlot/create "Synthetic Data" data "X" "y"))
#+end_src

** Reading the Dataset

#+begin_src clojure :results silent :exports both
(def batch-size 10)
(def dataset (-> (ds/new-array-dataset-builder)
                 (ds/set-data features)
                 (ds/opt-labels labels)
                 (ds/set-sampling batch-size false)
                 (ds/build)))
#+end_src

#+begin_src clojure :results output :exports both
(let [batch (.next (ds/get-data dataset ndm))
      X (-> (ds/get-batch-data batch)
            (nd/head))
      y (-> (ds/get-batch-labels batch)
            (nd/head))]
  (println (str X))
  (println (nd/to-vec (nd/+ (nd/dot X true-w) true-b)))
  (println (nd/to-vec y))
  (ds/close-batch batch))
#+end_src

#+RESULTS:
#+begin_example
ND: (10, 2) cpu() float32
[[-0.9229,  0.3828],
 [ 0.6635, -0.0755],
 [-0.1635, -0.7772],
 [ 0.1268,  0.3709],
 [-0.0902,  1.5387],
 [-0.3424, -2.0866],
 [-1.0622, -0.4591],
 [ 1.8084, -0.2429],
 [ 1.9257,  0.3537],
 [-1.608 ,  0.8233],
]

[1.0526552 5.783718 6.5155067 3.192329 -1.2117848 10.609713 3.6366546 8.642559 6.8487735 -1.8151007]
[1.0584149 5.790399 6.5163217 3.1864026 -1.2001294 10.603197 3.6390767 8.655985 6.8404655 -1.8164747]
#+end_example

** Initializing Model Parameters

#+begin_src clojure :results output :exports both
(def w (nd/random-normal ndm 0 0.01 [2 1] DataType/FLOAT32 (device/default-device)))
(def b (nd/zeros ndm [1]))
(println (nd/to-vec w))
(println (nd/to-vec b))
#+end_src

#+RESULTS:
: [-0.008550827 -0.004881284]
: [0.0]

** Defining the Model

#+begin_src clojure :results silent :export both
(defn linreg [X w b]
  (nd/+ (nd/dot X w) b))
#+end_src

** Defining the Loss Function

#+begin_src clojure :results silent :export both
(defn squared-loss [y-hat y]
  (nd// (nd/* (nd/- y-hat (nd/reshape y (nd/get-shape y-hat)))
              (nd/- y-hat (nd/reshape y (nd/get-shape y-hat))))
        2))
#+end_src

** Defining the Optimization Algorithm

stochastic gradient descent (SGD):

#+begin_src clojure :results silent :export both
(defn sgd [params lr batch-size]
  (doseq [param params]
    ;; param = param - param.gradient * lr / batchSize
    (nd/-! param (nd// (nd/* (nd/get-gradient param) lr) batch-size))))
#+end_src

** Training

#+begin_src clojure :results output :exports both
(def lr 0.03)
(def epochs 3)
(def i (atom 0))

(doseq [param [w b]]
  (nd/attach-gradient param))

(doseq [epoch (range epochs)]
  (doseq [batch (iterator-seq (ds/get-data dataset ndm))]
    (let [X (-> (ds/get-batch-data batch)
                (nd/head))
          y (-> (ds/get-batch-labels batch)
                (nd/head))]
      (swap! i inc)
      #_(print @i " ")
      (if (< @i 95)
        (do
          (with-open [gc (-> (engine/get-instance) (engine/new-gradient-collector))]
            (let [l (-> (linreg X w b) (squared-loss y))]
              (.backward gc l)))
          (sgd [w b] lr batch-size)
          (ds/close-batch batch)))))
  (let [train-loss (squared-loss (linreg features w b) labels)]
    (println "epoch" (inc epoch) ", loss " (nd/get-element (.mean train-loss)))))
#+end_src

#+RESULTS:
: epoch 1 , loss  0.04106543
: epoch 2 , loss  0.04106543
: epoch 3 , loss  0.04106543

#+begin_src clojure :results output :exports both
(println (nd/to-vec w))
(println (nd/to-vec true-w))
(def w-error (nd/to-vec (nd/- true-w (nd/reshape w (nd/get-shape true-w)))))
(println "Error in estimating w:" (vec w-error))
(println "Error in estimating w:" (- true-b (nd/get-element b)))
#+end_src

#+RESULTS:
: [1.8740603 -3.273324]
: [2.0 -3.4]
: Error in estimating w: [0.12593973 -0.12667608]
: Error in estimating w: 0.21756167411804217
