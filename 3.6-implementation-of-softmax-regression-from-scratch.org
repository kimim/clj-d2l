#+PROPERTY: header-args    :tangle src/clj_d2l/softmax_from_scratch.clj
* Implementation of Softmax Regression from Scratch

#+begin_src clojure :results silent
(ns clj-d2l.softmax-from-scratch
  (:require [clojure.java.io :as io]
            [clj-djl.ndarray :as nd]
            [clj-djl.device :as device]
            [clj-djl.engine :as engine]
            [clj-djl.training.dataset :as ds]
            [clj-djl.model :as model]
            [clj-djl.nn :as nn]
            [clj-djl.training.loss :as loss]
            [clj-djl.training.tracker :as tracker]
            [clj-djl.training.optimizer :as optimizer]
            [clj-djl.training :as training]
            [clj-djl.training.listener :as listener]
            [clj-djl.metric :as metric])
  (:import [ai.djl.ndarray.types DataType]
           [ai.djl.basicdataset FashionMnist]
           [ai.djl.training.dataset Dataset$Usage]
           [java.nio.file Paths]))
#+end_src

#+begin_src emacs-lisp
(setq org-babel-clojure-sync-nrepl-timeout 1000)
#+end_src

#+RESULTS:
: 1000

#+begin_src clojure :results output :exports both
(def batch-size 256)
(def random-shuffle true)

(def mnist-train (-> (FashionMnist/builder)
                     (ds/opt-usage Dataset$Usage/TRAIN)
                     (ds/set-sampling batch-size random-shuffle)
                     (ds/build)
                     (ds/prepare)))

(def mnist-test (-> (FashionMnist/builder)
                    (ds/opt-usage Dataset$Usage/TEST)
                    (ds/set-sampling batch-size random-shuffle)
                    (ds/build)
                    (ds/prepare)))

(println "train dataset size: "(.size mnist-train))
(println "test dataset size: " (.size mnist-test))
#+end_src

#+RESULTS:
: train dataset size:  60000
: test dataset size:  10000

** Initializing Model Parameters

#+begin_src clojure :results silent :exports both
(def num-inputs 784)
(def num-outputs 10)
(def manager (nd/new-base-manager))
(def W (nd/random-normal manager 0 0.01 [num-inputs num-outputs] DataType/FLOAT32 (device/default-device)))
(def b (nd/zeros manager [num-outputs] :float32))
#+end_src

** The Softmax

#+begin_src clojure :results output :exports both
(def X (nd/create manager [[1 2 3] [4 5 6]]))
(nd/pp (nd/sum X [0] true))
(nd/pp (nd/sum X [1] true))
(nd/pp (nd/sum X [0 1] true))
(nd/pp (nd/sum X [0 1] false))
(nd/pp (nd/sum X))
#+end_src

#+RESULTS:
#+begin_example
ND: (1, 3) cpu() int64
[[ 5,  7,  9],
]

ND: (2, 1) cpu() int64
[[ 6],
 [15],
]

ND: (1, 1) cpu() int64
[[21],
]

ND: () cpu() int64
21

ND: () cpu() int64
21

#+end_example


#+begin_src clojure :results output :exports both
(defn softmax [ndarray]
  (let [Xexp (nd/exp ndarray)
        partition (nd/sum Xexp [1] true)]
    (nd// Xexp partition)))

(def X (nd/random-normal manager [2 5]))
(nd/pp (softmax X))
(nd/pp (nd/sum (softmax X) [1]))
#+end_src

#+RESULTS:
: ND: (2, 5) cpu() float32
: [[0.0751, 0.2678, 0.195 , 0.3406, 0.1216],
:  [0.0554, 0.4219, 0.0528, 0.1088, 0.3611],
: ]
:
: ND: (2) cpu() float32
: [1.    , 1.    ]
:

** The Model

#+begin_src clojure :results silent :exports both
(defn net [ndarray]
  (let [current-W W
        current-b b]
    (-> ndarray
        (nd/reshape [-1 num-inputs])
        (nd/dot current-W)
        (nd/+ current-b)
        softmax)))
#+end_src

** The Loss Function

#+begin_src clojure :results output :exports both
(def y-hat (nd/create manager [[0.1 0.3 0.6][0.3 0.2 0.5]]))
(nd/pp (nd/get y-hat ":,{}" (nd/create manager [0 2])))
#+end_src

#+RESULTS:
: ND: (2, 1) cpu() float64
: [[0.1],
:  [0.5],
: ]
:

#+begin_src clojure :results output :exports both
(defn cross-entropy [y-hat y]
  (-> (nd/get y-hat ":, {}" (.toType y DataType/INT32 false))
      (.log)
      (.neg)))

(nd/pp (cross-entropy y-hat (nd/create manager [0 2])))
#+end_src

#+RESULTS:
: ND: (2, 1) cpu() float64
: [[2.3026],
:  [0.6931],
: ]
:

** Classification Accuracy

#+begin_src clojure :results silent :exports both
(defn accuracy [y-hat y]
  (if (> (nd/size (nd/get-shape y-hat)) 1)
    (-> (.argMax y-hat 1)
        (nd/to-type :int64 false)
        (nd/= (nd/to-type y :int64 false))
        (nd/sum)
        (nd/to-type :float32 false)
        (nd/get-element))
    (-> (nd/= y-hat (nd/to-type y :int64 false))
        (nd/sum)
        (nd/to-type :float32 false)
        (nd/get-element))))

(defn evaluate-accuracy [net data-iter]
  (let [acc (atom [0 0])]
    (doseq [batch (training/iter-seq data-iter)]
      (let [X (nd/head (.getData batch))
            y (nd/head (.getLabels batch))]
        (swap! acc update 0 + (accuracy (net X) y))
        (swap! acc update 1 + (nd/size y))
        (.close batch)))
    (reduce / @acc)))
#+end_src


#+begin_src clojure :results value :exports both
(evaluate-accuracy net (.getData mnist-test manager))
#+end_src

#+RESULTS:
: 0.0524


** Model Training

#+begin_src clojure :results silent :exports both
(defn accumulate [atom x y z]
  (swap! atom update 0 + x)
  (swap! atom update 1 + y)
  (swap! atom update 2 + z))

(defn sgd [params lr batch-size]
  (doseq [param params]
    (nd/-! param (nd// (nd/* (nd/get-gradient param) lr) batch-size))))
#+end_src



#+begin_src clojure :results silent :exports both
(defn train-epoch-ch3 [net train-iter lr loss updater]
  (let [acc (atom [0 0 0])]
    (doseq [param [W b]]
      (nd/attach-gradient param))
    (doseq [batch (training/iter-seq train-iter)]
      (let [X (-> batch ds/get-batch-data nd/head (nd/reshape [-1 num-inputs]))
            y (-> batch ds/get-batch-labels nd/head)
            ]
        (with-open [gc (-> (engine/get-instance) (engine/new-gradient-collector))]
          (let [y-hat (net X)
                l (loss y-hat y)]
            (.backward gc l)
            (accumulate acc (nd/get-element (nd/sum l)) (accuracy y-hat y) (nd/size y)))))
      (sgd [W b] lr batch-size)
      (.close batch))
    [(/ (@acc 0) (@acc 2)) (/ (@acc 1) (@acc 2))]))
#+end_src

#+begin_src clojure :results silent :exports both
(defn train-ch3 [net train-ds test-ds lr loss num-epochs updater]
  (doseq [i (range num-epochs)]
    (let [train-metrics (train-epoch-ch3 net (.getData train-ds manager) lr loss updater)
          accuracy (evaluate-accuracy net (.getData test-ds manager))
          train-accuracy (get train-metrics 1)
          train-loss (get train-metrics 0)]
      (println "Epoch " i ": Test Accuracy: " accuracy)
      (println "Train Accuracy: " train-accuracy)
      (println "Train Loss: "train-loss))))
#+end_src


#+begin_src clojure :results output :exports both
(def num-epochs 3)
(def lr 0.1)
(train-ch3 net mnist-train mnist-test lr cross-entropy num-epochs sgd)
#+end_src

#+RESULTS:
: Epoch  0 : Test Accuracy:  0.8213
: Train Accuracy:  0.8322333333333334
: Train Loss:  0.5015458701451619
: Epoch  1 : Test Accuracy:  0.8249
: Train Accuracy:  0.83705
: Train Loss:  0.48598715953826904
: Epoch  2 : Test Accuracy:  0.8276
: Train Accuracy:  0.8399166666666666
: Train Loss:  0.47418531277974446


** Prediction

#+begin_src clojure :results output :exports both
(defn predict-ch3 [net dataset ndmanager]
  (let [batch (.next (.getData dataset ndmanager))
        X (nd/head (ds/get-batch-data batch))
        y-hat (.argMax (net X) 1)
        y (nd/head (ds/get-batch-labels batch))]
    [y-hat y]))

(def prediction (predict-ch3 net mnist-test manager))
(println "Prediction:   " (take 20 (nd/to-vec (prediction 0))))
(println "Actual label: "(take 20 (map int (nd/to-vec (prediction 1)))))
#+end_src

#+RESULTS:
: Prediction:    (0 3 1 9 0 3 8 4 8 6 4 5 9 3 3 5 3 9 3 1)
: Actual label:  (6 3 1 9 0 3 8 2 8 4 4 5 9 3 3 5 4 9 3 1)
