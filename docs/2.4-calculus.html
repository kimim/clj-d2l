<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-05-17 Tue 07:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Kimi Ma" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
</style>
<link rel="stylesheet" type="text/css" href="css/style.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="sitemap.html"> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org1b17b13">1. Calculus</a>
<ul>
<li><a href="#org5e355b9">1.1. Derivatives and Differentiation</a></li>
<li><a href="#org7cb7548">1.2. Partial Derivatives</a></li>
<li><a href="#org1f0b96e">1.3. Gradients</a></li>
<li><a href="#org9ca883f">1.4. Chain Rule</a></li>
<li><a href="#orgd30255e">1.5. Summary</a></li>
<li><a href="#orgb9e854d">1.6. Exercises</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org1b17b13" class="outline-2">
<h2 id="org1b17b13"><span class="section-number-2">1.</span> Calculus</h2>
<div class="outline-text-2" id="text-1">
<p>
Finding the area of a polygon had remained mysterious until at least
2,500 years ago, when ancient Greeks divided a polygon into triangles
and summed their areas. To find the area of curved shapes, such as a
circle, ancient Greeks inscribed polygons in such shapes. As shown in
Section 2.4, an inscribed polygon with more sides of equal length
better approximates the circle. This process is also known as the
method of exhaustion.
</p>

<p>
In fact, the method of exhaustion is where <b>integral calculus</b> (will be
described in sec<sub>integral</sub><sub>calculus</sub>) originates from. More than 2,000
years later, the other branch of calculus, <b>differential calculus</b>, was
invented. Among the most critical applications of differential
calculus, optimization problems consider how to do something the
best. As discussed in Section 2.3.10.1, such problems are ubiquitous
in deep learning.
</p>

<p>
In deep learning, we train models, updating them successively so that
they get better and better as they see more and more data. Usually,
getting better means minimizing a <b>loss function</b>, a score that answers
the question &ldquo;how bad is our model?&rdquo; This question is more subtle than
it appears. Ultimately, what we really care about is producing a model
that performs well on data that we have never seen before. But we can
only fit the model to data that we can actually see. Thus we can
decompose the task of fitting models into two key concerns: i)
<b>optimization</b>: the process of fitting our models to observed data; ii)
<b>generalization</b>: the mathematical principles and practitioners&rsquo; wisdom
that guide as to how to produce models whose validity extends beyond
the exact set of data examples used to train them.
</p>

<p>
To help you understand optimization problems and methods in later
chapters, here we give a very brief primer on differential calculus
that is commonly used in deep learning.
</p>
</div>

<div id="outline-container-org5e355b9" class="outline-3">
<h3 id="org5e355b9"><span class="section-number-3">1.1.</span> Derivatives and Differentiation</h3>
<div class="outline-text-3" id="text-1-1">
<p>
We begin by addressing the calculation of derivatives, a crucial step
in nearly all deep learning optimization algorithms. In deep learning,
we typically choose loss functions that are differentiable with
respect to our model&rsquo;s parameters. Put simply, this means that for
each parameter, we can determine how rapidly the loss would increase
or decrease, were we to increase or decrease that parameter by an
infinitesimally small amount.
</p>

<p>
Suppose that we have a function \(f: \mathbb{R} \rightarrow
\mathbb{R}\), whose input and output are both scalars. The derivative
of \(f\) is defined as
</p>

\begin{equation}
\label{org56761fc}
f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h},
\end{equation}

<p>
if this limit exists. If \(f'(a)\) exists, \(f\) is said to be
<b>differentiable</b> at \(a\). If \(f\) is differentiable at every number of an
interval, then this function is differentiable on this interval. We
can interpret the derivative :\(f'(x)\) in \eqref{org56761fc} as the
<b>instantaneous</b> rate of change of \(f(x)\) with respect to \(x\). The
so-called instantaneous rate of change is based on the variation \(h\)
in \(x\), which approaches \(0\).
</p>

<p>
To illustrate derivatives, let us experiment with an example. Define
\(u = f(x) = 3x^2-4x\).
</p>

<p>
*Note: We will be using Double in this section to avoid incorrect
results since Double provides more decimal precision. Generally though,
we would use Float as deep learning frameworks by default use Fault.*
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.calculus</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.chart <span style="color: #110099;">:as</span> chart<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.plot <span style="color: #110099;">:as</span> plot<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">f</span> <span style="color: #7388d6;">[</span>x<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span>- <span style="color: #909183;">(</span>* 3 <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow x 2<span style="color: #709870;">)</span><span style="color: #909183;">)</span> <span style="color: #909183;">(</span>* 4 x<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
By setting \(x=1\) and letting \(h\) approach \(0\), the numerical
result of \(\frac{f(x+h) - f(x)}{h}\) in \eqref{org56761fc} approaches
\(2\). Though this experiment is not a mathematical proof, we will see
later that the derivative \(u'\) is \(2\) when \(x=1\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">numerical-lim</span> <span style="color: #7388d6;">[</span>f x h<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span>/ <span style="color: #909183;">(</span>- <span style="color: #709870;">(</span>f <span style="color: #907373;">(</span>+ x h<span style="color: #907373;">)</span><span style="color: #709870;">)</span> <span style="color: #709870;">(</span>f x<span style="color: #709870;">)</span><span style="color: #909183;">)</span> h<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;&gt;</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #909183;">(</span>/ 0.1 <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow 10 <span style="color: #000000;">%</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span> <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 5<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">fn</span> <span style="color: #709870;">[</span>h<span style="color: #709870;">]</span> <span style="color: #709870;">[</span>h <span style="color: #907373;">(</span>numerical-lim f 1 h<span style="color: #907373;">)</span><span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"h = "</span> <span style="color: #709870;">(</span><span style="color: #000000;">%</span> 0<span style="color: #709870;">)</span> <span style="color: #2A00FF;">", numerical limit = "</span> <span style="color: #709870;">(</span><span style="color: #000000;">%</span> 1<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">dorun</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
h =  0.1 , numerical limit =  2.3000000000000043
h =  0.01 , numerical limit =  2.0299999999999763
h =  0.001 , numerical limit =  2.002999999999311
h =  1.0E-4 , numerical limit =  2.0002999999979565
h =  1.0E-5 , numerical limit =  2.0000300000155846
</pre>


<p>
Let us familiarize ourselves with a few equivalent notations for
derivatives. Given \(y = f(x)\), where \(x\) and \(y\) are the
independent variable and the dependent variable of the function \(f\),
respectively. The following expressions are equivalent:
</p>

\begin{equation}
\label{org49191ec}
f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),
\end{equation}

<p>
where symbols \(\frac{d}{dx}\) and \(D\) are <b>differentiation operators</b>
that indicate operation of <b>differentiation</b>. We can use the following
rules to differentiate common functions:
</p>

<ul class="org-ul">
<li>\(DC = 0\) (\(C\) is a constant),</li>
<li>\(Dx^n = nx^{n-1}\) (the <b>power rule</b>, \(n\) is any real
number),</li>
<li>\(De^x = e^x\),</li>
<li>\(D\ln(x) = 1/x.\)</li>
</ul>

<p>
To differentiate a function that is formed from a few simpler
functions such as the above common functions, the following rules can
be handy for us. Suppose that functions \(f\) and \(g\) are both
differentiable and \(C\) is a constant, we have the <b>constant multiple
rule</b>
</p>

\begin{equation}
\label{orgae21e0c}
\frac{d}{dx} [Cf(x)] = C \frac{d}{dx} f(x),
\end{equation}

<p>
the <b>sum rule</b>
</p>

\begin{equation}
\label{orgc0ac61b}
\frac{d}{dx} [f(x) + g(x)] = \frac{d}{dx} f(x) + \frac{d}{dx} g(x),
\end{equation}

<p>
and the <b>quotient rule</b>
</p>

\begin{equation}
\label{org7a0e791}
\frac{d}{dx} \left[\frac{f(x)}{g(x)}\right] = \frac{g(x) \frac{d}{dx} [f(x)] - f(x) \frac{d}{dx} [g(x)]}{[g(x)]^2}.
\end{equation}

<p>
Now we can apply a few of the above rules to find
\(u' = f'(x) = 3 \frac{d}{dx} x^2-4\frac{d}{dx}x = 6x-4\). Thus, by
setting \(x = 1\), we have \(u' = 2\): this is supported by our
earlier experiment in this section where the numerical result approaches
\(2\). This derivative is also the slope of the tangent line to the
curve \(u = f(x)\) when \(x = 1\).
</p>

<p>
To visualize such an interpretation of derivatives, we will use <code>xchart</code>.
a simple plotting library.
</p>

<p>
We define <code>plot-lines</code> which will take as input three arrays.  The first
array will be the data in the x axis and the next two arrays will
contain the two functions that we want to plot in the y axis. In
addition to this data, the function requires us to specify the name of
the two lines we will be plotting, the label of both axes, and the
width and height of the figure. This function or a modified version of
it, will allow us to plot multiple curves succinctly since we will
need to visualize many curves throughout the book.
</p>

<p>
Now we can plot the function \(u = f(x)\) and its tangent line \(y =
2x - 3\) at \(x=1\), where the coefficient \(2\) is the slope of the
tangent line.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 3 1/32<span style="color: #909183;">)</span>
      y1 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> f x<span style="color: #909183;">)</span>
      y2 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span>- <span style="color: #907373;">(</span>* 2 <span style="color: #000000;">%</span><span style="color: #907373;">)</span> 3<span style="color: #709870;">)</span> x<span style="color: #909183;">)</span>
      chart <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">chart</span>/line <span style="color: #709870;">{</span><span style="color: #110099;">:title</span> <span style="color: #2A00FF;">"tangent line (x=1)"</span>
                         <span style="color: #110099;">:series</span> <span style="color: #907373;">[</span><span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y1"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y1<span style="color: #6276ba;">}</span>
                                  <span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y2"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y2<span style="color: #6276ba;">}</span><span style="color: #907373;">]</span><span style="color: #709870;">}</span><span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">plot</span>/store! chart <span style="color: #110099;">nil</span> <span style="color: #2A00FF;">"notes/figures/tangent_line.svg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="org6ad5d06" class="figure">
<p><img src="figures/tangent_line.svg" alt="tangent_line.svg" class="org-svg" />
</p>
</div>
</div>
</div>

<div id="outline-container-org7cb7548" class="outline-3">
<h3 id="org7cb7548"><span class="section-number-3">1.2.</span> Partial Derivatives</h3>
<div class="outline-text-3" id="text-1-2">
<p>
So far we have dealt with the differentiation of functions of just one
variable. In deep learning, functions often depend on <b>many</b> variables.
Thus, we need to extend the ideas of differentiation to these
<b>multivariate</b> functions.
</p>

<p>
Let \(y = f(x_1, x_2, \ldots, x_n)\) be a function with \(n\)
variables. The <b>partial derivative</b> of \(y\) with respect to its
\(i^\mathrm{th}\) parameter \(x_i\) is
</p>

\begin{equation}
\label{org4cbc9e4}
\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.
\end{equation}


<p>
To calculate \(\frac{\partial y}{\partial x_i}\), we can simply treat
\(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n\) as constants and
calculate the derivative of \(y\) with respect to \(x_i\).  For
notation of partial derivatives, the following are equivalent:
</p>

\begin{equation}
\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.
\end{equation}
</div>
</div>

<div id="outline-container-org1f0b96e" class="outline-3">
<h3 id="org1f0b96e"><span class="section-number-3">1.3.</span> Gradients</h3>
<div class="outline-text-3" id="text-1-3">
<p>
We can concatenate partial derivatives of a multivariate function with
respect to all its variables to obtain the <b>gradient</b> vector of the
function. Suppose that the input of function \(f: \mathbb{R}^n
\rightarrow \mathbb{R}\) is an \(n\)-dimensional vector \(\mathbf{x} =
[x_1, x_2, \ldots, x_n]^\top\) and the output is a scalar. The
gradient of the function \(f(\mathbf{x})\) with respect to
\(\mathbf{x}\) is a vector of \(n\) partial derivatives:
</p>

\begin{equation}
\label{orga02fab8}
\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top,
\end{equation}

<p>
where \(\nabla_{\mathbf{x}} f(\mathbf{x})\) is often replaced by
\(\nabla f(\mathbf{x})\) when there is no ambiguity.
</p>

<p>
Let \(\mathbf{x}\) be an \(n\)-dimensional vector, the following rules
are often used when differentiating multivariate functions:
</p>

<ul class="org-ul">
<li>For all \(\mathbf{A} \in \mathbb{R}^{m \times n}\),
\(\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top\),</li>
<li>For all \(\mathbf{A} \in \mathbb{R}^{n \times m}\),
\(\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}\),</li>
<li>For all \(\mathbf{A} \in \mathbb{R}^{n \times n}\),
\(\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}\),</li>
<li>\(\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}\).</li>
</ul>

<p>
Similarly, for any matrix \(\mathbf{X}\), we have
\(\nabla_{\mathbf{X}} \|\mathbf{X} \|_F^2 = 2\mathbf{X}\). As we will
see later, gradients are useful for designing optimization algorithms
in deep learning.
</p>
</div>
</div>

<div id="outline-container-org9ca883f" class="outline-3">
<h3 id="org9ca883f"><span class="section-number-3">1.4.</span> Chain Rule</h3>
<div class="outline-text-3" id="text-1-4">
<p>
However, such gradients can be hard to find. This is because
multivariate functions in deep learning are often <b>composite</b>, so we may
not apply any of the aforementioned rules to differentiate these
functions. Fortunately, the <b>chain rule</b> enables us to differentiate
composite functions.
</p>

<p>
Let us first consider functions of a single variable. Suppose that
functions \(y=f(u)\) and \(u=g(x)\) are both differentiable, then the
chain rule states that
</p>

\begin{equation}
\label{org268a459}
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}.
\end{equation}


<p>
Now let us turn our attention to a more general scenario where
functions have an arbitrary number of variables. Suppose that the
differentiable function \(y\) has variables \(u_1, u_2, \ldots, u_m\),
where each differentiable function \(u_i\) has variables \(x_1, x_2,
\ldots, x_n\). Note that \(y\) is a function of \(x_1, x_2, \ldots,
x_n\). Then the chain rule gives
</p>

\begin{equation}
\label{org57cbb5f}
\frac{dy}{dx_i} = \frac{dy}{du_1} \frac{du_1}{dx_i} + \frac{dy}{du_2} \frac{du_2}{dx_i} + \cdots + \frac{dy}{du_m} \frac{du_m}{dx_i}
\end{equation}

<p>
for any \(i = 1, 2, \ldots, n\).
</p>
</div>
</div>

<div id="outline-container-orgd30255e" class="outline-3">
<h3 id="orgd30255e"><span class="section-number-3">1.5.</span> Summary</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>Differential calculus and integral calculus are two branches of
calculus, where the former can be applied to the ubiquitous
optimization problems in deep learning.</li>
<li>A derivative can be interpreted as the instantaneous rate of change
of a function with respect to its variable. It is also the slope of
the tangent line to the curve of the function.</li>
<li>A gradient is a vector whose components are the partial derivatives
of a multivariate function with respect to all its variables.</li>
<li>The chain rule enables us to differentiate composite functions.</li>
</ul>
</div>
</div>

<div id="outline-container-orgb9e854d" class="outline-3">
<h3 id="orgb9e854d"><span class="section-number-3">1.6.</span> Exercises</h3>
<div class="outline-text-3" id="text-1-6">
<ol class="org-ol">
<li><p>
Plot the function \(y = f(x) = x^3 - \frac{1}{x}\) and its
tangent line when \(x = 1\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">f2</span> <span style="color: #7388d6;">[</span>x<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span>- <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow x 3<span style="color: #909183;">)</span> <span style="color: #909183;">(</span>/ 1 x<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>numerical-lim f2 1 0.000001<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
4.000001999737712
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>- <span style="color: #7388d6;">(</span>* 4 1<span style="color: #7388d6;">)</span> <span style="color: #7388d6;">(</span>f2 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
4.0
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span>* 1/100 <span style="color: #000000;">%</span><span style="color: #709870;">)</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 10 300<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
      y1 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> f2 x<span style="color: #909183;">)</span>
      y2 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span>- <span style="color: #907373;">(</span>* 4 <span style="color: #000000;">%</span><span style="color: #907373;">)</span> 4<span style="color: #709870;">)</span> x<span style="color: #909183;">)</span>
      chart <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">chart</span>/line <span style="color: #709870;">{</span><span style="color: #110099;">:title</span> <span style="color: #2A00FF;">"tangent line"</span>
                         <span style="color: #110099;">:series</span> <span style="color: #907373;">[</span><span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y1"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y1<span style="color: #6276ba;">}</span>
                                  <span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y2"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y2<span style="color: #6276ba;">}</span><span style="color: #907373;">]</span><span style="color: #709870;">}</span><span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">plot</span>/store! chart <span style="color: #110099;">nil</span> <span style="color: #2A00FF;">"notes/figures/exercise-2.4-1.svg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="orgda93b35" class="figure">
<p><img src="figures/exercise-2.4-1.svg" alt="exercise-2.4-1.svg" class="org-svg" />
</p>
</div></li>

<li><p>
Find the gradient of the function \(f(\mathbf{x}) = 3x_1^2 +
   5e^{x_2}\).
</p>

\begin{equation}
\label{orge7b9b0d}
\nabla_\mathbf{x} f(\mathbf{x})
= \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}\bigg]^\top
= \bigg[6x_1, 5e^{x_2}\bigg]^\top
\end{equation}</li>

<li>What is the gradient of the function \(f(\mathbf{x}) =
   \|\mathbf{x}\|_2\)?</li>
<li>Can you write out the chain rule for the case where \(u = f(x, y,
   z)\) and \(x = x(a, b)\), \(y = y(a, b)\), and \(z = z(a, b)\)?</li>
</ol>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Kimi Ma</p>
<p class="date">Created: 2022-05-17 Tue 07:53</p>
</div>
</body>
</html>
