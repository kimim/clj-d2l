<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-05-17 Tue 08:06 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Kimi Ma" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
</style>
<link rel="stylesheet" type="text/css" href="css/style.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="sitemap.html"> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb7ce759">1. Probability</a>
<ul>
<li><a href="#org1f6e26c">1.1. Basic Probability Theory</a></li>
<li><a href="#orgdfbc5e7">1.2. Axioms of Probability Theory</a></li>
<li><a href="#org43bc16e">1.3. Random Variables</a></li>
<li><a href="#org0cd6437">1.4. Dealing with Multiple Random Variables</a></li>
<li><a href="#org2de8d8e">1.5. Joint Probability</a></li>
<li><a href="#org59d4155">1.6. Conditional Probability</a></li>
<li><a href="#org0517933">1.7. Bayes&rsquo; theorem</a></li>
<li><a href="#org60ec4a9">1.8. Marginalization</a></li>
<li><a href="#org82ded80">1.9. Independence</a></li>
<li><a href="#orgde4f1b9">1.10. Application</a></li>
<li><a href="#orgcbfa45c">1.11. Expectation and Variance</a></li>
<li><a href="#org9eaba54">1.12. Summary</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgb7ce759" class="outline-2">
<h2 id="orgb7ce759"><span class="section-number-2">1.</span> Probability</h2>
<div class="outline-text-2" id="text-1">
<p>
In some form or another, machine learning is all about making
predictions. We might want to predict the <b>probability</b> of a patient
suffering a heart attack in the next year, given their clinical
history. In anomaly detection, we might want to assess how <b>likely</b> a
set of readings from an airplane&rsquo;s jet engine would be, were it
operating normally. In reinforcement learning, we want an agent to act
intelligently in an environment. This means we need to think about the
probability of getting a high reward under each of the available
actions. And when we build recommender systems we also need to think
about probability. For example, say <b>hypothetically</b> that we worked for
a large online bookseller. We might want to estimate the probability
that a particular user would buy a particular book. For this we need
to use the language of probability. Entire courses, majors, theses,
careers, and even departments, are devoted to probability. So
naturally, our goal in this section is not to teach the whole
subject. Instead we hope to get you off the ground, to teach you just
enough that you can start building your first deep learning models,
and to give you enough of a flavor for the subject that you can begin
to explore it on your own if you wish.
</p>

<p>
We have already invoked probabilities in previous sections without
articulating what precisely they are or giving a concrete example. Let
us get more serious now by considering the first case: distinguishing
cats and dogs based on photographs. This might sound simple but it is
actually a formidable challenge. To start with, the difficulty of the
problem may depend on the resolution of the image.
</p>
</div>

<div id="outline-container-org1f6e26c" class="outline-3">
<h3 id="org1f6e26c"><span class="section-number-3">1.1.</span> Basic Probability Theory</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Say that we cast a die and want to know what the chance is of seeing a
\(1\) rather than another digit. If the die is fair, all the six
outcomes \(\{1, \ldots, 6\}\) are equally likely to occur, and thus we
would see a \(1\) in one out of six cases. Formally we state that
\(1\) occurs with probability \(\frac{1}{6}\).
</p>

<p>
For a real die that we receive from a factory, we might not know those
proportions and we would need to check whether it is tainted. The only
way to investigate the die is by casting it many times and recording
the outcomes. For each cast of the die, we will observe a value in
\(\{1, \ldots, 6\}\). Given these outcomes, we want to investigate the
probability of observing each outcome.
</p>

<p>
One natural approach for each value is to take the individual count for
that value and to divide it by the total number of tosses. This gives us
an <b>estimate</b> of the probability of a given <b>event</b>. The <b>law of large
numbers</b> tell us that as the number of tosses grows this estimate will
draw closer and closer to the true underlying probability. Before going
into the details of what is going here, let us try it out.
</p>

<p>
To start, let us import the necessary packages.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.probability</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training <span style="color: #110099;">:as</span> t<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.chart <span style="color: #110099;">:as</span> chart<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.plot <span style="color: #110099;">:as</span> plot<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Next, we will want to be able to cast the die. In statistics we call
this process of drawing examples from probability distributions
<b>sampling</b>. The distribution that assigns probabilities to a number of
discrete choices is called the <b>multinomial distribution</b>. We will give
a more formal definition of <b>distribution</b> later, but at a high level,
think of it as just an assignment of probabilities to events.
</p>

<p>
To draw a single sample, we simply pass in a vector of probabilities.
The output is another vector of the same length: its value at index
\(i\) is the number of times the sampling outcome corresponds to
\(i\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/new-base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">fair-probs</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">repeat</span> 6 <span style="color: #709870;">(</span>/ 1.0 6<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #3F7F5F;">;</span><span style="color: #3F7F5F;">=&gt; [1/6 1/6 1/6 1/6 1/6 1/6]</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 1 fair-probs<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (6) cpu() int64
[ 0,  0,  1,  0,  0,  0]
</pre>


<p>
If you run the sampler a bunch of times, you will find that you get
out random values each time. As with estimating the fairness of a die,
we often want to generate many samples from the same distribution. It
would be unbearably slow to do this with a Clojure <code>loop</code>, so the
function we are using supports drawing multiple samples at once,
returning an array of independent samples in any shape we might
desire.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 10 fair-probs<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (6) cpu() int64
[ 3,  1,  0,  0,  3,  3]
</pre>


<p>
Now that we know how to sample rolls of a die, we can simulate 1000
rolls. We can then go through and count, after each of the 1000 rolls,
how many times each number was rolled. Specifically, we calculate the
relative frequency as the estimate of the true probability.
</p>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">counts</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 1000 fair-probs<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// counts 1000<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (6) cpu() float32
[0.183, 0.156, 0.182, 0.169, 0.153, 0.157]
</pre>


<p>
Because we generated the data from a fair die, we know that each
outcome has true probability \(\frac{1}{6}\), roughly \(0.167\), so
the above output estimates look good.
</p>

<p>
We can also visualize how these probabilities converge over time
towards the true probability. Let us conduct 500 groups of experiments
where each group draws 10 samples.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">counts</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 10 fair-probs <span style="color: #909183;">[</span>500<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">cum-counts</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/cumsum counts 0<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">estimates</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// cum-counts <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum cum-counts 1 <span style="color: #110099;">true</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 500<span style="color: #909183;">)</span>
      dies <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 6<span style="color: #909183;">)</span>
      names <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">mapv</span> #<span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> <span style="color: #2A00FF;">"P(die="</span> <span style="color: #000000;">%</span> <span style="color: #2A00FF;">")"</span><span style="color: #709870;">)</span> dies<span style="color: #909183;">)</span>
      ys <span style="color: #909183;">(</span>mapv #<span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/transpose estimates<span style="color: #6276ba;">)</span> <span style="color: #000000;">%</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span> dies<span style="color: #909183;">)</span>
      series <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">concat</span> <span style="color: #709870;">[</span><span style="color: #907373;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"0.167"</span>
                       <span style="color: #110099;">:xs</span> x
                       <span style="color: #110099;">:ys</span> <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">repeat</span> 500 0.167<span style="color: #6276ba;">)</span><span style="color: #907373;">}</span><span style="color: #709870;">]</span>
                     <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">fn</span> <span style="color: #6276ba;">[</span>n y<span style="color: #6276ba;">]</span> <span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> n <span style="color: #110099;">:xs</span> x <span style="color: #110099;">:ys</span> y<span style="color: #6276ba;">}</span><span style="color: #907373;">)</span>
                          names ys<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
      c <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">chart</span>/line <span style="color: #709870;">{</span><span style="color: #110099;">:series</span> series<span style="color: #709870;">}</span><span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">plot</span>/store! c <span style="color: #110099;">nil</span> <span style="color: #2A00FF;">"notes/figures/probability_dies.svg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="org452c224" class="figure">
<p><img src="figures/probability_dies.svg" alt="probability_dies.svg" class="org-svg" />
</p>
</div>


<p>
Each solid curve corresponds to one of the six values of the die and
gives our estimated probability that the die turns up that value as
assessed after each group of experiments. The blue line gives
the true underlying probability. As we get more data by conducting
more experiments, the \(6\) solid curves converge towards the true
probability.
</p>
</div>
</div>

<div id="outline-container-orgdfbc5e7" class="outline-3">
<h3 id="orgdfbc5e7"><span class="section-number-3">1.2.</span> Axioms of Probability Theory</h3>
<div class="outline-text-3" id="text-1-2">
<p>
When dealing with the rolls of a die, we call the set \(\mathcal{S} =
\{1, 2, 3, 4, 5, 6\}\) the <b>sample space</b> or <b>outcome space</b>, where each
element is an <b>outcome</b>. An <b>event</b> is a set of outcomes from a given
sample space. For instance, &ldquo;seeing a \(5\)&rdquo; (\(\{5\}\)) and &ldquo;seeing
an odd number&rdquo; (\(\{1, 3, 5\}\)) are both valid events of rolling a
die. Note that if the outcome of a random experiment is in event
\(\mathcal{A}\), then event \(\mathcal{A}\) has occurred. That is to
say, if \(3\) dots faced up after rolling a die, since \(3 \in \{1, 3,
5\}\), we can say that the event &ldquo;seeing an odd number&rdquo; has occurred.
</p>

<p>
Formally, <b>probability</b> can be thought of as a function that maps a set
to a real value. The probability of an event \(\mathcal{A}\) in the
given sample space \(\mathcal{S}\), denoted as \(P(\mathcal{A})\),
satisfies the following properties:
</p>

<ul class="org-ul">
<li>For any event \(\mathcal{A}\), its probability is never negative,
i.e., \(P(\mathcal{A}) \geq 0\);</li>
<li>Probability of the entire sample space is \(1\), i.e.,
\(P(\mathcal{S}) = 1\);</li>
<li>For any countable sequence of events \(\mathcal{A}_1, \mathcal{A}_2,
  \ldots\) that are <b>mutually exclusive</b> (\(\mathcal{A}_i \cap
  \mathcal{A}_j = \emptyset\) for all \(i \neq j\)), the probability
that any happens is equal to the sum of their individual
probabilities, i.e., \(P(\bigcup_{i=1}^{\infty} \mathcal{A}_i) =
  \sum_{i=1}^{\infty} P(\mathcal{A}_i)\).</li>
</ul>

<p>
These are also the axioms of probability theory, proposed by
Kolmogorov in 1933. Thanks to this axiom system, we can avoid any
philosophical dispute on randomness; instead, we can reason rigorously
with a mathematical language. For instance, by letting event
\(\mathcal{A}_1\) be the entire sample space and \(\mathcal{A}_i =
\emptyset\) for all \(i > 1\), we can prove that \(P(\emptyset) = 0\),
i.e., the probability of an impossible event is \(0\).
</p>
</div>
</div>

<div id="outline-container-org43bc16e" class="outline-3">
<h3 id="org43bc16e"><span class="section-number-3">1.3.</span> Random Variables</h3>
<div class="outline-text-3" id="text-1-3">
<p>
In our random experiment of casting a die, we introduced the notion of
a <b>random variable</b>. A random variable can be pretty much any quantity
and is not deterministic. It could take one value among a set of
possibilities in a random experiment. Consider a random variable \(X\)
whose value is in the sample space \(\mathcal{S} = \{1, 2, 3, 4, 5,
6\}\) of rolling a die. We can denote the event &ldquo;seeing a \(5\)&rdquo; as
\(\{X = 5\}\) or \(X = 5\), and its probability as \(P(\{X = 5\})\) or
\(P(X = 5)\). By \(P(X = a)\), we make a distinction between the
random variable \(X\) and the values (e.g., \(a\)) that \(X\) can
take. However, such pedantry results in a cumbersome notation. For a
compact notation, on one hand, we can just denote \(P(X)\) as the
<b>distribution</b> over the random variable \(X\): the distribution tells us
the probability that \(X\) takes any value. On the other hand, we can
simply write \(P(a)\) to denote the probability that a random variable
takes the value \(a\). Since an event in probability theory is a set
of outcomes from the sample space, we can specify a range of values
for a random variable to take. For example, \(P(1 \leq X \leq 3)\)
denotes the probability of the event \(\{1 \leq X \leq 3\}\), which
means \(\{X = 1, 2, \text{or}, 3\}\). Equivalently, \(P(1 \leq X \leq
3)\) represents the probability that the random variable \(X\) can
take a value from \(\{1, 2, 3\}\).
</p>

<p>
Note that there is a subtle difference between <b>discrete</b> random
variables, like the sides of a die, and <b>continuous</b> ones, like the
weight and the height of a person. There is little point in asking
whether two people have exactly the same height. If we take precise
enough measurements you will find that no two people on the planet have
the exact same height. In fact, if we take a fine enough measurement,
you will not have the same height when you wake up and when you go to
sleep. So there is no purpose in asking about the probability that
someone is 1.80139278291028719210196740527486202 meters tall. Given the
world population of humans the probability is virtually 0. It makes more
sense in this case to ask whether someoneâ€™s height falls into a given
interval, say between 1.79 and 1.81 meters. In these cases we quantify
the likelihood that we see a value as a <b>density</b>. The height of exactly
1.80 meters has no probability, but nonzero density. In the interval
between any two different heights we have nonzero probability. In the
rest of this section, we consider probability in discrete space. For
probability over continuous random variables, you may refer to
Section 18.6.
</p>
</div>
</div>

<div id="outline-container-org0cd6437" class="outline-3">
<h3 id="org0cd6437"><span class="section-number-3">1.4.</span> Dealing with Multiple Random Variables</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Very often, we will want to consider more than one random variable at
a time. For instance, we may want to model the relationship between
diseases and symptoms. Given a disease and a symptom, say &ldquo;flu&rdquo; and
&ldquo;cough&rdquo;, either may or may not occur in a patient with some
probability.  While we hope that the probability of both would be
close to zero, we may want to estimate these probabilities and their
relationships to each other so that we may apply our inferences to
effect better medical care.
</p>

<p>
As a more complicated example, images contain millions of pixels, thus
millions of random variables. And in many cases images will come with
a label, identifying objects in the image. We can also think of the
label as a random variable. We can even think of all the metadata as
random variables such as location, time, aperture, focal length, ISO,
focus distance, and camera type. All of these are random variables
that occur jointly. When we deal with multiple random variables, there
are several quantities of interest.
</p>
</div>
</div>

<div id="outline-container-org2de8d8e" class="outline-3">
<h3 id="org2de8d8e"><span class="section-number-3">1.5.</span> Joint Probability</h3>
<div class="outline-text-3" id="text-1-5">
<p>
The first is called the <b>joint probability</b> \(P(A = a, B=b)\). Given any
values \(a\) and \(b\), the joint probability lets us answer, what is
the probability that \(A=a\) and \(B=b\) simultaneously? Note that for
any values \(a\) and \(b\), \(P(A=a, B=b) \leq P(A=a)\). This has to
be the case, since for \(A=a\) and \(B=b\) to happen, \(A=a\) has to
happen <b>and</b> \(B=b\) also has to happen (and vice versa). Thus, \(A=a\)
and \(B=b\) cannot be more likely than \(A=a\) or \(B=b\)
individually.
</p>
</div>
</div>

<div id="outline-container-org59d4155" class="outline-3">
<h3 id="org59d4155"><span class="section-number-3">1.6.</span> Conditional Probability</h3>
<div class="outline-text-3" id="text-1-6">
<p>
This brings us to an interesting ratio: \(0 \leq \frac{P(A=a,
B=b)}{P(A=a)} \leq 1\). We call this ratio a <b>conditional probability</b>
and denote it by \(P(B=b \mid A=a)\): it is the probability of
\(B=b\), provided that \(A=a\) has occurred.
</p>
</div>
</div>

<div id="outline-container-org0517933" class="outline-3">
<h3 id="org0517933"><span class="section-number-3">1.7.</span> Bayes&rsquo; theorem</h3>
<div class="outline-text-3" id="text-1-7">
<p>
Using the definition of conditional probabilities, we can derive one
of the most useful and celebrated equations in statistics: <b>Bayes&rsquo;
theorem</b>. It goes as follows. By construction, we have the
<b>multiplication rule</b> that \(P(A, B) = P(B \mid A) P(A)\). By symmetry,
this also holds for \(P(A, B) = P(A \mid B) P(B)\). Assume that \(P(B)
> 0\). Solving for one of the conditional variables we get
</p>

\begin{equation}
\label{org1b4aa62}
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.
\end{equation}

<p>
Note that here we use the more compact notation where \(P(A, B)\)
is a <b>joint distribution</b> and \(P(A \mid B)\) is a <b>conditional
distribution</b>. Such distributions can be evaluated for particular
values \(A = a, B=b\).
</p>
</div>
</div>

<div id="outline-container-org60ec4a9" class="outline-3">
<h3 id="org60ec4a9"><span class="section-number-3">1.8.</span> Marginalization</h3>
<div class="outline-text-3" id="text-1-8">
<p>
Bayes&rsquo; theorem is very useful if we want to infer one thing from the
other, say cause and effect, but we only know the properties in the
reverse direction, as we will see later in this section. One important
operation that we need, to make this work, is <b>marginalization</b>. It is
the operation of determining \(P(B)\) from \(P(A, B)\). We can see
that the probability of \(B\) amounts to accounting for all possible
choices of \(A\) and aggregating the joint probabilities over all of
them:
</p>

\begin{equation}
\label{org19e3a31}
P(B) = \sum_{A} P(A, B),
\end{equation}

<p>
which is also known as the <b>sum rule</b>. The probability or distribution
as a result of marginalization is called a <b>marginal probability</b> or a
<b>marginal distribution</b>.
</p>
</div>
</div>

<div id="outline-container-org82ded80" class="outline-3">
<h3 id="org82ded80"><span class="section-number-3">1.9.</span> Independence</h3>
<div class="outline-text-3" id="text-1-9">
<p>
Another useful property to check for is <b>dependence</b> vs. <b>independence</b>.
Two random variables \(A\) and \(B\) being independent means that the
occurrence of one event of \(A\) does not reveal any information about
the occurrence of an event of \(B\). In this case \(P(B \mid A) =
P(B)\). Statisticians typically express this as \(A \perp B\). From
Bayes&rsquo; theorem, it follows immediately that also \(P(A \mid B) =
P(A)\). In all the other cases we call \(A\) and \(B\) dependent. For
instance, two successive rolls of a die are independent. In contrast,
the position of a light switch and the brightness in the room are not
(they are not perfectly deterministic, though, since we could always
have a broken light bulb, power failure, or a broken switch).
</p>

<p>
Since \(P(A \mid B) = \frac{P(A, B)}{P(B)} = P(A)\) is equivalent to
\(P(A, B) = P(A)P(B)\), two random variables are independent if and
only if their joint distribution is the product of their individual
distributions. Likewise, two random variables \(A\) and \(B\) are
<b>conditionally independent</b> given another random variable \(C\) if and
only if \(P(A, B \mid C) = P(A \mid C)P(B \mid C)\). This is expressed
as \(A \perp B \mid C\).
</p>
</div>
</div>

<div id="outline-container-orgde4f1b9" class="outline-3">
<h3 id="orgde4f1b9"><span class="section-number-3">1.10.</span> Application</h3>
<div class="outline-text-3" id="text-1-10">
<p>
Let us put our skills to the test. Assume that a doctor administers an
HIV test to a patient. This test is fairly accurate and it fails only
with 1% probability if the patient is healthy but reporting him as
diseased. Moreover, it never fails to detect HIV if the patient
actually has it. We use \(D_1\) to indicate the diagnosis (\(1\) if
positive and \(0\) if negative) and \(H\) to denote the HIV status
(\(1\) if positive and \(0\) if negative).  Table <a href="#org92f5877">1</a>
lists such conditional probabilities.
</p>

<table id="org92f5877" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Conditional probability of \(P(D_1 \mid H)\).</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Conditional probability</th>
<th scope="col" class="org-right">\(H=1\)</th>
<th scope="col" class="org-right">\(H=0\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(P(D_1 = 1 \mid H)\)</td>
<td class="org-right">1</td>
<td class="org-right">0.01</td>
</tr>

<tr>
<td class="org-left">\(P(D_1 = 0 \mid H)\)</td>
<td class="org-right">0</td>
<td class="org-right">0.99</td>
</tr>
</tbody>
</table>

<p>
Note that the column sums are all 1 (but the row sums are not), since
the conditional probability needs to sum up to 1, just like the
probability. Let us work out the probability of the patient having HIV
if the test comes back positive, i.e., \(P(H = 1 \mid D_1 = 1)\).
Obviously this is going to depend on how common the disease is, since
it affects the number of false alarms. Assume that the population is
quite healthy, e.g., \(P(H=1) = 0.0015\). To apply Bayes&rsquo; theorem, we
need to apply marginalization and the multiplication rule to determine
</p>

\begin{equation}
  \begin{aligned}
    &P(D_1 = 1) \\
    =& P(D_1=1, H=0) + P(D_1=1, H=1)  \\
    =& P(D_1=1 \mid H=0) P(H=0) + P(D_1=1 \mid H=1) P(H=1) \\
    =& 0.011485.
  \end{aligned}
\end{equation}

<p>
Thus, we get
</p>

\begin{equation}
\begin{aligned}
&P(H = 1 \mid D_1 = 1)\\ =& \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)} \\ =& 0.1306
\end{aligned}
\end{equation}

<p>
In other words, there is only a 13.06% chance that the patient
actually has HIV, despite using a very accurate test. As we can see,
probability can be counter intuitive.
</p>

<p>
What should a patient do upon receiving such terrifying news? Likely,
the patient would ask the physician to administer another test to get
clarity. The second test has different characteristics and it is not
as good as the first one, as shown in Table <a href="#org25794e3">2</a>.
</p>

<table id="org25794e3" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> Conditional probability of \(P(D_2 \mid H)\).</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Conditional probability</th>
<th scope="col" class="org-right">\(H=1\)</th>
<th scope="col" class="org-right">\(H=0\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(P(D_2 = 1 \mid H)\)</td>
<td class="org-right">0.98</td>
<td class="org-right">0.03</td>
</tr>

<tr>
<td class="org-left">\(P(D_2 = 0 \mid H)\)</td>
<td class="org-right">0.02</td>
<td class="org-right">0.97</td>
</tr>
</tbody>
</table>

<p>
Unfortunately, the second test comes back positive, too. Let us work
out the requisite probabilities to invoke Bayes&rsquo; theorem by assuming
the conditional independence:
</p>

\begin{equation}
\begin{aligned}
&P(D_1 = 1, D_2 = 1 \mid H = 0) \\
=& P(D_1 = 1 \mid H = 0) P(D_2 = 1 \mid H = 0)  \\
=& 0.0003,
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
&P(D_1 = 1, D_2 = 1 \mid H = 1) \\
=& P(D_1 = 1 \mid H = 1) P(D_2 = 1 \mid H = 1)  \\
=& 0.98.
\end{aligned}
\end{equation}

<p>
Now we can apply marginalization and the multiplication rule:
</p>

\begin{equation}
\begin{aligned}
&P(D_1 = 1, D_2 = 1) \\
=& P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\
=& P(D_1 = 1, D_2 = 1 \mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \mid H = 1)P(H=1)\\
=& 0.00176955.
\end{aligned}
\end{equation}

<p>
In the end, the probability of the patient having HIV given both
positive tests is
</p>

\begin{equation}
\begin{aligned}
&P(H = 1 \mid D_1 = 1, D_2 = 1)\\
=& \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \\
=& 0.8307.
\end{aligned}
\end{equation}

<p>
That is, the second test allowed us to gain much higher confidence
that not all is well. Despite the second test being considerably less
accurate than the first one, it still significantly improved our
estimate.
</p>
</div>
</div>

<div id="outline-container-orgcbfa45c" class="outline-3">
<h3 id="orgcbfa45c"><span class="section-number-3">1.11.</span> Expectation and Variance</h3>
<div class="outline-text-3" id="text-1-11">
<p>
To summarize key characteristics of probability distributions, we need
some measures. The <b>expectation</b> (or average) of the random variable
\(X\) is denoted as
</p>

\begin{equation}
E[X] = \sum_{x} x P(X = x).
\end{equation}

<p>
When the input of a function \(f(x)\) is a random variable drawn from
the distribution \(P\) with different values \(x\), the expectation of
\(f(x)\) is computed as
</p>

\begin{equation}
E_{x \sim P}[f(x)] = \sum_x f(x) P(x).
\end{equation}

<p>
In many cases we want to measure by how much the random variable \(X\)
deviates from its expectation. This can be quantified by the variance
</p>


\begin{equation}
  \mathrm{Var}[X] = E\left[(X - E[X])^2\right] =
  E[X^2] - E[X]^2.
\end{equation}

<p>
Its square root is called the <b>standard deviation</b>. The variance of a
function of a random variable measures by how much the function
deviates from the expectation of the function, as different values
\(x\) of the random variable are sampled from its distribution:
</p>

\begin{equation}
\mathrm{Var}[f(x)] = E\left[\left(f(x) - E[f(x)]\right)^2\right].
\end{equation}
</div>
</div>

<div id="outline-container-org9eaba54" class="outline-3">
<h3 id="org9eaba54"><span class="section-number-3">1.12.</span> Summary</h3>
<div class="outline-text-3" id="text-1-12">
<ul class="org-ul">
<li>We can sample from probability distributions.</li>
<li>We can analyze multiple random variables using joint distribution,
conditional distribution, Bayes&rsquo; theorem, marginalization, and
independence assumptions.</li>
<li>Expectation and variance offer useful measures to summarize key
characteristics of probability distributions.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Kimi Ma</p>
<p class="date">Created: 2022-05-17 Tue 08:06</p>
</div>
</body>
</html>
