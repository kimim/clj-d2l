<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-05-17 Tue 07:59 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Kimi Ma" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
</style>
<link rel="stylesheet" type="text/css" href="css/style.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="sitemap.html"> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#lin-reg-scratch">1. Linear Regression Implementation from Scratch</a>
<ul>
<li><a href="#org3b0b6e5">1.1. Generating the Dataset</a></li>
<li><a href="#orgaa33555">1.2. Reading the Dataset</a></li>
<li><a href="#orga57bf25">1.3. Initializing Model Parameters</a></li>
<li><a href="#org1e28a98">1.4. Defining the Model</a></li>
<li><a href="#orga1d91d8">1.5. Defining the Loss Function</a></li>
<li><a href="#orga010d44">1.6. Defining the Optimization Algorithm</a></li>
<li><a href="#org9fca057">1.7. Training</a></li>
<li><a href="#orge399cb3">1.8. Summary</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-lin-reg-scratch" class="outline-2">
<h2 id="lin-reg-scratch"><span class="section-number-2">1.</span> Linear Regression Implementation from Scratch</h2>
<div class="outline-text-2" id="text-lin-reg-scratch">
<p>
Now that you understand the key ideas behind linear regression, we can
begin to work through a hands-on implementation in code. In this
section, we will implement the entire method from scratch, including
the data pipeline, the model, the loss function, and the minibatch
stochastic gradient descent optimizer. While modern deep learning
frameworks can automate nearly all of this work, implementing things
from scratch is the only way to make sure that you really know what
you are doing. Moreover, when it comes time to customize models,
defining our own layers or loss functions, understanding how things
work under the hood will prove handy. In this section, we will rely
only on tensors and auto differentiation. Afterwards, we will
introduce a more concise implementation, taking advantage of bells and
whistles of deep learning frameworks.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.linreg</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.device <span style="color: #110099;">:as</span> device<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.engine <span style="color: #110099;">:as</span> engine<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training <span style="color: #110099;">:as</span> t<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.dataset <span style="color: #110099;">:as</span> ds<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>

<div id="outline-container-org3b0b6e5" class="outline-3">
<h3 id="org3b0b6e5"><span class="section-number-3">1.1.</span> Generating the Dataset</h3>
<div class="outline-text-3" id="text-1-1">
<p>
To keep things simple, we will construct an artificial dataset
according to a linear model with additive noise. Our task will be to
recover this model&rsquo;s parameters using the finite set of examples
contained in our dataset. We will keep the data low-dimensional so we
can visualize it easily. In the following code snippet, we generate a
dataset containing 1000 examples, each consisting of 2 features
sampled from a standard normal distribution. Thus our synthetic
dataset will be a matrix \(\mathbf{X}\in \mathbb{R}^{1000 \times 2}\).
</p>

<p>
The true parameters generating our dataset will be \(\mathbf{w} = [2,
-3.4]^\top\) and \(b = 4.2\), and our synthetic labels will be
assigned according to the following linear model with the noise term
\(\epsilon\):
</p>

\begin{equation}
\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.
\end{equation}


<p>
You could think of \(\epsilon\) as capturing potential measurement
errors on the features and labels. We will assume that the standard
assumptions hold and thus that \(\epsilon\) obeys a normal
distribution with mean of 0. To make our problem easy, we will set its
standard deviation to 0.01. The following code generates our synthetic
dataset.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">synthetic-data</span> <span style="color: #7388d6;">[</span>ndm w b num<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>X <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm <span style="color: #907373;">[</span>num <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size w<span style="color: #6276ba;">)</span><span style="color: #907373;">]</span><span style="color: #709870;">)</span>
        y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot X w<span style="color: #907373;">)</span> b<span style="color: #709870;">)</span>
        noise <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm 0 0.01 <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape y<span style="color: #907373;">)</span> <span style="color: #110099;">:float32</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">[</span>X <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ y noise<span style="color: #709870;">)</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/new-base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">true-w</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">float-array</span> <span style="color: #709870;">[</span>2 -3.4<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">true-b</span> 4.2<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">dp</span> <span style="color: #7388d6;">(</span>synthetic-data ndm true-w true-b 1000<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">features</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> dp 0<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">labels</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> dp 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Note that each row in features consists of a 2-dimensional data
example and that each row in labels consists of a 1-dimensional label
value (a scalar).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>str <span style="color: #2A00FF;">"features(0): "</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> features <span style="color: #709870;">[</span>0<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #2A00FF;">"</span><span style="color: #000000; background-color: #f8f8f8; font-weight: bold;">\n</span><span style="color: #2A00FF;">labels(0): "</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element labels <span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
features(0): [1.1630785 2.2122061]
labels(0): -1.0015316
</pre>


<p>
By generating a scatter plot using the second feature and <code>labels</code>, we
can clearly observe the linear correlation between the two.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> features <span style="color: #2A00FF;">":, 1"</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
      y <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec labels<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">d2l</span>/plot-scatter
   <span style="color: #2A00FF;">"notes/figures/synthetic_data.svg"</span>
   <span style="color: #2A00FF;">"data"</span>
   x
   y<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="org19d27d4" class="figure">
<p><img src="figures/synthetic_data.svg" alt="synthetic_data.svg" class="org-svg" />
</p>
</div>
</div>
</div>


<div id="outline-container-orgaa33555" class="outline-3">
<h3 id="orgaa33555"><span class="section-number-3">1.2.</span> Reading the Dataset</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Recall that training models consists of making multiple passes over the
dataset, grabbing one minibatch of examples at a time, and using them to
update our model. Since this process is so fundamental to training
machine learning algorithms, it is worth defining a utility function to
shuffle the dataset and access it in minibatches.
</p>

<p>
In the following code, we define the <code>data-iter</code> function to demonstrate
one possible implementation of this functionality. The function takes
a batch size, a matrix of features, and a vector of labels, yielding
minibatches of the size <code>batch-size</code>. Each minibatch consists of a tuple
of features and labels.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">data-iter</span> <span style="color: #7388d6;">[</span>batch-size features labels<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>num-examples <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size features<span style="color: #709870;">)</span>
        indices <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> num-examples<span style="color: #709870;">)</span>
        indices <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">shuffle</span> indices<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">for</span> <span style="color: #709870;">[</span>i <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 num-examples batch-size<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">[</span><span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;&gt;</span> <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> i <span style="color: #858580;">(</span><span style="color: #7F0055; font-weight: bold;">min</span> <span style="color: #80a880;">(</span>+ i batch-size<span style="color: #80a880;">)</span> num-examples<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span>map #<span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> features <span style="color: #80a880;">[</span><span style="color: #000000;">%</span><span style="color: #80a880;">]</span><span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/stack<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
       <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;&gt;</span> <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> i <span style="color: #858580;">(</span><span style="color: #7F0055; font-weight: bold;">min</span> <span style="color: #80a880;">(</span>+ i batch-size<span style="color: #80a880;">)</span> num-examples<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span>map #<span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> labels <span style="color: #80a880;">[</span><span style="color: #000000;">%</span><span style="color: #80a880;">]</span><span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/stack<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
In general, note that we want to use reasonably sized minibatches to
take advantage of the GPU hardware, which excels at parallelizing
operations. Because each example can be fed through our models in
parallel and the gradient of the loss function for each example can also
be taken in parallel, GPUs allow us to process hundreds of examples in
scarcely more time than it might take to process just a single example.
</p>

<p>
To build some intuition, let us read and print the first small batch
of data examples. The shape of the features in each minibatch tells us
both the minibatch size and the number of input features. Likewise,
our minibatch of labels will have a shape given by <code>batch-size</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #909183;">(</span>data-iter 10 features labels<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example" id="orgc1e4397">
ND: (10, 2) cpu() float32
[[ 1.1631,  2.2122],
 [ 0.4838,  0.774 ],
 [ 0.2996,  1.0434],
 [ 0.153 ,  1.1839],
 [-1.1688,  1.8917],
 [ 1.5581, -1.2347],
 [-0.5459, -1.771 ],
 [-2.3556, -0.4514],
 [ 0.5414,  0.5794],
 [ 2.6785, -1.8561],
]
</pre>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #909183;">(</span>data-iter 10 features labels<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (10) cpu() float32
[-1.0015,  2.5281,  1.2197,  0.4807, -4.5658, 11.5155,  9.1389,  1.0271,  3.3058, 15.8548]
</pre>


<p>
As we run the iteration, we obtain distinct minibatches successively
until the entire dataset has been exhausted (try this). While the
iteration implemented above is good for didactic purposes, it is
inefficient in ways that might get us in trouble on real problems. For
example, it requires that we load all the data in memory and that we
perform lots of random memory access. The built-in iterators
implemented in a deep learning framework are considerably more
efficient and they can deal with both data stored in files and data
fed via data streams.
</p>
</div>
</div>

<div id="outline-container-orga57bf25" class="outline-3">
<h3 id="orga57bf25"><span class="section-number-3">1.3.</span> Initializing Model Parameters</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Before we can begin optimizing our model&rsquo;s parameters by minibatch
stochastic gradient descent, we need to have some parameters in the
first place. In the following code, we initialize weights by sampling
random numbers from a normal distribution with mean 0 and a standard
deviation of 0.01, and setting the bias to 0.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">w</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm 0 0.01 <span style="color: #909183;">[</span>2 1<span style="color: #909183;">]</span> <span style="color: #110099;">:float32</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">b</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/zeros ndm <span style="color: #909183;">[</span>1<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/set-requires-gradient w <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/set-requires-gradient b <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec w<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec b<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
[0.0013263007 0.0072455113]
[0.0]
</pre>


<p>
After initializing our parameters, our next task is to update them until
they fit our data sufficiently well. Each update requires taking the
gradient of our loss function with respect to the parameters. Given this
gradient, we can update each parameter in the direction that may reduce
the loss.
</p>

<p>
Since nobody wants to compute gradients explicitly (this is tedious
and error prone), we use automatic differentiation, as introduced in
Section
<a href="2.5-automatic-differentiation.html#automatic_differentiation">2.5-automatic-differentiation.html#automatic_differentiation</a>,
to compute the gradient.
</p>
</div>
</div>

<div id="outline-container-org1e28a98" class="outline-3">
<h3 id="org1e28a98"><span class="section-number-3">1.4.</span> Defining the Model</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Next, we must define our model, relating its inputs and parameters to
its outputs. Recall that to calculate the output of the linear model,
we simply take the matrix-vector dot product of the input features
\(\mathbf{X}\) and the model weights \(\mathbf{w}\), and add the
offset \(b\) to each example. Note that below \(\mathbf{Xw}\) is a
vector and \(b\) is a scalar. Recall the broadcasting mechanism as
described in Section
<a href="2.1-data-manipulation.html#9dcbe412-db7e-485a-bb3c-d7181f2f7f05">2.1-data-manipulation.html#9dcbe412-db7e-485a-bb3c-d7181f2f7f05</a>. When
we add a vector and a scalar, the scalar is added to each component of
the vector.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">linreg</span>
  <span style="color: #2A00FF;">"The linear regression model."</span>
  <span style="color: #7388d6;">[</span>X w b<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot X w<span style="color: #909183;">)</span> b<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orga1d91d8" class="outline-3">
<h3 id="orga1d91d8"><span class="section-number-3">1.5.</span> Defining the Loss Function</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Since updating our model requires taking the gradient of our loss
function, we ought to define the loss function first. Here we will use
the squared loss function as described in Section
<a href="3.1-linear-regression.html#lin_reg">3.1-linear-regression.html#lin_reg</a>. In the implementation,
we need to transform the true value <code>y</code> into the predicted value&rsquo;s shape
<code>y-hat</code>. The result returned by the following function will also have
the same shape as <code>y-hat</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">squared-loss</span> <span style="color: #7388d6;">[</span>y-hat y<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- y-hat <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape y <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape y-hat<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span>
              <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- y-hat <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape y <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape y-hat<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
        2<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orga010d44" class="outline-3">
<h3 id="orga010d44"><span class="section-number-3">1.6.</span> Defining the Optimization Algorithm</h3>
<div class="outline-text-3" id="text-1-6">
<p>
As we discussed in Section
<a href="3.1-linear-regression.html#lin_reg">3.1-linear-regression.html#lin_reg</a>, linear regression has a
closed-form solution. However, this is not a book about linear
regression: it is a book about deep learning. Since none of the other
models that this book introduces can be solved analytically, we will
take this opportunity to introduce your first working example of
minibatch stochastic gradient descent.
</p>

<p>
At each step, using one minibatch randomly drawn from our dataset, we
will estimate the gradient of the loss with respect to our parameters.
Next, we will update our parameters in the direction that may reduce
the loss. The following code applies the minibatch stochastic gradient
descent update, given a set of parameters, a learning rate, and a
batch size. The size of the update step is determined by the learning
rate <code>lr</code>. Because our loss is calculated as a sum over the minibatch of
examples, we normalize our step size by the batch size (<code>batch-size</code>),
so that the magnitude of a typical step size does not depend heavily
on our choice of the batch size.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">sgd</span>
  <span style="color: #2A00FF;">"Minibatch stochastic gradient descent."</span>
  <span style="color: #7388d6;">[</span>params lr batch-size<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span>param params<span style="color: #909183;">]</span>
    <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">param = param - param.gradient * lr / batchSize</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/-! param <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-gradient param<span style="color: #6276ba;">)</span> lr<span style="color: #907373;">)</span> batch-size<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org9fca057" class="outline-3">
<h3 id="org9fca057"><span class="section-number-3">1.7.</span> Training</h3>
<div class="outline-text-3" id="text-1-7">
<p>
Now that we have all of the parts in place, we are ready to implement
the main training loop. It is crucial that you understand this code
because you will see nearly identical training loops over and over
again throughout your career in deep learning.
</p>

<p>
In each iteration, we will grab a minibatch of training examples, and
pass them through our model to obtain a set of predictions. After
calculating the loss, we initiate the backwards pass through the
network, storing the gradients with respect to each
parameter. Finally, we will call the optimization algorithm <code>sgd</code> to
update the model parameters.
</p>

<p>
In summary, we will execute the following loop:
</p>
<ul class="org-ul">
<li>Initialize parameters \((\mathbf{w}, b)\)</li>
<li>Repeat until done
<ul class="org-ul">
<li>Compute gradient \(\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)}
    \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}
    l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)\)</li>
<li>Update parameters \((\mathbf{w}, b) \leftarrow (\mathbf{w}, b) -
    \eta \mathbf{g}\)</li>
</ul></li>
</ul>

<p>
In each <b>epoch</b>, we will iterate through the entire dataset (using the
<code>data-iter</code> function) once passing through every example in the training
dataset (assuming that the number of examples is divisible by the
batch size). The number of epochs <code>num-epochs</code> and the learning rate <code>lr</code>
are both hyperparameters, which we set here to 3 and 0.03,
respectively. Unfortunately, setting hyperparameters is tricky and
requires some adjustment by trial and error. We elide these details
for now but revise them later in Section 11.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">lr</span> 0.03<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">num-epochs</span> 3<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">batch-size</span> 10<span style="color: #707183;">)</span>
<span style="color: #707183;">[</span>lr num-epochs batch-size<span style="color: #707183;">]</span>
</pre>
</div>

<pre class="example">
[0.03 3 10]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">datasets</span> <span style="color: #7388d6;">(</span>data-iter batch-size features labels<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.linreg/datasets
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #7388d6;">[</span>epoch <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> num-epochs<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span><span style="color: #709870;">[</span>X y<span style="color: #709870;">]</span> datasets<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #709870;">[</span>gc <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #907373;">[</span>l <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #858580;">(</span>linreg X w b<span style="color: #858580;">)</span> <span style="color: #858580;">(</span>squared-loss y<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span><span style="color: #907373;">]</span>
        <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc l<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>sgd <span style="color: #709870;">[</span>w b<span style="color: #709870;">]</span> lr batch-size<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"epoch "</span> epoch
           <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #709870;">(</span>squared-loss <span style="color: #907373;">(</span>linreg features w b<span style="color: #907373;">)</span> labels<span style="color: #709870;">)</span>
               <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/mean<span style="color: #709870;">)</span>
               <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
epoch  0 3.3477103E-4
epoch  1 5.24727E-5
epoch  2 5.1000967E-5
</pre>


<p>
In this case, because we synthesized the dataset ourselves, we know
precisely what the true parameters are. Thus, we can evaluate our
success in training by comparing the true parameters with those that
we learned through our training loop. Indeed they turn out to be very
close to each other.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec w<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec true-w<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">w-error</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- true-w <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape w <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-shape true-w<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Error in estimating w:"</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">vec</span> w-error<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Error in estimating w:"</span> <span style="color: #7388d6;">(</span>- true-b <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element b<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
[1.999567 -3.400089]
[2.0 -3.4]
Error in estimating w: [4.3296814E-4 8.893013E-5]
Error in estimating w: 5.704879760743964E-4
</pre>


<p>
Note that we should not take it for granted that we are able to
recover the parameters perfectly. However, in machine learning, we are
typically less concerned with recovering true underlying parameters,
and more concerned with parameters that lead to highly accurate
prediction. Fortunately, even on difficult optimization problems,
stochastic gradient descent can often find remarkably good solutions,
owing partly to the fact that, for deep networks, there exist many
configurations of the parameters that lead to highly accurate
prediction.
</p>
</div>
</div>


<div id="outline-container-orge399cb3" class="outline-3">
<h3 id="orge399cb3"><span class="section-number-3">1.8.</span> Summary</h3>
<div class="outline-text-3" id="text-1-8">
<ul class="org-ul">
<li>We saw how a deep network can be implemented and optimized from
scratch, using just tensors and auto differentiation, without any
need for defining layers or fancy optimizers.</li>
<li>This section only scratches the surface of what is possible. In the
following sections, we will describe additional models based on the
concepts that we have just introduced and learn how to implement
them more concisely.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Kimi Ma</p>
<p class="date">Created: 2022-05-17 Tue 07:59</p>
</div>
</body>
</html>
