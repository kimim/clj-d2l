<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-05-17 Tue 08:13 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>CLJ D2L</title>
<meta name="author" content="Kimi Ma" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
</style>
<link rel="stylesheet" type="text/css" href="css/style.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="sitemap.html"> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content" class="content">
<h1 class="title">CLJ D2L</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgae486b2">Notation</a>
<ul>
<li><a href="#org6092e37">Numbers</a></li>
<li><a href="#org962af57">Set Theory</a></li>
<li><a href="#orgf021315">Functions and Operators</a></li>
<li><a href="#org516c3d9">Calculus</a></li>
<li><a href="#org94e6d9d">Probability and Information Theory</a></li>
<li><a href="#org0da6d12">Complexity</a></li>
</ul>
</li>
<li><a href="#org70709e1">1. Introduction</a></li>
<li><a href="#org2f5c4d6">2. Preliminaries</a>
<ul>
<li><a href="#orgad8cd02">2.1. Data Manipulation</a>
<ul>
<li><a href="#orgac49908">2.1.1. Getting Started</a></li>
<li><a href="#org26d0a53">2.1.2. Operations</a>
<ul>
<li><a href="#org375da40">2.1.2.1. Operations</a></li>
</ul>
</li>
<li><a href="#orgfea52dd">2.1.3. Broadcasting Mechanism</a></li>
<li><a href="#org47736b6">2.1.4. Indexing and Slicing</a></li>
<li><a href="#org83ad0dd">2.1.5. Saving Memory</a></li>
<li><a href="#org2e1a0f5">2.1.6. Conversion to Other Clojure Objects</a></li>
</ul>
</li>
<li><a href="#orgf7a92b1">2.2. Data Preprocessing</a>
<ul>
<li><a href="#org4b66757">2.2.1. Reading the Dataset</a></li>
<li><a href="#orgd4a77d0">2.2.2. Handling Missing Data</a></li>
<li><a href="#orge717448">2.2.3. Conversion to the Tensor Format</a></li>
<li><a href="#org2c601c2">2.2.4. Summary</a></li>
</ul>
</li>
<li><a href="#orge3dd06c">2.3. Linear Algebra</a>
<ul>
<li><a href="#orga23896d">2.3.1. Scalars</a></li>
<li><a href="#orgbea84b2">2.3.2. Vectors</a></li>
<li><a href="#org5e5fbb8">2.3.3. Length, Dimensionality, and Shape</a></li>
<li><a href="#org66e578d">2.3.4. Matrices</a></li>
<li><a href="#org9ecbd6f">2.3.5. Tensors / NDArrays</a></li>
<li><a href="#orgce2ed21">2.3.6. Basic Properties of Tensor Arithmetic</a></li>
<li><a href="#lin-alg-reduction">2.3.7. Reduction</a>
<ul>
<li><a href="#lin-alg-non-reduction">2.3.7.1. Non-Reduction Sum</a></li>
</ul>
</li>
<li><a href="#org96430a4">2.3.8. Dot Products</a></li>
<li><a href="#orgea00a04">2.3.9. Matrix-Vector Products</a></li>
<li><a href="#orge8d5a74">2.3.10. Matrix-Matrix Multiplication</a></li>
<li><a href="#org6bf845b">2.3.11. Norms</a></li>
<li><a href="#orgba50537">2.3.12. Norms and Objectives</a></li>
<li><a href="#org313d17b">2.3.13. More on Linear Algebra</a></li>
<li><a href="#org6705c49">2.3.14. Summary</a></li>
</ul>
</li>
<li><a href="#orgbba6516">2.4. Calculus</a>
<ul>
<li><a href="#orgeba1e32">2.4.1. Derivatives and Differentiation</a></li>
<li><a href="#org18f3f26">2.4.2. Partial Derivatives</a></li>
<li><a href="#orgcab85af">2.4.3. Gradients</a></li>
<li><a href="#org82e3d29">2.4.4. Chain Rule</a></li>
<li><a href="#org23bf4b1">2.4.5. Summary</a></li>
<li><a href="#orgc32520d">2.4.6. Exercises</a></li>
</ul>
</li>
<li><a href="#org97b9449">2.5. Automatic Differentiation</a>
<ul>
<li><a href="#org2fe6e60">2.5.1. A Simple Example</a></li>
<li><a href="#orgd6cdf45">2.5.2. Backward for Non-Scalar Variables</a></li>
<li><a href="#org2ce69a1">2.5.3. Detaching Computation</a></li>
<li><a href="#org9c66f3a">2.5.4. Computing the Gradient of Clojure Control Flow</a></li>
<li><a href="#org048d748">2.5.5. Summary</a></li>
</ul>
</li>
<li><a href="#org2f76be0">2.6. Probability</a>
<ul>
<li><a href="#org67ae8bc">2.6.1. Basic Probability Theory</a></li>
<li><a href="#org8c4aad2">2.6.2. Axioms of Probability Theory</a></li>
<li><a href="#org59f4cfd">2.6.3. Random Variables</a></li>
<li><a href="#org87cf653">2.6.4. Dealing with Multiple Random Variables</a></li>
<li><a href="#orgf6f8a7d">2.6.5. Joint Probability</a></li>
<li><a href="#org363fa62">2.6.6. Conditional Probability</a></li>
<li><a href="#orgd3731db">2.6.7. Bayes&rsquo; theorem</a></li>
<li><a href="#org98612ae">2.6.8. Marginalization</a></li>
<li><a href="#org2965e5a">2.6.9. Independence</a></li>
<li><a href="#org99a4142">2.6.10. Application</a></li>
<li><a href="#orgd74199c">2.6.11. Expectation and Variance</a></li>
<li><a href="#org039e327">2.6.12. Summary</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgeddee54">3. Linear Neural Networks</a>
<ul>
<li><a href="#lin_reg">3.1. Linear Regression</a>
<ul>
<li><a href="#orge96e1a0">3.1.1. Basic Elements of Linear Regression</a>
<ul>
<li><a href="#org7dd6dc1">3.1.1.1. Linear Model</a></li>
<li><a href="#org78446b1">3.1.1.2. Loss Function</a></li>
<li><a href="#org4785318">3.1.1.3. Analytic Solution</a></li>
<li><a href="#org5005ba1">3.1.1.4. Minibatch Stochastic Gradient Descent</a></li>
<li><a href="#org1fcf043">3.1.1.5. Making Predictions with the Learned Model</a></li>
</ul>
</li>
<li><a href="#orgebbd0ba">3.1.2. Vectorization for Speed</a></li>
<li><a href="#org30ac4aa">3.1.3. The Normal Distribution and Squared Loss</a></li>
<li><a href="#org822f80f">3.1.4. From Linear Regression to Deep Networks</a>
<ul>
<li><a href="#orgcdb607b">3.1.4.1. Neural Network Diagram</a></li>
<li><a href="#orga7ae26c">3.1.4.2. Biology</a></li>
</ul>
</li>
<li><a href="#orga45d194">3.1.5. Summary</a></li>
</ul>
</li>
<li><a href="#lin-reg-scratch">3.2. Linear Regression Implementation from Scratch</a>
<ul>
<li><a href="#orgcf29b3d">3.2.1. Generating the Dataset</a></li>
<li><a href="#orgf5ec15e">3.2.2. Reading the Dataset</a></li>
<li><a href="#orgb2c811b">3.2.3. Initializing Model Parameters</a></li>
<li><a href="#org575f338">3.2.4. Defining the Model</a></li>
<li><a href="#org319ddef">3.2.5. Defining the Loss Function</a></li>
<li><a href="#org89bfe83">3.2.6. Defining the Optimization Algorithm</a></li>
<li><a href="#orgb161f26">3.2.7. Training</a></li>
<li><a href="#orgb56c1a6">3.2.8. Summary</a></li>
</ul>
</li>
<li><a href="#org3f21f99">3.3. Concise Implementation of Linear Regression</a>
<ul>
<li><a href="#org91ddedd">3.3.1. Generating the Dataset</a></li>
<li><a href="#org433afa7">3.3.2. Reading the Dataset</a></li>
</ul>
</li>
<li><a href="#org2976a67">3.4. Defining the Model</a>
<ul>
<li><a href="#orgf26d353">3.4.1. Initializing Model Parameters</a></li>
<li><a href="#orgf7bdd94">3.4.2. Defining the Loss Function</a></li>
<li><a href="#org41fd1a5">3.4.3. Defining the Optimization Algorithm</a></li>
<li><a href="#org44ce3c7">3.4.4. Instantiate Configuration and Trainer</a></li>
<li><a href="#orgf672d83">3.4.5. Initializing Model Parameters</a></li>
<li><a href="#orgb6f16e7">3.4.6. Metrics</a></li>
<li><a href="#orgd102c1f">3.4.7. Training</a></li>
<li><a href="#org0759565">3.4.8. Saving Your Model</a></li>
<li><a href="#org5c69300">3.4.9. Summary</a></li>
</ul>
</li>
<li><a href="#sec-softmax">3.5. Softmax Regression</a></li>
<li><a href="#org9e9632f">3.6. The Image Classification Dataset</a>
<ul>
<li><a href="#orgfa03a78">3.6.1. Getting the Dataset</a></li>
<li><a href="#orgbf85acb">3.6.2. Reading a Minibatch</a></li>
<li><a href="#orgeff911e">3.6.3. Summary</a></li>
</ul>
</li>
<li><a href="#org124da78">3.7. Implementation of Softmax Regression from Scratch</a>
<ul>
<li><a href="#orga5a573c">3.7.1. Initializing Model Parameters</a></li>
<li><a href="#org05c0eef">3.7.2. Defining the Softmax Operation</a></li>
<li><a href="#org295ba2b">3.7.3. Defining the Model</a></li>
<li><a href="#org425aa8b">3.7.4. Defining the Loss Function</a></li>
<li><a href="#orgfd4ce81">3.7.5. Classification Accuracy</a></li>
<li><a href="#org15594fe">3.7.6. Training</a></li>
<li><a href="#orgf659fca">3.7.7. Prediction</a></li>
<li><a href="#org3cb93ff">3.7.8. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgae486b2" class="outline-2">
<h2 id="orgae486b2">Notation</h2>
<div class="outline-text-2" id="text-orgae486b2">
<p>
The notation used throughout this book is summarized below.
</p>
</div>

<div id="outline-container-org6092e37" class="outline-3">
<h3 id="org6092e37">Numbers</h3>
<div class="outline-text-3" id="text-org6092e37">
<ul class="org-ul">
<li>\(x\): A scalar</li>
<li>\(\mathbf{x}\): A vector</li>
<li>\(\mathbf{X}\): A matrix</li>
<li>\(\mathsf{X}\): A tensor</li>
<li>\(\mathbf{I}\): An identity matrix</li>
<li>\(x_i\), \([\mathbf{x}]_i\): The \(i^\mathrm{th}\) element of vector
\(\mathbf{x}\)</li>
<li>\(x_{ij}\), \([\mathbf{X}]_{ij}\): The element of matrix \(\mathbf{X}\)
at row \(i\) and column \(j\)</li>
</ul>
</div>
</div>

<div id="outline-container-org962af57" class="outline-3">
<h3 id="org962af57">Set Theory</h3>
<div class="outline-text-3" id="text-org962af57">
<ul class="org-ul">
<li>\(\mathcal{X}\): A set</li>
<li>\(\mathbb{Z}\): The set of integers</li>
<li>\(\mathbb{R}\): The set of real numbers</li>
<li>\(\mathbb{R}^n\): The set of $n$-dimensional vectors of real numbers</li>
<li>\(\mathbb{R}^{a\times b}\): The set of matrices of real numbers with
\(a\) rows and \(b\) columns</li>
<li>\(\mathcal{A}\cup\mathcal{B}\): Union of sets \(\mathcal{A}\) and
\(\mathcal{B}\)</li>
<li>\(\mathcal{A}\cap\mathcal{B}\): Intersection of sets \(\mathcal{A}\)
and \(\mathcal{B}\)</li>
<li>\(\mathcal{A}\setminus\mathcal{B}\): Subtraction of set
\(\mathcal{B}\) from set \(\mathcal{A}\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgf021315" class="outline-3">
<h3 id="orgf021315">Functions and Operators</h3>
<div class="outline-text-3" id="text-orgf021315">
<ul class="org-ul">
<li>\(f(\cdot)\): A function</li>
<li>\(\log(\cdot)\): The natural logarithm</li>
<li>\(\exp(\cdot)\): The exponential function</li>
<li>\(\mathbf{1}_\mathcal{X}\): The indicator function</li>
<li>\(\mathbf{(\cdot)}^\top\): Transpose of a vector or a matrix</li>
<li>\(\mathbf{X}^{-1}\): Inverse of matrix \(\mathbf{X}\)</li>
<li>\(\odot\): Hadamard (elementwise) product</li>
<li>\([\cdot, \cdot]\): Concatenation</li>
<li>\(\lvert \mathcal{X} \rvert\): Cardinality of set \(\mathcal{X}\)</li>
<li>\(\|\cdot\|_p\): \(\ell_p\) norm</li>
<li>\(\|\cdot\|\): \(\ell_2\) norm</li>
<li>\(\langle \mathbf{x}, \mathbf{y} \rangle\): Dot product of vectors
\(\mathbf{x}\) and \(\mathbf{y}\)</li>
<li>\(\sum\): Series addition</li>
<li>\(\prod\): Series multiplication</li>
</ul>
</div>
</div>

<div id="outline-container-org516c3d9" class="outline-3">
<h3 id="org516c3d9">Calculus</h3>
<div class="outline-text-3" id="text-org516c3d9">
<ul class="org-ul">
<li>\(\frac{dy}{dx}\): Derivative of \(y\) with respect to \(x\)</li>
<li>\(\frac{\partial y}{\partial x}\): Partial derivative of \(y\) with
respect to \(x\)</li>
<li>\(\nabla_{\mathbf{x}} y\): Gradient of \(y\) with respect to
\(\mathbf{x}\)</li>
<li>\(\int_a^b f(x) \;dx\): Definite integral of \(f\) from \(a\) to \(b\)
with respect to \(x\)</li>
<li>\(\int f(x) \;dx\): Indefinite integral of \(f\) with respect to \(x\)</li>
</ul>
</div>
</div>

<div id="outline-container-org94e6d9d" class="outline-3">
<h3 id="org94e6d9d">Probability and Information Theory</h3>
<div class="outline-text-3" id="text-org94e6d9d">
<ul class="org-ul">
<li>\(P(\cdot)\): Probability distribution</li>
<li>\(z \sim P\): Random variable \(z\) has probability distribution \(P\)</li>
<li>\(P(X \mid Y)\): Conditional probability of \(X \mid Y\)</li>
<li>\(p(x)\): Probability density function</li>
<li>\({E}_{x} [f(x)]\): Expectation of \(f\) with respect to \(x\)</li>
<li>\(X \perp Y\): Random variables \(X\) and \(Y\) are independent</li>
<li>\(X \perp Y \mid Z\): Random variables \(X\) and \(Y\) are conditionally
independent given random variable \(Z\)</li>
<li>\(\mathrm{Var}(X)\): Variance of random variable \(X\)</li>
<li>\(\sigma_X\): Standard deviation of random variable \(X\)</li>
<li>\(\mathrm{Cov}(X, Y)\): Covariance of random variables \(X\) and \(Y\)</li>
<li>\(\rho(X, Y)\): Correlation of random variables \(X\) and \(Y\)</li>
<li>\(H(X)\): Entropy of random variable \(X\)</li>
<li>\(D_{\mathrm{KL}}(P\|Q)\): KL-divergence of distributions \(P\) and
\(Q\)</li>
</ul>
</div>
</div>

<div id="outline-container-org0da6d12" class="outline-3">
<h3 id="org0da6d12">Complexity</h3>
<div class="outline-text-3" id="text-org0da6d12">
<ul class="org-ul">
<li>\(\mathcal{O}\): Big O notation</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org70709e1" class="outline-2">
<h2 id="org70709e1"><span class="section-number-2">1.</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Refer to: <a href="http://d2l.ai/chapter_introduction/index.html">http://d2l.ai/chapter_introduction/index.html</a>
</p>
</div>
</div>
<div id="outline-container-org2f5c4d6" class="outline-2">
<h2 id="org2f5c4d6"><span class="section-number-2">2.</span> Preliminaries</h2>
<div class="outline-text-2" id="text-2">
<p>
To get started with deep learning, we will need to develop a few basic
skills. All machine learning is concerned with extracting information
from data. So we will begin by learning the practical skills for
storing, manipulating, and preprocessing data.
</p>

<p>
Moreover, machine learning typically requires working with large
datasets, which we can think of as tables, where the rows correspond
to examples and the columns correspond to attributes. Linear algebra
gives us a powerful set of techniques for working with tabular
data. We will not go too far into the weeds but rather focus on the
basic of matrix operations and their implementation.
</p>

<p>
Additionally, <b>deep learning is all about optimization</b>. We have a
model with some parameters and we want to find those that fit our
data the best. Determining which way to move each parameter at each
step of an algorithm requires a little bit of calculus, which will
be briefly introduced. Fortunately, the autograd package
automatically computes differentiation for us, and we will cover it
next.
</p>

<p>
Next, <b>machine learning is concerned with making predictions</b>: what is
the likely value of some unknown attribute, given the information
that we observe? To reason rigorously under uncertainty we will need
to invoke the language of probability.
</p>

<p>
In the end, the official documentation provides plenty of
descriptions and examples that are beyond this book. To conclude the
chapter, we will show you how to look up documentation for the
needed information.
</p>

<p>
This book has kept the mathematical content to the minimum necessary
to get a proper understanding of deep learning. However, it does not
mean that this book is mathematics free. Thus, this chapter provides
a rapid introduction to basic and frequently-used mathematics to
allow anyone to understand at least most of the mathematical content
of the book. If you wish to understand all of the mathematical
content, further reviewing the online appendix on mathematics should
be sufficient.
</p>
</div>

<div id="outline-container-orgad8cd02" class="outline-3">
<h3 id="orgad8cd02"><span class="section-number-3">2.1.</span> Data Manipulation</h3>
<div class="outline-text-3" id="text-2-1">
<p>
In order to get anything done, we need some way to store and
manipulate data. Generally, there are two important things we need
to do with data: (i) acquire them; and (ii) process them once they
are inside the computer. There is no point in acquiring data without
some way to store it, so let us get our hands dirty first by playing
with synthetic data. To start, we introduce the $n$-dimensional
array, which is also called the <i>ndarray</i>.
</p>

<p>
If you have worked with NumPy, the most widely-used scientific
computing package in Python, then you will find this section
familiar. No matter which framework you use, its tensor class
(<i>ndarray</i> in MXNet, DJL and clj-djl, <i>Tensor</i> in both PyTorch and
TensorFlow) is similar to NumPy&rsquo;s ndarray with a few killer
features. First, GPU is well-supported to accelerate the computation
whereas NumPy only supports CPU computation. Second, the tensor
class supports automatic differentiation. These properties make the
tensor class suitable for deep learning. Throughout the book, when
we say ndarrays, we are referring to instances of the ndarray class
unless otherwise stated.
</p>
</div>

<div id="outline-container-orgac49908" class="outline-4">
<h4 id="orgac49908"><span class="section-number-4">2.1.1.</span> Getting Started</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
In this section, we aim to get you up and running, equipping you
with the basic math and numerical computing tools that you will
build on as you progress through the book. Do not worry if you
struggle to grok some of the mathematical concepts or library
functions. The following sections will revisit this material in the
context of practical examples and it will sink. On the other hand,
if you already have some background and want to go deeper into the
mathematical content, just skip this section.
</p>

<p>
To start, we import the ndarray namespace from clj-djl. Here, the
ndarray namespace includes functions supported by clj-djl.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.data-manipulation</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
An ndarray represents a (possibly multi-dimensional) array of
numerical values. With one axis, an ndarray corresponds (in math) to
a vector. With two axes, an ndarray corresponds to a
matrix. NDArrays with more than two axes do not have special
mathematical names.
</p>

<p>
To start, we can use arange to create a row vector x containing the
first 12 integers starting with 0. Each of the values in an ndarray
is called an element of the ndarray. For instance, there are 12
elements in the ndarray x. Unless otherwise specified, a new ndarray
will be stored in main memory and designated for CPU-based
computation.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">x</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 0 12<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
x
</pre>
</div>

<pre class="example">
ND: (12) cpu() int32
[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]
</pre>


<p>
We can access an ndarray&rsquo;s shape (the length along each axis) by
inspecting its shape property.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(12)
</pre>


<p>
If we just want to know the total number of elements in an ndarray,
i.e., the product of all of the shape elements, we can inspect its
size. Because we are dealing with a vector here, the single element
of its shape is same to its size. The difference is that <code>shape</code>
will return a <i>Shape</i> object.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
12
</pre>


<p>
To change the shape of an ndarray without altering either the number
of elements or their values, we can invoke the <code>reshape</code> function. For
example, we can transform our ndarray, x, from a row vector with
shape (12) to a matrix with shape (3, 4). This new ndarray contains
the exact same values, but views them as a matrix organized as 3
rows and 4 columns. To reiterate, although the shape has changed,
the elements have not. Note that the size is unaltered by reshaping.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape x <span style="color: #909183;">[</span>3 4<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
y
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int32
[[ 0,  1,  2,  3],
 [ 4,  5,  6,  7],
 [ 8,  9, 10, 11],
]
</pre>


<p>
Reshaping by manually specifying every dimension is unnecessary. If
our target shape is a matrix with shape (height, width), then after we
know the width, the height is given implicitly. Why should we have to
perform the division ourselves? In the example above, to get a matrix
with 3 rows, we specified both that it should have 3 rows and 4
columns. Fortunately, ndarrays can automatically work out one
dimension given the rest. We invoke this capability by placing -1 for
the dimension that we would like ndarrays to automatically infer. In
our case, instead of calling <code>(reshape x [3 4])</code>, we could have
equivalently called <code>(nd/reshape x [-1 4])</code> or <code>(nd/reshape x [3 -1])</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape x <span style="color: #909183;">[</span>3 -1<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
y
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int32
[[ 0,  1,  2,  3],
 [ 4,  5,  6,  7],
 [ 8,  9, 10, 11],
]
</pre>


<p>
Passing create method with only Shape will grab a chunk of memory and
hands us back a matrix without bothering to change the value of any of
its entries. This is remarkably efficient but we must be careful
because the entries might take arbitrary values, including very big
ones!
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape <span style="color: #909183;">[</span>3 4<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() float32
[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
 [ 1.21782351e+29,  0.00000000e+00,  1.21783258e+29,  0.00000000e+00],
 [ 1.19888078e+29,  0.00000000e+00,  1.21744119e+29,  0.00000000e+00],
]
</pre>


<p>
Typically, we will want our matrices initialized either with zeros,
ones, some other constants, or numbers randomly sampled from a
specific distribution. We can create a ndarray representing a tensor
with all elements set to 0 and a shape of <code>[2 3 4]</code> as follows:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/zeros ndm <span style="color: #7388d6;">[</span>2 3 4<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example" id="org5754678">
ND: (2, 3, 4) cpu() float32
[[[0., 0., 0., 0.],
  [0., 0., 0., 0.],
  [0., 0., 0., 0.],
 ],
 [[0., 0., 0., 0.],
  [0., 0., 0., 0.],
  [0., 0., 0., 0.],
 ],
]
</pre>

<p>
Similarly, we can create ndarrays with each element set to 1 as follows:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/ones ndm <span style="color: #7388d6;">[</span>2 3 4<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example" id="org7ff3110">
ND: (2, 3, 4) cpu() float32
[[[1., 1., 1., 1.],
  [1., 1., 1., 1.],
  [1., 1., 1., 1.],
 ],
 [[1., 1., 1., 1.],
  [1., 1., 1., 1.],
  [1., 1., 1., 1.],
 ],
]
</pre>

<p>
Often, we want to randomly sample the values for each element in an
ndarray from some probability distribution. For example, when we
construct arrays to serve as parameters in a neural network, we will
typically initialize their values randomly. The following snippet
creates an ndarray with shape (3, 4). Each of its elements is randomly
sampled from a standard Gaussian (normal) distribution with a mean of
0 and a standard deviation of 1.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm 0 1 <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape <span style="color: #909183;">[</span>3 4<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() float32
[[ 1.1631,  2.2122,  0.4838,  0.774 ],
 [ 0.2996,  1.0434,  0.153 ,  1.1839],
 [-1.1688,  1.8917,  1.5581, -1.2347],
]
</pre>


<p>
We can directly use a clojure vec as the shape:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm 0 1 <span style="color: #7388d6;">[</span>3 4<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() float32
[[-0.5459, -1.771 , -2.3556, -0.4514],
 [ 0.5414,  0.5794,  2.6785, -1.8561],
 [ 1.2546, -1.9769, -0.5488, -0.208 ],
]
</pre>


<p>
You can also just pass the shape and it will use default values for mean and
standard deviation (0 and 1).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm <span style="color: #7388d6;">[</span>3 4<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() float32
[[-0.6811,  0.2444, -0.1353, -0.0372],
 [ 0.3772, -0.4877,  0.4102, -0.0226],
 [ 0.5713,  0.5746, -2.758 ,  1.4661],
]
</pre>


<p>
We can also specify the exact values for each element in the desired ndarray by
supplying a clojure vec (or list) containing the numerical values. Here, the
outermost list corresponds to axis 0, and the inner list to axis 1.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #7388d6;">[</span>2 1 4 3 1 2 3 4 4 3 2 1<span style="color: #7388d6;">]</span> <span style="color: #7388d6;">[</span>3 4<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int64
[[ 2,  1,  4,  3],
 [ 1,  2,  3,  4],
 [ 4,  3,  2,  1],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #7388d6;">[</span><span style="color: #909183;">[</span>2 1 4 3<span style="color: #909183;">][</span>1 2 3 4<span style="color: #909183;">][</span>4 3 2 1<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int64
[[ 2,  1,  4,  3],
 [ 1,  2,  3,  4],
 [ 4,  3,  2,  1],
]
</pre>
</div>
</div>

<div id="outline-container-org26d0a53" class="outline-4">
<h4 id="org26d0a53"><span class="section-number-4">2.1.2.</span> Operations</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
This book is not about software engineering. Our interests are not
limited to simply reading and writing data from/to arrays. We want to
perform mathematical operations on those arrays. Some of the simplest
and most useful operations are the elementwise operations. These apply
a standard scalar operation to each element of an array. For functions
that take two arrays as inputs, elementwise operations apply some
standard binary operator on each pair of corresponding elements from
the two arrays. We can create an elementwise function from any
function that maps from a scalar to a scalar.
</p>

<p>
In mathematical notation, we would denote such a unary scalar operator
(taking one input) by the signature \(f: \mathbb{R} \ rightarrow
\mathbb{R}\). This just means that the function is mapping from any
real number (\(\mathbb{R}\)) onto another. Likewise, we denote a binary
scalar operator (taking two real inputs, and yielding one output) by
the signature \(f: \mathbb{R}, \mathbb{R} \rightarrow
\mathbb{R}\). Given any two vectors \(\mathbf{u}\) and \(\mathbf{v}\) <b>of
the same shape</b>, and a binary operator \(f\), we can produce a vector
\(\mathbf{c} = F(\mathbf{u}, \mathbf{v})\) by setting \(c_i \gets f(u_i,
v_i)\) for all \(i\), where \(c_i, u_i\), and \(v_i\) are the \(i^\mathrm{th}\)
elements of vectors \(\mathbf{c}\), \(\mathbf{u}\), and
\(\mathbf{v}\). Here, we produced the vector-valued \(F: \mathbb{R}^d,
\mathbb{R}^d \rightarrow \mathbb{R}^d\) by <b>lifting</b> the scalar function
to an elementwise vector operation.
</p>

<p>
The common standard arithmetic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>) have all been
<b>lifted</b> to elementwise operations for any identically-shaped ndarrays
of arbitrary shape. We can call elementwise operations on any two
ndarrays of the same shape. In the following example, we use commas to
formulate a 5-element tuple, where each element is the result of an
elementwise operation.
</p>
</div>

<div id="outline-container-org375da40" class="outline-5">
<h5 id="org375da40"><span class="section-number-5">2.1.2.1.</span> Operations</h5>
<div class="outline-text-5" id="text-2-1-2-1">
<p>
The common standard arithmetic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>) have all been
lifted to elementwise operations.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">x</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span>1. 2. 4. 8.<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span>2. 2. 2. 2.<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float64
[ 3.,  4.,  6., 10.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float64
[-1.,  0.,  2.,  6.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float64
[0.5, 1. , 2. , 4. ]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/pow x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float64
[ 1.,  4., 16., 64.]
</pre>


<p>
Many more operations can be applied elementwise, including unary
operators like exponentiation.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/exp x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float64
[ 2.71828183e+00,  7.38905610e+00,  5.45981500e+01,  2.98095799e+03]
</pre>


<p>
In addition to elementwise computations, we can also perform linear
algebra operations, including vector dot products and matrix
multiplication. We will explain the crucial bits of linear algebra
(with no assumed prior knowledge) in -Section 2.3-.
</p>

<p>
We can also concatenate multiple ndarrays together, stacking them
end-to-end to form a larger ndarray. We just need to provide a list of
ndarrays and tell the system along which axis to concatenate. The
example below shows what happens when we concatenate two matrices
along rows (axis 0, the first element of the shape) vs. columns (axis
1, the second element of the shape). We can see that the first output
ndarray&rsquo;s shape is (6, 4), its axis-0 length (6) is the sum of the two
input ndarrays&rsquo; axis-0 lengths \((3+3)\); while the second output
ndarray&rsquo;s shape is (3, 8), its axis-1 length (8) is the sum of the two
input ndarrays&rsquo; axis-1 lengths \((4+4)\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">X</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 12<span style="color: #909183;">)</span>
           <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape <span style="color: #709870;">[</span>3 4<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
X
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int32
[[ 0,  1,  2,  3],
 [ 4,  5,  6,  7],
 [ 8,  9, 10, 11],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span><span style="color: #709870;">[</span>2 1 4 3<span style="color: #709870;">][</span>1 2 3 4<span style="color: #709870;">][</span>4 3 2 1<span style="color: #709870;">]</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
Y
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int64
[[ 2,  1,  4,  3],
 [ 1,  2,  3,  4],
 [ 4,  3,  2,  1],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">concat only support int32 and float32 datatype</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-type Y <span style="color: #110099;">:int32</span> <span style="color: #110099;">false</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">concat</span> Y Y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (6, 4) cpu() int32
[[ 2,  1,  4,  3],
 [ 1,  2,  3,  4],
 [ 4,  3,  2,  1],
 [ 2,  1,  4,  3],
 [ 1,  2,  3,  4],
 [ 4,  3,  2,  1],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">concat</span> X Y 1<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 8) cpu() int32
[[ 0,  1,  2,  3,  2,  1,  4,  3],
 [ 4,  5,  6,  7,  1,  2,  3,  4],
 [ 8,  9, 10, 11,  4,  3,  2,  1],
]
</pre>


<p>
The third argument of <code>nd/concat</code> is to specify the axis to concatenate,
default is axis-0.
</p>

<p>
Sometimes, we want to construct a binary ndarray via logical
statements. Take <code>(nd/= X Y)</code> as an example. For each position, if X and
Y are equal at that position, the corresponding entry in the new
tensor takes a value of <code>true</code>, meaning that the logical statement <code>(nd/=
X Y)</code> is true at that position; otherwise that position takes <code>false</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/= X Y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() boolean
[[false,  true, false,  true],
 [false, false, false, false],
 [false, false, false, false],
]
</pre>


<p>
Summing all the elements in the ndarray yields a ndarray with only one
element.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum X<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() int32
66
</pre>
</div>
</div>
</div>

<div id="outline-container-orgfea52dd" class="outline-4">
<h4 id="orgfea52dd"><span class="section-number-4">2.1.3.</span> Broadcasting Mechanism</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
In the above section, we saw how to perform elementwise operations on two
ndarrays of the same shape. Under certain conditions, even when shapes differ,
we can still perform elementwise operations by invoking the broadcasting
mechanism. This mechanism works in the following way: First, expand one or both
arrays by copying elements appropriately so that after this transformation, the
two ndarrays have the same shape. Second, carry out the elementwise operations
on the resulting arrays.
</p>

<p>
In most cases, we broadcast along an axis where an array initially only has
length 1, such as in the following example:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">a</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 3<span style="color: #909183;">)</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape <span style="color: #709870;">[</span>3 1<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
a
</pre>
</div>

<pre class="example">
ND: (3, 1) cpu() int32
[[ 0],
 [ 1],
 [ 2],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">b</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 2<span style="color: #909183;">)</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape <span style="color: #709870;">[</span>1 2<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
b
</pre>
</div>

<pre class="example">
ND: (1, 2) cpu() int32
[[ 0,  1],
]
</pre>


<p>
Since a and b are \(3 \times 1\) and \(1 \times 2\) matrices respectively,
their shapes do not match up if we want to add them. We broadcast the
entries of both matrices into a larger \(3 \times 2\) matrix as follows:
for matrix a it replicates the columns and for matrix b it replicates
the rows before adding up both elementwise.
</p>

<p>
The result of \(a\) broadcasted is:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">concat</span> a a 1<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 2) cpu() int32
[[ 0,  0],
 [ 1,  1],
 [ 2,  2],
]
</pre>


<p>
The result of \(b\) broadcasted is:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;&gt;</span> b
     <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">concat</span> b<span style="color: #7388d6;">)</span>
     <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">concat</span> b<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 2) cpu() int32
[[ 0,  1],
 [ 0,  1],
 [ 0,  1],
]
</pre>


<p>
Thus the result is:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ a b<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 2) cpu() int32
[[ 0,  1],
 [ 1,  2],
 [ 2,  3],
]
</pre>
</div>
</div>

<div id="outline-container-org47736b6" class="outline-4">
<h4 id="org47736b6"><span class="section-number-4">2.1.4.</span> Indexing and Slicing</h4>
<div class="outline-text-4" id="text-2-1-4">
<p>
Just as in any other Python array, elements in a ndarray can be
accessed by index. As in any Python array, the first element has index
0 and ranges are specified to include the first but before the last
element. As in standard Python lists, we can access elements according
to their relative position to the end of the list by using negative
indices.
</p>

<p>
Java and Clojure do not support <code>operator[]</code> overload, a simulation is
done with index and slice string.
</p>

<div class="org-src-container">
<pre class="src src-clojure">X
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int32
[[ 0,  1,  2,  3],
 [ 4,  5,  6,  7],
 [ 8,  9, 10, 11],
]
</pre>


<p>
Thus, [-1] selects the last element and [1:3] selects the second and the third
elements as follows:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> X <span style="color: #2A00FF;">"-1"</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() int32
[ 8,  9, 10, 11]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> X <span style="color: #2A00FF;">"1:3"</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (2, 4) cpu() int32
[[ 4,  5,  6,  7],
 [ 8,  9, 10, 11],
]
</pre>


<p>
Beyond reading, we can also set elements of a matrix by specifying indices.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">set</span> X <span style="color: #2A00FF;">"1,2"</span> 999<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int32
[[  0,   1,   2,   3],
 [  4,   5, 999,   7],
 [  8,   9,  10,  11],
]
</pre>


<p>
If we want to assign multiple elements the same value, we simply index all of
them and then assign them the value. For instance, [0:2, :] accesses the first
and second rows, where : takes all the elements along axis 1 (column). While we
discussed indexing for matrices, this obviously also works for vectors and for
tensors of more than 2 dimensions.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">set</span> X <span style="color: #2A00FF;">"0:2,:"</span> 12<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int32
[[12, 12, 12, 12],
 [12, 12, 12, 12],
 [ 8,  9, 10, 11],
]
</pre>
</div>
</div>

<div id="outline-container-org83ad0dd" class="outline-4">
<h4 id="org83ad0dd"><span class="section-number-4">2.1.5.</span> Saving Memory</h4>
<div class="outline-text-4" id="text-2-1-5">
<p>
Running operations can cause new memory to be allocated to host
results. For example, if we write <code>(def Y2 (nd/+! X Y)</code>, we will
dereference the ndarray that Y used to point to and instead point Y at
the newly allocated memory. In the following example, we demonstrate
this with Clojure&rsquo;s <code>identical?</code> function, which results <code>true</code> if the two
object are exactly the same. After running Y&rsquo; = Y + X, we will find
that Y and Y&rsquo; are different objects. That is because Clojure first
evaluates Y + X, allocating new memory for the result and then makes Y
point to this new location in memory.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/zeros ndm <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-shape X<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span>' <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ Y X<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">identical?</span> Y Y'<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
false
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span>'' <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+! Y X<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">identical?</span> Y Y''<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
true
</pre>


<p>
Running operations can cause new memory to be allocated to host
results. For example, if we write y = x.add(y), we will dereference
the ndarray that y used to point to and instead point y at the newly
allocated memory.
</p>

<p>
This might be undesirable for two reasons. First, we do not want to
run around allocating memory unnecessarily all the time. In machine
learning, we might have hundreds of megabytes of parameters and update
all of them multiple times per second. Typically, we will want to
perform these updates in place. Second, we might point at the same
parameters from multiple variables. If we do not update in place,
other references will still point to the old memory location, making
it possible for parts of our code to inadvertently reference stale
parameters.
</p>

<p>
Fortunately, performing in-place operations in DJL is easy. We can
assign the result of an operation to a previously allocated array
using inplace operators like addi, subi, muli, and divi.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/zeros ndm <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-shape X<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span>' <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ Y X<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">identical?</span> Y Y'<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
false
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span>'' <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+! Y X<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">identical?</span> Y Y''<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
true
</pre>
</div>
</div>

<div id="outline-container-org2e1a0f5" class="outline-4">
<h4 id="org2e1a0f5"><span class="section-number-4">2.1.6.</span> Conversion to Other Clojure Objects</h4>
<div class="outline-text-4" id="text-2-1-6">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">type</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec X<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
class clojure.lang.PersistentVector
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec X<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
[12 12 12 12 12 12 12 12 8 9 10 11]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>type <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">to-array</span> X<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
class [Ljava.lang.Integer;
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">type</span> X<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
class ai.djl.mxnet.engine.MxNDArray
</pre>


<div class="org-src-container">
<pre class="src src-clojure">X
</pre>
</div>

<pre class="example">
ND: (3, 4) cpu() int32
[[12, 12, 12, 12],
 [12, 12, 12, 12],
 [ 8,  9, 10, 11],
]
</pre>


<p>
To convert a size-1 tensor to a scalar
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">a</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span>3.5<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
a
</pre>
</div>

<pre class="example">
ND: (1) cpu() float64
[3.5]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element a<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
3.5
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf7a92b1" class="outline-3">
<h3 id="orgf7a92b1"><span class="section-number-3">2.2.</span> Data Preprocessing</h3>
<div class="outline-text-3" id="text-2-2">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.data-preprocess</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.dataframe <span style="color: #110099;">:as</span> df<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.dataframe.functional <span style="color: #110099;">:as</span> functional<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>

<div id="outline-container-org4b66757" class="outline-4">
<h4 id="org4b66757"><span class="section-number-4">2.2.1.</span> Reading the Dataset</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
As an example, we begin by creating an artificial dataset that is
stored in a csv (comma-separated values) file
../data/house<sub>tiny.csv</sub>. Data stored in other formats may be processed
in similar ways.
</p>

<p>
Below we write the dataset row by row into a csv file.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>filename <span style="color: #2A00FF;">"data/house_tiny.csv"</span>
      records <span style="color: #909183;">[</span><span style="color: #2A00FF;">"NumRooms,Alley,Price</span><span style="color: #000000; background-color: #f8f8f8; font-weight: bold;">\n</span><span style="color: #2A00FF;">"</span>  <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">Column names</span>
               <span style="color: #2A00FF;">"NA,Pave,127500</span><span style="color: #000000; background-color: #f8f8f8; font-weight: bold;">\n</span><span style="color: #2A00FF;">"</span>        <span style="color: #3F7F5F;">;;</span><span style="color: #3F7F5F;">Each row represents a data example</span>
               <span style="color: #2A00FF;">"2,NA,106000</span><span style="color: #000000; background-color: #f8f8f8; font-weight: bold;">\n</span><span style="color: #2A00FF;">"</span>
               <span style="color: #2A00FF;">"4,NA,178100</span><span style="color: #000000; background-color: #f8f8f8; font-weight: bold;">\n</span><span style="color: #2A00FF;">"</span>
               <span style="color: #2A00FF;">"NA,NA,140000</span><span style="color: #000000; background-color: #f8f8f8; font-weight: bold;">\n</span><span style="color: #2A00FF;">"</span><span style="color: #909183;">]</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">io</span>/make-parents filename<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">dorun</span>
   <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">spit</span> filename <span style="color: #000000;">%</span> <span style="color: #110099;">:append</span> <span style="color: #110099;">true</span><span style="color: #709870;">)</span> records<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">slurp</span> filename<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
NumRooms,Alley,Price
NA,Pave,127500
2,NA,106000
4,NA,178100
NA,NA,140000
</pre>


<p>
To load the raw dataset from the created csv file, we require the
<code>clj-djl.dataframe</code> package and invoke the read function to read
directly from the csv we created. This dataset has four rows and three
columns, where each row describes the number of rooms (NumRooms),
the alley type (Alley), and the price (Price) of a house.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">data</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/dataframe <span style="color: #2A00FF;">"data/house_tiny.csv"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
data
</pre>
</div>

<pre class="example">
data/house_tiny.csv [4 3]:

| NumRooms | Alley |  Price |
|---------:|-------|-------:|
|          |  Pave | 127500 |
|        2 |       | 106000 |
|        4 |       | 178100 |
|          |       | 140000 |
</pre>
</div>
</div>

<div id="outline-container-orgd4a77d0" class="outline-4">
<h4 id="orgd4a77d0"><span class="section-number-4">2.2.2.</span> Handling Missing Data</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
Note that there are some blank spaces which are missing values. To
handle missing data, typical methods include imputation and deletion,
where imputation replaces missing values with substituted ones, while
deletion ignores missing values. Here we will consider imputation.
</p>

<p>
We split the data into inputs and outputs by creating new dataframes
and specifying the columns desired, where the former takes the first
two columns while the latter only keeps the last column. For numerical
values in inputs that are missing, we replace the missing data entries
with the mean value of the same column.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">dataframe</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>data <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/replace-missing
              data <span style="color: #907373;">[</span><span style="color: #2A00FF;">"NumRooms"</span><span style="color: #907373;">]</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">functional</span>/mean<span style="color: #709870;">)</span>
        data <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/update-column
              data <span style="color: #2A00FF;">"Alley_nan"</span>
              <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">if</span> <span style="color: #858580;">(</span><span style="color: #7F0055; font-weight: bold;">nil?</span> <span style="color: #000000;">%</span><span style="color: #858580;">)</span> 1 0<span style="color: #6276ba;">)</span> <span style="color: #6276ba;">(</span>data <span style="color: #2A00FF;">"Alley"</span><span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span>
        data <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/update-column
              data <span style="color: #2A00FF;">"Alley_Pave"</span>
              <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">if</span> <span style="color: #858580;">(</span><span style="color: #7F0055; font-weight: bold;">some?</span> <span style="color: #000000;">%</span><span style="color: #858580;">)</span> 1 0<span style="color: #6276ba;">)</span> <span style="color: #6276ba;">(</span>data <span style="color: #2A00FF;">"Alley"</span><span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span>
        inputs <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/select-columns
                data <span style="color: #907373;">[</span><span style="color: #2A00FF;">"NumRooms"</span> <span style="color: #2A00FF;">"Alley_Pave"</span> <span style="color: #2A00FF;">"Alley_nan"</span><span style="color: #907373;">]</span><span style="color: #709870;">)</span>
        outputs <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/select-columns
                 data <span style="color: #907373;">[</span><span style="color: #2A00FF;">"Price"</span><span style="color: #907373;">]</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">[</span>inputs outputs<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> dataframe<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
data/house_tiny.csv [4 3]:

| NumRooms | Alley_Pave | Alley_nan |
|---------:|-----------:|----------:|
|        3 |          1 |         0 |
|        2 |          0 |         1 |
|        4 |          0 |         1 |
|        3 |          0 |         1 |
</pre>



<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> dataframe<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
data/house_tiny.csv [4 1]:

|  Price |
|-------:|
| 127500 |
| 106000 |
| 178100 |
| 140000 |
</pre>
</div>
</div>


<div id="outline-container-orge717448" class="outline-4">
<h4 id="orge717448"><span class="section-number-4">2.2.3.</span> Conversion to the Tensor Format</h4>
<div class="outline-text-4" id="text-2-2-3">
<p>
Now that all the entries in inputs and outputs are numerical, they can
be converted to the NDArray format. Once data are in this format, they
can be further manipulated with those NDArray functionalities that we
have introduced in Section 2.1.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/new-base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">X</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/-&gt;ndarray ndm <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> dataframe<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">Y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">df</span>/-&gt;ndarray ndm <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> dataframe<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
X
</pre>
</div>

<pre class="example">
ND: (4, 3) cpu() int32
[[ 3,  1,  0],
 [ 2,  0,  1],
 [ 4,  0,  1],
 [ 3,  0,  1],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure">Y
</pre>
</div>

<pre class="example">
ND: (4, 1) cpu() int64
[[127500],
 [106000],
 [178100],
 [140000],
]
</pre>
</div>
</div>

<div id="outline-container-org2c601c2" class="outline-4">
<h4 id="org2c601c2"><span class="section-number-4">2.2.4.</span> Summary</h4>
<div class="outline-text-4" id="text-2-2-4">
<ul class="org-ul">
<li>Like many other extension packages in the vast ecosystem of clojure,
<code>clj-djl.dataframe</code> can work together with NDArray.</li>
<li>Imputation and deletion can be used to handle missing data.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orge3dd06c" class="outline-3">
<h3 id="orge3dd06c"><span class="section-number-3">2.3.</span> Linear Algebra</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Now that you can store and manipulate data, let us briefly review the
subset of basic linear algebra that you will need to understand and
implement most of models covered in this book. Below, we introduce the
basic mathematical objects, arithmetic, and operations in linear
algebra, expressing each of them through mathematical notation and the
corresponding implementation in code.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.linear-algebra</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>

<div id="outline-container-orga23896d" class="outline-4">
<h4 id="orga23896d"><span class="section-number-4">2.3.1.</span> Scalars</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
If you never studied linear algebra or machine learning, then your
past experience with math probably consisted of thinking about one
number at a time. And, if you ever balanced a checkbook or even paid
for dinner at a restaurant then you already know how to do basic
things like adding and multiplying pairs of numbers. For example, the
temperature in Palo Alto is \(52\) degrees Fahrenheit. Formally, we call
values consisting of just one numerical quantity <i>scalars</i>. If you
wanted to convert this value to Celsius (the metric system&rsquo;s more
sensible temperature scale), you would evaluate the expression
\(c=\frac{5}{9}(f32)\), setting \(f\) to \(52\). In this equation, each of
the terms  \(5\), \(9\), and \(32\)  are scalar values. The placeholders
\(c\) and \(f\) are called <i>variables</i> and they represent unknown scalar
values. We denote the space of all (continuous) real-valued scalars by
\(\mathbb{R}\).
</p>

<p>
In this book, we adopt the mathematical notation where scalar
variables are denoted by ordinary lower-cased letters (e.g., \(x\), \(y\),
and \(z\)). For expedience, we will punt on rigorous definitions of what
precisely <b>space</b> is, but just remember for now that the expression \(x
\in \mathbb{R}\) is a formal way to say that \(x\) is a <i>real-valued
scalar</i>. The symbol \(\in\) can be pronounced &ldquo;in&rdquo; and simply denotes
membership in a set. Analogously, we could write \(x, y \in \{0,1\}\) to
state that \(x\) and \(y\) are numbers whose value can only be \(0\) or \(1\).
</p>

<p>
A scalar is represented by a NDArray with just one element. In the
next snippet, we instantiate two scalars and perform some familiar
arithmetic operations with them, namely addition, multiplication,
division, and exponentiation.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">x</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm 3.<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm 2.<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float64
5.
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float64
6.
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float64
1.5
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/pow x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float64
9.
</pre>
</div>
</div>

<div id="outline-container-orgbea84b2" class="outline-4">
<h4 id="orgbea84b2"><span class="section-number-4">2.3.2.</span> Vectors</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
You can think of a vector as simply a list of scalar values. We call
these values the <i>elements</i> (<i>entries</i> or <i>components</i>) of the vector. When
our vectors represent examples from our dataset, their values hold
some real-world significance. For example, if we were training a model
to predict the risk that a loan defaults, we might associate each
applicant with a vector whose components correspond to their income,
length of employment, number of previous defaults, and other
factors. If we were studying the risk of heart attacks hospital
patients potentially face, we might represent each patient by a vector
whose components capture their most recent vital signs, cholesterol
levels, minutes of exercise per day, etc. In math notation, we will
usually denote vectors as bold-faced, lower-cased letters (e.g.,
\(\mathbf{x}\), \(\mathbf{y}\), and \(\mathbf{z}\).
</p>

<p>
We work with vectors via one-dimensional NDArrays. In general NDArrays
can have arbitrary lengths, subject to the memory limits of your
machine.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">x</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 4.<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
x
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[0., 1., 2., 3.]
</pre>


<p>
We can refer to any element of a vector by using a subscript. For
example, we can refer to the i<sup>th</sup> element of \(\mathbf{x}\) by
\(x_i\). Note that the element \(x_i\) is a scalar, so we do not bold-face
the font when referring to it. Extensive literature considers column
vectors to be the default orientation of vectors, so does this
book. In math, a vector \(\mathbf{x}\) can be written as
</p>

\begin{equation}
\mathbf{x} =  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix},
\end{equation}


<p>
where \(x_1, \ldots, x_n\) are elements of the vector. In code, we
access any element by indexing into the NDArray.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> x 3<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
3.
</pre>
</div>
</div>

<div id="outline-container-org5e5fbb8" class="outline-4">
<h4 id="org5e5fbb8"><span class="section-number-4">2.3.3.</span> Length, Dimensionality, and Shape</h4>
<div class="outline-text-4" id="text-2-3-3">
<p>
Let us revisit some concepts from <a href="2.1-data-manipulation.html">Section 2.1</a>. A vector is just an
array of numbers. And just as every array has a length, so does every
vector. In math notation, if we want to say that a vector \(\mathbf{x}\)
consists of \(n\) real-valued scalars, we can express this as
\(\mathbf{x} \in \mathbb{R}^n\). The length of a vector is commonly
called the <b>dimension</b> of the vector.
</p>

<p>
As with an ordinary Java array, we can access the length. In the case
of a NDArray we can achieve this by using the <code>(size ndarray 0)</code>
function, where \(0\) means the axis-0.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
4
</pre>


<p>
When a NDArray represents a vector (with precisely one axis), we can
also access its length via the <code>shape</code> function. The shape lists the
length (dimensionality) along each axis of the NDArray. For NDArrays
with just one axis, the shape has just one element.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(4)
</pre>


<p>
or we can use <code>get-shape</code> function:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-shape x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(4)
</pre>


<p>
Note that the word &ldquo;dimension&rdquo; tends to get overloaded in these
contexts and this tends to confuse people. To clarify, we use the
dimensionality of a <i>vector</i> or an <i>axis</i> to refer to its length, i.e.,
the number of elements of a vector or an axis. However, we use the
dimensionality of a NDArray to refer to the number of axes that a
NDArray has. In this sense, the dimensionality of some axis of a
NDArray will be the length of that axis.
</p>
</div>
</div>

<div id="outline-container-org66e578d" class="outline-4">
<h4 id="org66e578d"><span class="section-number-4">2.3.4.</span> Matrices</h4>
<div class="outline-text-4" id="text-2-3-4">
<p>
Just as vectors generalize scalars from order zero to order one,
matrices generalize vectors from order one to order two. Matrices,
which we will typically denote with bold-faced, capital letters (e.g.,
\(\mathbf{X}\), \(\mathbf{Y}\), and \(\mathbf{Z}\)), are represented in code
as NDArray with two axes.
</p>

<p>
In math notation, we use \(\mathbf{A} \in \mathbb{R}^{m \times n}\) to
express that the matrix \(\mathbf{A}\) consists of \(m\) rows and \(n\)
columns of real-valued scalars. Visually, we can illustrate any matrix
\(\mathbf{A} \in \mathbb{R}^{m \times n}\) as a table, where each
element \(a_{ij}\) belongs to the \(i\)<sup>th</sup> row and \(j\)<sup>th</sup> column:
</p>

\begin{equation}
  \mathbf{A}=
  \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn} \\
  \end{bmatrix}.
\end{equation}

<p>
For any \(\mathbf{A} \in \mathbb{R}^{m \times n}\), the shape of
\(\mathbf{A}\) is \((m, n)\) or \(m \times n\). Specifically, when a matrix
has the same number of rows and columns, its shape becomes a square;
thus, it is called a <b>square matrix</b>.
</p>

<p>
We can create an \(m \times n\) matrix by specifying a shape with two
components \(m\) and \(n\) when calling any of our favorite functions for
instantiating a NDArray.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">A</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 20.<span style="color: #909183;">)</span>
           <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape 5 4<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
A
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  1.,  2.,  3.],
 [ 4.,  5.,  6.,  7.],
 [ 8.,  9., 10., 11.],
 [12., 13., 14., 15.],
 [16., 17., 18., 19.],
]
</pre>


<p>
We can access the scalar element \(a_{ij}\) of a matrix \(\mathbf{A}\) in
(2.3.2) by specifying the indices for the row (\(i\)) and column (\(j\)),
such as \([\mathbf{A}]_{ij}\). When the scalar elements of a matrix
\(\mathbf{A}\), such as in (2.3.2), are not given, we may simply use the
lower-case letter of the matrix \(\mathbf{A}\) with the index subscript,
\(a_{ij}\), to refer to \([\mathbf{A}]_{ij}\). To keep notation simple,
commas are inserted to separate indices only when necessary, such as
\(a_{2,3j}\) and \([\mathbf{A}]_{2i1,3}\).
</p>

<p>
Sometimes, we want to flip the axes. When we exchange a matrix&rsquo;s rows
and columns, the result is called the transpose of the
matrix. Formally, we signify a matrix \(\mathbf{A}\)&rsquo;s transpose by
\(\mathbf{A}^\top\) and if \(\mathbf{B}=\mathbf{A}^\top\), then
\(b_{ij}=a_{ji}\) for any \(i\) and \(j\). Thus, the transpose of
\(\mathbf{A}\) in (2.3.2) is a \(n \times m\) matrix:
</p>

\begin{equation}
  \mathbf{A}^\top =
  \begin{bmatrix}
    a_{11} & a_{21} & \dots  & a_{m1} \\
    a_{12} & a_{22} & \dots  & a_{m2} \\
    \vdots & \vdots & \ddots  & \vdots \\
    a_{1n} & a_{2n} & \dots  & a_{mn}
  \end{bmatrix}.
\end{equation}

<p>
Now we access a matrix&rsquo;s transpose in code.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/transpose A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4, 5) cpu() float32
[[ 0.,  4.,  8., 12., 16.],
 [ 1.,  5.,  9., 13., 17.],
 [ 2.,  6., 10., 14., 18.],
 [ 3.,  7., 11., 15., 19.],
]
</pre>


<p>
There is also a simplified function for the same purpose:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/t A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4, 5) cpu() float32
[[ 0.,  4.,  8., 12., 16.],
 [ 1.,  5.,  9., 13., 17.],
 [ 2.,  6., 10., 14., 18.],
 [ 3.,  7., 11., 15., 19.],
]
</pre>


<p>
As a special type of the square matrix, a <b>symmetric matrix</b>
\(\mathbf{A}\) is equal to its transpose:
\(\mathbf{A}=\mathbf{A}^\top\). Here we define a symmetric matrix
\(\mathbf{B}\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">B</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span><span style="color: #709870;">[</span>1 2 3<span style="color: #709870;">]</span> <span style="color: #709870;">[</span>2 0 4<span style="color: #709870;">]</span> <span style="color: #709870;">[</span>3 4 5<span style="color: #709870;">]</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
B
</pre>
</div>

<pre class="example">
ND: (3, 3) cpu() int64
[[ 1,  2,  3],
 [ 2,  0,  4],
 [ 3,  4,  5],
]
</pre>


<p>
Now we compare \(\mathbf{B}\) with its transpose.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/= B <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/t B<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (3, 3) cpu() boolean
[[ true,  true,  true],
 [ true,  true,  true],
 [ true,  true,  true],
]
</pre>


<p>
Matrices are useful data structures: they allow us to organize data
that have different modalities of variation. For example, rows in our
matrix might correspond to different houses (data examples), while
columns might correspond to different attributes. This should sound
familiar if you have ever used spreadsheet software or have read
Section 2.2. Thus, although the default orientation of a single vector
is a column vector, in a matrix that represents a tabular dataset, it
is more conventional to treat each data example as a row vector in the
matrix. And, as we will see in later chapters, this convention will
enable common deep learning practices. For example, along the
outermost axis of a NDArray, we can access or enumerate minibatches of
data examples, or just data examples if no minibatch exists.
</p>
</div>
</div>

<div id="outline-container-org9ecbd6f" class="outline-4">
<h4 id="org9ecbd6f"><span class="section-number-4">2.3.5.</span> Tensors / NDArrays</h4>
<div class="outline-text-4" id="text-2-3-5">
<p>
Just as vectors generalize scalars, and matrices generalize vectors,
we can build data structures with even more axes. NDArrays (&ldquo;NDArrays&rdquo;
in this subsection refer to algebraic objects) give us a generic way
of describing $n$-dimensional arrays with an arbitrary number of
axes. Vectors, for example, are first-order NDArrays, and matrices are
second-order NDArrays. NDArrays are denoted with capital letters of a
special font face (e.g., \(\mathbf{X}\), \(\mathbf{Y}\), and \(\mathbf{Z}\))
and their indexing mechanism (e.g., \(x_{ijk}\) and \([X]_{1,2i1,3}\)) is
similar to that of matrices.
</p>

<p>
NDArrays will become more important when we start working with images,
which arrive as $n$-dimensional arrays with 3 axes corresponding to
the height, width, and a channel axis for stacking the color channels
(red, green, and blue). For now, we will skip over higher order
NDArrays and focus on the basics.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">X</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 24.<span style="color: #909183;">)</span>
           <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape 2 3 4<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
X
</pre>
</div>

<pre class="example" id="org194259e">
ND: (2, 3, 4) cpu() float32
[[[ 0.,  1.,  2.,  3.],
  [ 4.,  5.,  6.,  7.],
  [ 8.,  9., 10., 11.],
 ],
 [[12., 13., 14., 15.],
  [16., 17., 18., 19.],
  [20., 21., 22., 23.],
 ],
]
</pre>
</div>
</div>


<div id="outline-container-orgce2ed21" class="outline-4">
<h4 id="orgce2ed21"><span class="section-number-4">2.3.6.</span> Basic Properties of Tensor Arithmetic</h4>
<div class="outline-text-4" id="text-2-3-6">
<p>
Scalars, vectors, matrices, and NDArrays (&ldquo;NDArrays&rdquo; in this
subsection refer to algebraic objects) of an arbitrary number of axes
have some nice properties that often come in handy. For example, you
might have noticed from the definition of an elementwise operation
that any elementwise unary operation does not change the shape of its
operand. Similarly, given any two NDArrays with the same shape, the
result of any binary elementwise operation will be a NDArray of that
same shape. For example, adding two matrices of the same shape
performs elementwise addition over these two matrices.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">A</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 20.<span style="color: #909183;">)</span>
           <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape 5 4<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">B</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/duplicate A<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure">A
</pre>
</div>


<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  1.,  2.,  3.],
 [ 4.,  5.,  6.,  7.],
 [ 8.,  9., 10., 11.],
 [12., 13., 14., 15.],
 [16., 17., 18., 19.],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure">B
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  1.,  2.,  3.],
 [ 4.,  5.,  6.,  7.],
 [ 8.,  9., 10., 11.],
 [12., 13., 14., 15.],
 [16., 17., 18., 19.],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ A B<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  2.,  4.,  6.],
 [ 8., 10., 12., 14.],
 [16., 18., 20., 22.],
 [24., 26., 28., 30.],
 [32., 34., 36., 38.],
]
</pre>


<p>
Specifically, elementwise multiplication of two matrices is called
their <i>Hadamard product</i> (math notation \(\odot\)). Consider matrix
\(\mathbf{B} \in \mathbb{R}^{m \times n}\) whose element of row \(i\) and
column \(j\) is \(b_{ij}\). The Hadamard product of matrices \(\mathbf{A}\)
(defined in (2.3.2)) and \(\mathbf{B}\)
</p>

\begin{equation}
   \mathbf{A} \odot \mathbf{B} =
   \begin{bmatrix}
       a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\
       a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\
       \vdots & \vdots & \ddots & \vdots \\
       a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}
   \end{bmatrix}.
\end{equation}

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* A B<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[  0.,   1.,   4.,   9.],
 [ 16.,  25.,  36.,  49.],
 [ 64.,  81., 100., 121.],
 [144., 169., 196., 225.],
 [256., 289., 324., 361.],
]
</pre>


<p>
Multiplying or adding a NDArray by a scalar also does not change the
shape of the NDArray, where each element of the operand NDArray will
be added or multiplied by the scalar.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">a</span> 2<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">X</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 24.<span style="color: #909183;">)</span>
           <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape 2 3 4<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ X a<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example" id="orgf0cbac0">
ND: (2, 3, 4) cpu() float32
[[[ 2.,  3.,  4.,  5.],
  [ 6.,  7.,  8.,  9.],
  [10., 11., 12., 13.],
 ],
 [[14., 15., 16., 17.],
  [18., 19., 20., 21.],
  [22., 23., 24., 25.],
 ],
]
</pre>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* X a<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(2, 3, 4)
</pre>
</div>
</div>


<div id="outline-container-lin-alg-reduction" class="outline-4">
<h4 id="lin-alg-reduction"><span class="section-number-4">2.3.7.</span> Reduction</h4>
<div class="outline-text-4" id="text-lin-alg-reduction">
<p>
One useful operation that we can perform with arbitrary NDArrays is to
calculate the sum of their elements. In mathematical notation, we
express sums using the \(\sum\) symbol. To express the sum of the
elements in a vector \(x\) of length \(d\), we write \(\sum^d_{i=1}
x_i\). In code, we can just call the function for calculating the sum.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">x</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 4.<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
x
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[0., 1., 2., 3.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
6.
</pre>


<p>
We can express sums over the elements of NDArrays of arbitrary
shape. For example, the sum of the elements of an \(m \times n\) matrix
\(\mathbf{A}\) could be written \(\sum^m_{i=1} \sum^n_{j=1} a_{ij}\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(5, 4)
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
190.
</pre>


<p>
By default, invoking the function for calculating the sum <i>reduces</i> a
NDArray along all its axes to a scalar. We can also specify the axes
along which the NDArray is reduced via summation. Take matrices as an
example. To reduce the row dimension (axis 0) by summing up elements
of all the rows, we specify <code>[0]</code> when invoking the function. Since the
input matrix reduces along axis 0 to generate the output vector, the
dimension of axis 0 of the input is lost in the output shape.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">A-sum-axis0</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum A <span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
A-sum-axis0
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[40., 45., 50., 55.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape A-sum-axis0<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(4)
</pre>


<p>
Specifying <code>[1]</code> will reduce the column dimension (axis 1) by summing up
elements of all the columns. Thus, the dimension of axis 1 of the
input is lost in the output shape.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">A-sum-axis1</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum A <span style="color: #909183;">[</span>1<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
A-sum-axis1
</pre>
</div>

<pre class="example">
ND: (5) cpu() float32
[ 6., 22., 38., 54., 70.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape A-sum-axis1<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(5)
</pre>


<p>
Reducing a matrix along both rows and columns via summation is
equivalent to summing up all the elements of the matrix.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum A <span style="color: #7388d6;">[</span>0 1<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
190.
</pre>


<p>
A related quantity is the <b>mean</b>, which is also called the average. We
calculate the mean by dividing the sum by the total number of
elements. In code, we could just call the function for calculating the
mean on NDArrays of arbitrary shape.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/mean A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
9.5
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum A<span style="color: #7388d6;">)</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size A<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
9.5
</pre>


<p>
Likewise, the function for calculating the mean can also reduce a
NDArray along the specified axes.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/mean A <span style="color: #7388d6;">[</span>0<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[ 8.,  9., 10., 11.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(5, 4)
</pre>



<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum A <span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape A<span style="color: #909183;">)</span> 0<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[ 8.,  9., 10., 11.]
</pre>
</div>


<div id="outline-container-lin-alg-non-reduction" class="outline-5">
<h5 id="lin-alg-non-reduction"><span class="section-number-5">2.3.7.1.</span> Non-Reduction Sum</h5>
<div class="outline-text-5" id="text-lin-alg-non-reduction">
<p>
However, sometimes it can be useful to keep the number of axes
unchanged when invoking the function for calculating the sum or mean.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">sum-A</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum A <span style="color: #909183;">[</span>1<span style="color: #909183;">]</span> <span style="color: #110099;">true</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
sum-A
</pre>
</div>

<pre class="example">
ND: (5, 1) cpu() float32
[[ 6.],
 [22.],
 [38.],
 [54.],
 [70.],
]
</pre>


<p>
For instance, since <code>sum-A</code> still keeps its two axes after summing each
row, we can divide <code>A</code> by <code>sum-A</code> with broadcasting.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// A sum-A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[0.    , 0.1667, 0.3333, 0.5   ],
 [0.1818, 0.2273, 0.2727, 0.3182],
 [0.2105, 0.2368, 0.2632, 0.2895],
 [0.2222, 0.2407, 0.2593, 0.2778],
 [0.2286, 0.2429, 0.2571, 0.2714],
]
</pre>


<p>
If we want to calculate the cumulative sum of elements of A along some
axis, say axis 0 (row by row), we can call the <code>cumsum</code> function. This
function will not reduce the input NDArray along any axis.
</p>

<div class="org-src-container">
<pre class="src src-clojure">A
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  1.,  2.,  3.],
 [ 4.,  5.,  6.,  7.],
 [ 8.,  9., 10., 11.],
 [12., 13., 14., 15.],
 [16., 17., 18., 19.],
]
</pre>



<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/cumsum A 0<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  1.,  2.,  3.],
 [ 4.,  6.,  8., 10.],
 [12., 15., 18., 21.],
 [24., 28., 32., 36.],
 [40., 45., 50., 55.],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/cumsum A 1<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  1.,  3.,  6.],
 [ 4.,  9., 15., 22.],
 [ 8., 17., 27., 38.],
 [12., 25., 39., 54.],
 [16., 33., 51., 70.],
]
</pre>
</div>
</div>
</div>


<div id="outline-container-org96430a4" class="outline-4">
<h4 id="org96430a4"><span class="section-number-4">2.3.8.</span> Dot Products</h4>
<div class="outline-text-4" id="text-2-3-8">
<p>
So far, we have only performed elementwise operations, sums, and
averages. And if this was all we could do, linear algebra probably
would not deserve its own section. However, one of the most
fundamental operations is the dot product. Given two vectors \(x,y \in
\mathbb{R}^d\), their dot product \(x^\top y\) (or \(\langle x,y \rangle\))
is a sum over the products of the elements at the same position:
\(x^\top y = \sum^d_{i=1} x_i y_i\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/ones ndm <span style="color: #909183;">[</span>4<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.linear-algebra/y
</pre>


<div class="org-src-container">
<pre class="src src-clojure">x
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[0., 1., 2., 3.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure">y
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[1., 1., 1., 1.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot x y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
6.
</pre>


<p>
Note that we can express the dot product of two vectors equivalently
by performing an elementwise multiplication and then a sum:
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* x y<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
6.
</pre>


<p>
Dot products are useful in a wide range of contexts. For example,
given some set of values, denoted by a vector \(x \in \mathbb{R}^d\) and
a set of weights denoted by \(w \in \mathbb{R}^d\), the weighted sum of
the values in \(x\) according to the weights \(w\) could be expressed as
the dot product \(x^\top w\). When the weights are non-negative and sum
to one (i.e., (\(\sum^d_{i=1} w_i = 1\))), the dot product expresses a
weighted average. After normalizing two vectors to have the unit
length, the dot products express the cosine of the angle between
them. We will formally introduce this notion of length later in this
section.
</p>
</div>
</div>

<div id="outline-container-orgea00a04" class="outline-4">
<h4 id="orgea00a04"><span class="section-number-4">2.3.9.</span> Matrix-Vector Products</h4>
<div class="outline-text-4" id="text-2-3-9">
<p>
Now that we know how to calculate dot products, we can begin to
understand matrix-vector products. Recall the matrix \(\mathbf{A} \in
\mathbb{R}^{m \times n}\) and the vector \(x \in \mathbb{R}^n\) defined
and visualized in (2.3.2) and (2.3.1) respectively. Let us start off
by visualizing the matrix \(\mathbf{A}\) in terms of its row vectors
</p>

\begin{equation}
  \mathbf{A}=
  \begin{bmatrix}
    \mathbf{a}^\top_{1} \\
    \mathbf{a}^\top_{2} \\
    \vdots \\
    \mathbf{a}^\top_m \\
  \end{bmatrix},
\end{equation}

<p>
where each \(\mathbf{a}^\top_i \in \mathbb{R}^n\) is a row vector
representing the \(i\)<sup>th</sup> row of the matrix \(\mathbf{A}\). The
matrix-vector product \(\mathbf{A}\mathbf{x}\) is simply a column vector
of length \(m\), whose \(i\)<sup>th</sup> element is the dot product
\(\mathbf{a}^top_i \mathbf{x}\):
</p>

\begin{equation}
  \mathbf{A}\mathbf{x}
  = \begin{bmatrix}
    \mathbf{a}^\top_{1} \\
    \mathbf{a}^\top_{2} \\
    \vdots \\
    \mathbf{a}^\top_m \\
  \end{bmatrix}\mathbf{x}
  = \begin{bmatrix}
    \mathbf{a}^\top_{1} \mathbf{x}  \\
    \mathbf{a}^\top_{2} \mathbf{x} \\
    \vdots\\
    \mathbf{a}^\top_{m} \mathbf{x}\\
  \end{bmatrix}.
\end{equation}

<p>
We can think of multiplication by a matrix \(\mathbf{A} \in
\mathbb{R}^{m \times n}\) as a transformation that projects vectors
from \(\mathbb{R}^n\) to \(\mathbb{R}^m\). These transformations turn out
to be remarkably useful. For example, we can represent rotations as
multiplications by a square matrix. As we will see in subsequent
chapters, we can also use matrix-vector products to describe the most
intensive calculations required when computing each layer in a neural
network given the values of the previous layer.
</p>

<p>
Expressing matrix-vector products in code with NDArrays, we use the
same dot function as for dot products. When we call <code>(nd/dot A x)</code> with
a matrix \(\mathbf{A}\) and a vector \(\mathbf{x}\), the matrix-vector
product is performed. Note that the column dimension of \(\mathbf{A}\)
(its length along axis 1) must be the same as the dimension of
\(\mathbf{x}\) (its length).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape A<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(5, 4)
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(4)
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot A x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (5) cpu() float32
[ 14.,  38.,  62.,  86., 110.]
</pre>
</div>
</div>


<div id="outline-container-orge8d5a74" class="outline-4">
<h4 id="orge8d5a74"><span class="section-number-4">2.3.10.</span> Matrix-Matrix Multiplication</h4>
<div class="outline-text-4" id="text-2-3-10">
<p>
If you have gotten the hang of dot products and matrix-vector
products, then matrix-matrix multiplication should be straightforward.
</p>

<p>
Say that we have two matrices \(\mathbf{A} \in \mathbb{R}^{n \times k}\)
and \(\mathbf{B} \in \mathbb{R}^{k \times m}\):
</p>

\begin{equation}
   \mathbf{A}=\begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1k} \\
    a_{21} & a_{22} & \cdots & a_{2k} \\
   \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nk} \\
   \end{bmatrix},\quad
   \mathbf{B}=\begin{bmatrix}
    b_{11} & b_{12} & \cdots & b_{1m} \\
    b_{21} & b_{22} & \cdots & b_{2m} \\
   \vdots & \vdots & \ddots & \vdots \\
    b_{k1} & b_{k2} & \cdots & b_{km} \\
   \end{bmatrix}.
\end{equation}

<p>
Denote by \(\mathbf{a}^\top_i \in \mathbb{R}^k\) the row vector
representing the \(i\)<sup>th</sup> row of the matrix \(\mathbf{A}\), and let
\(\mathbf{b}_j \in \mathbb{R}^k\) be the column vector from the \(j\)<sup>th</sup>
column of the matrix \(\mathbf{B}\). To produce the matrix product
\(\mathbf{C}=\mathbf{AB}\), it is easiest to think of \(\mathbf{A}\) in
terms of its row vectors and \(\mathbf{B}\) in terms of its column
vectors:
</p>

\begin{equation}
   \mathbf{A}=
   \begin{bmatrix}
   \mathbf{a}^\top_{1} \\
   \mathbf{a}^\top_{2} \\
   \vdots \\
   \mathbf{a}^\top_n \\
   \end{bmatrix},
   \quad \mathbf{B}=\begin{bmatrix}
    \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
   \end{bmatrix}.
\end{equation}

<p>
Then the matrix product \(\mathbf{C} \in \mathbb{R}^{n \times m}\) is
produced as we simply compute each element \(c_{ij}\) as the dot product
\(\mathbf{a}^\top_i \mathbf{b}_j\):
</p>

\begin{equation}
   \mathbf{C} = \mathbf{AB} = \begin{bmatrix}
   \mathbf{a}^\top_{1} \\
   \mathbf{a}^\top_{2} \\
   \vdots \\
   \mathbf{a}^\top_n \\
   \end{bmatrix}
   \begin{bmatrix}
    \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
   \end{bmatrix}
   = \begin{bmatrix}
   \mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
    \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
    \vdots & \vdots & \ddots &\vdots\\
   \mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
   \end{bmatrix}.
\end{equation}

<p>
We can think of the matrix-matrix multiplication \(\mathbf{AB}\) as
simply performing \(m\) matrix-vector products and stitching the results
together to form an \(n \times m\) matrix. In the following snippet, we
perform matrix multiplication on \(\mathbf{A}\) and \(\mathbf{B}\). Here,
\(\mathbf{A}\) is a matrix with 5 rows and 4 columns, and \(\mathbf{B}\)
is a matrix with 4 rows and 3 columns. After multiplication, we obtain
a matrix with 5 rows and 3 columns.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">B</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/ones ndm <span style="color: #909183;">[</span>4 3<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
B
</pre>
</div>

<pre class="example">
ND: (4, 3) cpu() float32
[[1., 1., 1.],
 [1., 1., 1.],
 [1., 1., 1.],
 [1., 1., 1.],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure">A
</pre>
</div>

<pre class="example">
ND: (5, 4) cpu() float32
[[ 0.,  1.,  2.,  3.],
 [ 4.,  5.,  6.,  7.],
 [ 8.,  9., 10., 11.],
 [12., 13., 14., 15.],
 [16., 17., 18., 19.],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot A B<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (5, 3) cpu() float32
[[ 6.,  6.,  6.],
 [22., 22., 22.],
 [38., 38., 38.],
 [54., 54., 54.],
 [70., 70., 70.],
]
</pre>


<p>
Matrix-matrix multiplication can be simply called matrix
multiplication, and should not be confused with the Hadamard product.
</p>
</div>
</div>


<div id="outline-container-org6bf845b" class="outline-4">
<h4 id="org6bf845b"><span class="section-number-4">2.3.11.</span> Norms</h4>
<div class="outline-text-4" id="text-2-3-11">
<p>
Some of the most useful operators in linear algebra are
norms. Informally, the <b>norm of a vector</b> tells us how big a vector
is. The notion of size under consideration here concerns not
dimensionality but rather the magnitude of the components.
</p>

<p>
In linear algebra, a vector norm is a function \(f\) that maps a vector
to a scalar, satisfying a handful of properties. Given any vector
\(\mathbf{x}\), the first property says that if we scale all the
elements of a vector by a constant factor \(\alpha\), its norm also
scales by the absolute value of the same constant factor:
</p>

\begin{equation}
f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).
\end{equation}

<p>
The second property is the familiar triangle inequality:
</p>

\begin{equation}
f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).
\end{equation}

<p>
The third property simply says that the norm must be non-negative:
</p>

\begin{equation}
f(\mathbf{x}) \geq 0.
\end{equation}

<p>
That makes sense, as in most contexts the smallest size for anything
is 0. The final property requires that the smallest norm is achieved
and only achieved by a vector consisting of all zeros.
</p>

\begin{equation}
\forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.
\end{equation}

<p>
You might notice that norms sound a lot like measures of distance. And
if you remember Euclidean distances (think Pythagoras&rsquo; theorem) from
grade school, then the concepts of non-negativity and the triangle
inequality might ring a bell. In fact, the Euclidean distance is a
norm: specifically it is the \(L_2\) norm. Suppose that the elements in
the $n$-dimensional vector \(\mathbf{x}\) are \(x_1, \ldots, x_n\). The
\(L_2\) norm of \(\mathbf{x}\) is the square root of the sum of the
squares of the vector elements:
</p>

\begin{equation}
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},
\end{equation}

<p>
where the subscript \(2\) is often omitted in \(L_2\) norms, i.e.,
\(\|\mathbf{x}\|\) is equivalent to \(\|\mathbf{x}\|_2\). In code, we can
calculate the \(L_2\) norm of a vector as follows.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">l2norm</span> <span style="color: #7388d6;">[</span>ndarray<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> ndarray
      <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/pow 2<span style="color: #909183;">)</span>
      <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum<span style="color: #909183;">)</span>
      <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sqrt<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">u</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span>3. -4.<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
u
</pre>
</div>

<pre class="example">
ND: (2) cpu() float64
[ 3., -4.]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>l2norm u<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float64
5.
</pre>


<p>
In deep learning, we work more often with the squared \(L_2\) norm. You
will also frequently encounter the \(L_1\) norm, which is expressed as
the sum of the absolute values of the vector elements:
</p>

\begin{equation}
\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.
\end{equation}

<p>
As compared with the \(L_2\) norm, it is less influenced by outliers. To
calculate the \(L_1\) norm, we compose the absolute value function with
a sum over the elements.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/abs u<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float64
7.
</pre>


<p>
Both the \(L_2\) norm and the \(L_1\) norm are special cases of the more
general \(L_p\) norm:
</p>

\begin{equation}
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
\end{equation}

<p>
Analogous to \(L_2\) norms of vectors, the <i>Frobenius norm</i> of a matrix
\(\mathbf{X} \in \mathbb{R}^{m \times n}\) is the square root of the sum
of the squares of the matrix elements:
</p>

\begin{equation}
\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.
\end{equation}

<p>
The Frobenius norm satisfies all the properties of vector norms. It
behaves as if it were an \(L_2\) norm of a matrix-shaped
vector. Invoking the following function will calculate the Frobenius
norm of a matrix.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>l2norm <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/ones ndm <span style="color: #909183;">[</span>4 9<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() float32
6.
</pre>
</div>
</div>

<div id="outline-container-orgba50537" class="outline-4">
<h4 id="orgba50537"><span class="section-number-4">2.3.12.</span> Norms and Objectives</h4>
<div class="outline-text-4" id="text-2-3-12">
<p>
While we do not want to get too far ahead of ourselves, we can plant
some intuition already about why these concepts are useful. In deep
learning, we are often trying to solve optimization problems: <b>maximize</b>
the probability assigned to observed data; <b>minimize</b> the distance
between predictions and the ground-truth observations. Assign vector
representations to items (like words, products, or news articles) such
that the distance between similar items is minimized, and the distance
between dissimilar items is maximized. Oftentimes, the objectives,
perhaps the most important components of deep learning algorithms
(besides the data), are expressed as norms.
</p>
</div>
</div>

<div id="outline-container-org313d17b" class="outline-4">
<h4 id="org313d17b"><span class="section-number-4">2.3.13.</span> More on Linear Algebra</h4>
<div class="outline-text-4" id="text-2-3-13">
<p>
In just this section, we have taught you all the linear algebra that
you will need to understand a remarkable chunk of modern deep
learning. There is a lot more to linear algebra and a lot of that
mathematics is useful for machine learning. For example, matrices can
be decomposed into factors, and these decompositions can reveal
low-dimensional structure in real-world datasets. There are entire
subfields of machine learning that focus on using matrix
decompositions and their generalizations to high-order NDArrays to
discover structure in datasets and solve prediction problems. But this
book focuses on deep learning. And we believe you will be much more
inclined to learn more mathematics once you have gotten your hands
dirty deploying useful machine learning models on real datasets. So
while we reserve the right to introduce more mathematics much later
on, we will wrap up this section here.
</p>

<p>
If you are eager to learn more about linear algebra, you may refer to
either the online appendix on linear algebraic operations or other
excellent resources [Strang, 1993][Kolter, 2008][Petersen et al.,
2008].
</p>
</div>
</div>

<div id="outline-container-org6705c49" class="outline-4">
<h4 id="org6705c49"><span class="section-number-4">2.3.14.</span> Summary</h4>
<div class="outline-text-4" id="text-2-3-14">
<ul class="org-ul">
<li>Scalars, vectors, matrices, and NDArrays are basic mathematical
objects in linear algebra.</li>
<li>Vectors generalize scalars, and matrices generalize vectors.</li>
<li>Scalars, vectors, matrices, and NDArrays have zero, one, two, and an
arbitrary number of axes, respectively.</li>
<li>A NDArray can be reduced along the specified axes by sum and mean.</li>
<li>Elementwise multiplication of two matrices is called their Hadamard
product. It is different from matrix multiplication.</li>
<li>In deep learning, we often work with norms such as the \(L_1\) norm,
the \(L_2\) norm, and the Frobenius norm.</li>
<li>We can perform a variety of operations over scalars, vectors,
matrices, and NDArrays.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgbba6516" class="outline-3">
<h3 id="orgbba6516"><span class="section-number-3">2.4.</span> Calculus</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Finding the area of a polygon had remained mysterious until at least
2,500 years ago, when ancient Greeks divided a polygon into triangles
and summed their areas. To find the area of curved shapes, such as a
circle, ancient Greeks inscribed polygons in such shapes. As shown in
Section 2.4, an inscribed polygon with more sides of equal length
better approximates the circle. This process is also known as the
method of exhaustion.
</p>

<p>
In fact, the method of exhaustion is where <b>integral calculus</b> (will be
described in sec<sub>integral</sub><sub>calculus</sub>) originates from. More than 2,000
years later, the other branch of calculus, <b>differential calculus</b>, was
invented. Among the most critical applications of differential
calculus, optimization problems consider how to do something the
best. As discussed in Section 2.3.10.1, such problems are ubiquitous
in deep learning.
</p>

<p>
In deep learning, we train models, updating them successively so that
they get better and better as they see more and more data. Usually,
getting better means minimizing a <b>loss function</b>, a score that answers
the question &ldquo;how bad is our model?&rdquo; This question is more subtle than
it appears. Ultimately, what we really care about is producing a model
that performs well on data that we have never seen before. But we can
only fit the model to data that we can actually see. Thus we can
decompose the task of fitting models into two key concerns: i)
<b>optimization</b>: the process of fitting our models to observed data; ii)
<b>generalization</b>: the mathematical principles and practitioners&rsquo; wisdom
that guide as to how to produce models whose validity extends beyond
the exact set of data examples used to train them.
</p>

<p>
To help you understand optimization problems and methods in later
chapters, here we give a very brief primer on differential calculus
that is commonly used in deep learning.
</p>
</div>

<div id="outline-container-orgeba1e32" class="outline-4">
<h4 id="orgeba1e32"><span class="section-number-4">2.4.1.</span> Derivatives and Differentiation</h4>
<div class="outline-text-4" id="text-2-4-1">
<p>
We begin by addressing the calculation of derivatives, a crucial step
in nearly all deep learning optimization algorithms. In deep learning,
we typically choose loss functions that are differentiable with
respect to our model&rsquo;s parameters. Put simply, this means that for
each parameter, we can determine how rapidly the loss would increase
or decrease, were we to increase or decrease that parameter by an
infinitesimally small amount.
</p>

<p>
Suppose that we have a function \(f: \mathbb{R} \rightarrow
\mathbb{R}\), whose input and output are both scalars. The derivative
of \(f\) is defined as
</p>

\begin{equation}
\label{orgda04469}
f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h},
\end{equation}

<p>
if this limit exists. If \(f'(a)\) exists, \(f\) is said to be
<b>differentiable</b> at \(a\). If \(f\) is differentiable at every number of an
interval, then this function is differentiable on this interval. We
can interpret the derivative :\(f'(x)\) in \eqref{orgda04469} as the
<b>instantaneous</b> rate of change of \(f(x)\) with respect to \(x\). The
so-called instantaneous rate of change is based on the variation \(h\)
in \(x\), which approaches \(0\).
</p>

<p>
To illustrate derivatives, let us experiment with an example. Define
\(u = f(x) = 3x^2-4x\).
</p>

<p>
*Note: We will be using Double in this section to avoid incorrect
results since Double provides more decimal precision. Generally though,
we would use Float as deep learning frameworks by default use Fault.*
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.calculus</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.chart <span style="color: #110099;">:as</span> chart<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.plot <span style="color: #110099;">:as</span> plot<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">f</span> <span style="color: #7388d6;">[</span>x<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span>- <span style="color: #909183;">(</span>* 3 <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow x 2<span style="color: #709870;">)</span><span style="color: #909183;">)</span> <span style="color: #909183;">(</span>* 4 x<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
By setting \(x=1\) and letting \(h\) approach \(0\), the numerical
result of \(\frac{f(x+h) - f(x)}{h}\) in \eqref{orgda04469} approaches
\(2\). Though this experiment is not a mathematical proof, we will see
later that the derivative \(u'\) is \(2\) when \(x=1\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">numerical-lim</span> <span style="color: #7388d6;">[</span>f x h<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span>/ <span style="color: #909183;">(</span>- <span style="color: #709870;">(</span>f <span style="color: #907373;">(</span>+ x h<span style="color: #907373;">)</span><span style="color: #709870;">)</span> <span style="color: #709870;">(</span>f x<span style="color: #709870;">)</span><span style="color: #909183;">)</span> h<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;&gt;</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #909183;">(</span>/ 0.1 <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow 10 <span style="color: #000000;">%</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span> <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 5<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">fn</span> <span style="color: #709870;">[</span>h<span style="color: #709870;">]</span> <span style="color: #709870;">[</span>h <span style="color: #907373;">(</span>numerical-lim f 1 h<span style="color: #907373;">)</span><span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"h = "</span> <span style="color: #709870;">(</span><span style="color: #000000;">%</span> 0<span style="color: #709870;">)</span> <span style="color: #2A00FF;">", numerical limit = "</span> <span style="color: #709870;">(</span><span style="color: #000000;">%</span> 1<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">dorun</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
h =  0.1 , numerical limit =  2.3000000000000043
h =  0.01 , numerical limit =  2.0299999999999763
h =  0.001 , numerical limit =  2.002999999999311
h =  1.0E-4 , numerical limit =  2.0002999999979565
h =  1.0E-5 , numerical limit =  2.0000300000155846
</pre>


<p>
Let us familiarize ourselves with a few equivalent notations for
derivatives. Given \(y = f(x)\), where \(x\) and \(y\) are the
independent variable and the dependent variable of the function \(f\),
respectively. The following expressions are equivalent:
</p>

\begin{equation}
\label{org5726755}
f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),
\end{equation}

<p>
where symbols \(\frac{d}{dx}\) and \(D\) are <b>differentiation operators</b>
that indicate operation of <b>differentiation</b>. We can use the following
rules to differentiate common functions:
</p>

<ul class="org-ul">
<li>\(DC = 0\) (\(C\) is a constant),</li>
<li>\(Dx^n = nx^{n-1}\) (the <b>power rule</b>, \(n\) is any real
number),</li>
<li>\(De^x = e^x\),</li>
<li>\(D\ln(x) = 1/x.\)</li>
</ul>

<p>
To differentiate a function that is formed from a few simpler
functions such as the above common functions, the following rules can
be handy for us. Suppose that functions \(f\) and \(g\) are both
differentiable and \(C\) is a constant, we have the <b>constant multiple
rule</b>
</p>

\begin{equation}
\label{orgb68afac}
\frac{d}{dx} [Cf(x)] = C \frac{d}{dx} f(x),
\end{equation}

<p>
the <b>sum rule</b>
</p>

\begin{equation}
\label{org3cd5fcc}
\frac{d}{dx} [f(x) + g(x)] = \frac{d}{dx} f(x) + \frac{d}{dx} g(x),
\end{equation}

<p>
and the <b>quotient rule</b>
</p>

\begin{equation}
\label{orga4b489c}
\frac{d}{dx} \left[\frac{f(x)}{g(x)}\right] = \frac{g(x) \frac{d}{dx} [f(x)] - f(x) \frac{d}{dx} [g(x)]}{[g(x)]^2}.
\end{equation}

<p>
Now we can apply a few of the above rules to find
\(u' = f'(x) = 3 \frac{d}{dx} x^2-4\frac{d}{dx}x = 6x-4\). Thus, by
setting \(x = 1\), we have \(u' = 2\): this is supported by our
earlier experiment in this section where the numerical result approaches
\(2\). This derivative is also the slope of the tangent line to the
curve \(u = f(x)\) when \(x = 1\).
</p>

<p>
To visualize such an interpretation of derivatives, we will use <code>xchart</code>.
a simple plotting library.
</p>

<p>
We define <code>plot-lines</code> which will take as input three arrays.  The first
array will be the data in the x axis and the next two arrays will
contain the two functions that we want to plot in the y axis. In
addition to this data, the function requires us to specify the name of
the two lines we will be plotting, the label of both axes, and the
width and height of the figure. This function or a modified version of
it, will allow us to plot multiple curves succinctly since we will
need to visualize many curves throughout the book.
</p>

<p>
Now we can plot the function \(u = f(x)\) and its tangent line \(y =
2x - 3\) at \(x=1\), where the coefficient \(2\) is the slope of the
tangent line.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 3 1/32<span style="color: #909183;">)</span>
      y1 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> f x<span style="color: #909183;">)</span>
      y2 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span>- <span style="color: #907373;">(</span>* 2 <span style="color: #000000;">%</span><span style="color: #907373;">)</span> 3<span style="color: #709870;">)</span> x<span style="color: #909183;">)</span>
      chart <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">chart</span>/line <span style="color: #709870;">{</span><span style="color: #110099;">:title</span> <span style="color: #2A00FF;">"tangent line (x=1)"</span>
                         <span style="color: #110099;">:series</span> <span style="color: #907373;">[</span><span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y1"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y1<span style="color: #6276ba;">}</span>
                                  <span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y2"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y2<span style="color: #6276ba;">}</span><span style="color: #907373;">]</span><span style="color: #709870;">}</span><span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">plot</span>/store! chart <span style="color: #110099;">nil</span> <span style="color: #2A00FF;">"notes/figures/tangent_line.svg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="org7e3e25f" class="figure">
<p><img src="figures/tangent_line.svg" alt="tangent_line.svg" class="org-svg" />
</p>
</div>
</div>
</div>

<div id="outline-container-org18f3f26" class="outline-4">
<h4 id="org18f3f26"><span class="section-number-4">2.4.2.</span> Partial Derivatives</h4>
<div class="outline-text-4" id="text-2-4-2">
<p>
So far we have dealt with the differentiation of functions of just one
variable. In deep learning, functions often depend on <b>many</b> variables.
Thus, we need to extend the ideas of differentiation to these
<b>multivariate</b> functions.
</p>

<p>
Let \(y = f(x_1, x_2, \ldots, x_n)\) be a function with \(n\)
variables. The <b>partial derivative</b> of \(y\) with respect to its
\(i^\mathrm{th}\) parameter \(x_i\) is
</p>

\begin{equation}
\label{orge083780}
\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.
\end{equation}


<p>
To calculate \(\frac{\partial y}{\partial x_i}\), we can simply treat
\(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n\) as constants and
calculate the derivative of \(y\) with respect to \(x_i\).  For
notation of partial derivatives, the following are equivalent:
</p>

\begin{equation}
\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.
\end{equation}
</div>
</div>

<div id="outline-container-orgcab85af" class="outline-4">
<h4 id="orgcab85af"><span class="section-number-4">2.4.3.</span> Gradients</h4>
<div class="outline-text-4" id="text-2-4-3">
<p>
We can concatenate partial derivatives of a multivariate function with
respect to all its variables to obtain the <b>gradient</b> vector of the
function. Suppose that the input of function \(f: \mathbb{R}^n
\rightarrow \mathbb{R}\) is an \(n\)-dimensional vector \(\mathbf{x} =
[x_1, x_2, \ldots, x_n]^\top\) and the output is a scalar. The
gradient of the function \(f(\mathbf{x})\) with respect to
\(\mathbf{x}\) is a vector of \(n\) partial derivatives:
</p>

\begin{equation}
\label{orgbcc17ba}
\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top,
\end{equation}

<p>
where \(\nabla_{\mathbf{x}} f(\mathbf{x})\) is often replaced by
\(\nabla f(\mathbf{x})\) when there is no ambiguity.
</p>

<p>
Let \(\mathbf{x}\) be an \(n\)-dimensional vector, the following rules
are often used when differentiating multivariate functions:
</p>

<ul class="org-ul">
<li>For all \(\mathbf{A} \in \mathbb{R}^{m \times n}\),
\(\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top\),</li>
<li>For all \(\mathbf{A} \in \mathbb{R}^{n \times m}\),
\(\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}\),</li>
<li>For all \(\mathbf{A} \in \mathbb{R}^{n \times n}\),
\(\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}\),</li>
<li>\(\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}\).</li>
</ul>

<p>
Similarly, for any matrix \(\mathbf{X}\), we have
\(\nabla_{\mathbf{X}} \|\mathbf{X} \|_F^2 = 2\mathbf{X}\). As we will
see later, gradients are useful for designing optimization algorithms
in deep learning.
</p>
</div>
</div>

<div id="outline-container-org82e3d29" class="outline-4">
<h4 id="org82e3d29"><span class="section-number-4">2.4.4.</span> Chain Rule</h4>
<div class="outline-text-4" id="text-2-4-4">
<p>
However, such gradients can be hard to find. This is because
multivariate functions in deep learning are often <b>composite</b>, so we may
not apply any of the aforementioned rules to differentiate these
functions. Fortunately, the <b>chain rule</b> enables us to differentiate
composite functions.
</p>

<p>
Let us first consider functions of a single variable. Suppose that
functions \(y=f(u)\) and \(u=g(x)\) are both differentiable, then the
chain rule states that
</p>

\begin{equation}
\label{orgee05063}
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}.
\end{equation}


<p>
Now let us turn our attention to a more general scenario where
functions have an arbitrary number of variables. Suppose that the
differentiable function \(y\) has variables \(u_1, u_2, \ldots, u_m\),
where each differentiable function \(u_i\) has variables \(x_1, x_2,
\ldots, x_n\). Note that \(y\) is a function of \(x_1, x_2, \ldots,
x_n\). Then the chain rule gives
</p>

\begin{equation}
\label{orgc0fd4de}
\frac{dy}{dx_i} = \frac{dy}{du_1} \frac{du_1}{dx_i} + \frac{dy}{du_2} \frac{du_2}{dx_i} + \cdots + \frac{dy}{du_m} \frac{du_m}{dx_i}
\end{equation}

<p>
for any \(i = 1, 2, \ldots, n\).
</p>
</div>
</div>

<div id="outline-container-org23bf4b1" class="outline-4">
<h4 id="org23bf4b1"><span class="section-number-4">2.4.5.</span> Summary</h4>
<div class="outline-text-4" id="text-2-4-5">
<ul class="org-ul">
<li>Differential calculus and integral calculus are two branches of
calculus, where the former can be applied to the ubiquitous
optimization problems in deep learning.</li>
<li>A derivative can be interpreted as the instantaneous rate of change
of a function with respect to its variable. It is also the slope of
the tangent line to the curve of the function.</li>
<li>A gradient is a vector whose components are the partial derivatives
of a multivariate function with respect to all its variables.</li>
<li>The chain rule enables us to differentiate composite functions.</li>
</ul>
</div>
</div>

<div id="outline-container-orgc32520d" class="outline-4">
<h4 id="orgc32520d"><span class="section-number-4">2.4.6.</span> Exercises</h4>
<div class="outline-text-4" id="text-2-4-6">
<ol class="org-ol">
<li><p>
Plot the function \(y = f(x) = x^3 - \frac{1}{x}\) and its
tangent line when \(x = 1\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">f2</span> <span style="color: #7388d6;">[</span>x<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span>- <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow x 3<span style="color: #909183;">)</span> <span style="color: #909183;">(</span>/ 1 x<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>numerical-lim f2 1 0.000001<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
4.000001999737712
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>- <span style="color: #7388d6;">(</span>* 4 1<span style="color: #7388d6;">)</span> <span style="color: #7388d6;">(</span>f2 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
4.0
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span>* 1/100 <span style="color: #000000;">%</span><span style="color: #709870;">)</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 10 300<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
      y1 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> f2 x<span style="color: #909183;">)</span>
      y2 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span>- <span style="color: #907373;">(</span>* 4 <span style="color: #000000;">%</span><span style="color: #907373;">)</span> 4<span style="color: #709870;">)</span> x<span style="color: #909183;">)</span>
      chart <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">chart</span>/line <span style="color: #709870;">{</span><span style="color: #110099;">:title</span> <span style="color: #2A00FF;">"tangent line"</span>
                         <span style="color: #110099;">:series</span> <span style="color: #907373;">[</span><span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y1"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y1<span style="color: #6276ba;">}</span>
                                  <span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"y2"</span>
                                   <span style="color: #110099;">:xs</span> x
                                   <span style="color: #110099;">:ys</span> y2<span style="color: #6276ba;">}</span><span style="color: #907373;">]</span><span style="color: #709870;">}</span><span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">plot</span>/store! chart <span style="color: #110099;">nil</span> <span style="color: #2A00FF;">"notes/figures/exercise-2.4-1.svg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="org9d991c7" class="figure">
<p><img src="figures/exercise-2.4-1.svg" alt="exercise-2.4-1.svg" class="org-svg" />
</p>
</div></li>

<li><p>
Find the gradient of the function \(f(\mathbf{x}) = 3x_1^2 +
   5e^{x_2}\).
</p>

\begin{equation}
\label{org7ef0601}
\nabla_\mathbf{x} f(\mathbf{x})
= \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}\bigg]^\top
= \bigg[6x_1, 5e^{x_2}\bigg]^\top
\end{equation}</li>

<li>What is the gradient of the function \(f(\mathbf{x}) =
   \|\mathbf{x}\|_2\)?</li>
<li>Can you write out the chain rule for the case where \(u = f(x, y,
   z)\) and \(x = x(a, b)\), \(y = y(a, b)\), and \(z = z(a, b)\)?</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org97b9449" class="outline-3">
<h3 id="org97b9449"><span class="section-number-3">2.5.</span> Automatic Differentiation</h3>
<div class="outline-text-3" id="text-2-5">
<p>
As we have explained in Section 2.4, differentiation is a crucial step
in nearly all deep learning optimization algorithms. While the
calculations for taking these derivatives are straightforward,
requiring only some basic calculus, for complex models, working out
the updates by hand can be a pain (and often error-prone).
</p>

<p>
Deep learning frameworks expedite this work by automatically
calculating derivatives, i.e., <b>automatic differentiation</b>. In practice,
based on our designed model the system builds a <b>computational graph</b>,
tracking which data combined through which operations to produce the
output. Automatic differentiation enables the system to subsequently
backpropagate gradients. Here, <b>backpropagate</b> simply means to trace
through the computational graph, filling in the partial derivatives
with respect to each parameter.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.auto-diff</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training <span style="color: #110099;">:as</span> t<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>

<div id="outline-container-org2fe6e60" class="outline-4">
<h4 id="org2fe6e60"><span class="section-number-4">2.5.1.</span> A Simple Example</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
As a toy example, say that we are interested in differentiating the
function \(y = 2\mathbf{x}^{\top}\mathbf{x}\) with respect to the
column vector \(\mathbf{x}\). To start, let us create the variable
\(x\) and assign it an initial value.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">x</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm 4.<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
x
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[0., 1., 2., 3.]
</pre>


<p>
Before we even calculate the gradient of \(y\) with respect to
\(\mathbf{x}\), we will need a place to store it. It is important that
we do not allocate new memory every time we take a derivative with
respect to a parameter because we will often update the same
parameters thousands or millions of times and could quickly run out of
memory. Note that a gradient of a scalar-valued function with respect
to a vector \(\mathbf{x}\) is itself vector-valued and has the same
shape as \(\mathbf{x}\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/set-requires-gradient x <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[0., 0., 0., 0.]
</pre>


<p>
We place our code inside a <code>with-open</code> and declare the
<code>gradient-collector</code> object that will build the computational graph. Now
let us calculate \(y\).
</p>

<p>
Since \(\mathbf{x}\) is a vector of length 4, an inner product of
\(\mathbf{x}\) and \(\mathbf{x}\) is performed, yielding the scalar
output that we assign to \(\mathbf{y}\). Next, we can automatically
calculate the gradient of \(\mathbf{y}\) with respect to each component
of \(\mathbf{x}\) by calling the function for backpropagation and
printing the gradient.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #7388d6;">[</span>gc <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot x x<span style="color: #907373;">)</span> 2<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> x<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> y<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc y<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32 hasGradient
[0., 1., 2., 3.]

ND: () cpu() float32
28.

</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[ 0.,  4.,  8., 12.]
</pre>



<p>
The gradient of the function \(y = 2\mathbf{x}^{\top}\mathbf{x}\) with
respect to \(\mathbf{x}\) should be \(4\mathbf{x}\). Let us quickly
verify that our desired gradient was calculated correctly.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/= <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient x<span style="color: #7388d6;">)</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* x 4<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() boolean
[ true,  true,  true,  true]
</pre>


<p>
Now let us calculate another function of \(\mathbf{x}\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #7388d6;">[</span>gc <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum x<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> x<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc y<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32 hasGradient
[0., 1., 2., 3.]

ND: (4) cpu() float32
[1., 1., 1., 1.]
</pre>
</div>
</div>


<div id="outline-container-orgd6cdf45" class="outline-4">
<h4 id="orgd6cdf45"><span class="section-number-4">2.5.2.</span> Backward for Non-Scalar Variables</h4>
<div class="outline-text-4" id="text-2-5-2">
<p>
Technically, when \(y\) is not a scalar, the most natural
interpretation of the differentiation of a vector \(\mathbf{y}\) with
respect to a vector \(\mathbf{x}\) is a matrix. For higher-order and
higher-dimensional \(\mathbf{y}\) and \(\mathbf{x}\), the
differentiation result could be a high-order tensor.
</p>

<p>
However, while these more exotic objects do show up in advanced
machine learning (including in deep learning), more often when we are
calling backward on a vector, we are trying to calculate the
derivatives of the loss functions for each constituent of a <b>batch</b> of
training examples.  Here, our intent is not to calculate the
differentiation matrix but rather the sum of the partial derivatives
computed individually for each example in the batch.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #7388d6;">[</span>gc <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* x x<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc y<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient x<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() float32
[0., 2., 4., 6.]
</pre>
</div>
</div>


<div id="outline-container-org2ce69a1" class="outline-4">
<h4 id="org2ce69a1"><span class="section-number-4">2.5.3.</span> Detaching Computation</h4>
<div class="outline-text-4" id="text-2-5-3">
<p>
Sometimes, we wish to move some calculations outside of the recorded
computational graph. For example, say that \(\mathbf{y}\) was
calculated as a function of \(\mathbf{x}\), and that subsequently
\(\mathbf{z}\) was calculated as a function of both \(\mathbf{y}\) and
\(\mathbf{x}\). Now, imagine that we wanted to calculate the gradient
of \(\mathbf{z}\) with respect to \(\mathbf{x}\), but wanted for some
reason to treat \(\mathbf{y}\) as a constant, and only take into
account the role that \(\mathbf{x}\) played after \(\mathbf{y}\) was
calculated.
</p>

<p>
Here, we can detach \(\mathbf{y}\) using <code>stop-gradient</code> to return a new
variable \(\mathbf{u}\) that has the same value as \(\mathbf{y}\) but
discards any information about how \(\mathbf{y}\) was computed in the
computational graph. In other words, the gradient will not flow
backwards through \(\mathbf{u}\) to \(\mathbf{x}\). Thus, the
following backpropagation function computes the partial derivative of
\(\mathbf{z} = \mathbf{u} \times \mathbf{x}\) with respect to
\(\mathbf{x}\) while treating \(\mathbf{u}\) as a constant, instead of
the partial derivative of \(\mathbf{z} = \mathbf{x} \times \mathbf{x}
\times \mathbf{x}\) with respect to \(\mathbf{x}\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #7388d6;">[</span>gc <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* x x<span style="color: #709870;">)</span>
        u <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/stop-gradient y<span style="color: #709870;">)</span>
        z <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* u x<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc z<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/= u <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient x<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() boolean
[ true,  true,  true,  true]
</pre>


<p>
We can subsequently invoke backpropagation on \(\mathbf{y}\) to get
the derivative of \(\mathbf{y} = \mathbf{x} \times \mathbf{x}\) with
respect to \(\mathbf{x}\), which is \(2 \times \mathbf{x}\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #7388d6;">[</span>gc <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* x x<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc y<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/= <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient x<span style="color: #709870;">)</span> <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* x 2<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (4) cpu() boolean
[ true,  true,  true,  true]
</pre>
</div>
</div>

<div id="outline-container-org9c66f3a" class="outline-4">
<h4 id="org9c66f3a"><span class="section-number-4">2.5.4.</span> Computing the Gradient of Clojure Control Flow</h4>
<div class="outline-text-4" id="text-2-5-4">
<p>
One benefit of using automatic differentiation is that even if
building the computational graph of a function required passing
through a maze of Clojure control flow (e.g., conditionals, loops, and
arbitrary function calls), we can still calculate the gradient of the
resulting variable.  In the following snippet, note that the number of
iterations of the <code>loop</code> and the evaluation of the <code>if</code> statement both
depend on the value of the input \(\mathbf{a}\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">f</span> <span style="color: #7388d6;">[</span>a<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">loop</span> <span style="color: #909183;">[</span>b <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* a 2<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">if</span> <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element <span style="color: #907373;">(</span>.lt <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/norm b<span style="color: #6276ba;">)</span> 1000<span style="color: #907373;">)</span><span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">recur</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* b 2<span style="color: #907373;">)</span><span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">if</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element <span style="color: #6276ba;">(</span>.gt <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum b<span style="color: #858580;">)</span> 0<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
        b
        <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* b 100<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Let us compute the gradient.
</p>

<p>
We can then analyze the <code>f</code> function defined above. Note that it is
piecewise linear in its input \(\mathbf{a}\). In other words, for any
\(\mathbf{a}\) there exists some constant scalar \(k\) such that
\(f(\mathbf{a}) = k \times \mathbf{a}\), where the value of \(k\)
depends on the input \(\mathbf{a}\). Consequently <code>(nd// d a)</code> allows us
to verify that the gradient is correct.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">a</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm <span style="color: #909183;">[</span>10<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
a
</pre>
</div>

<pre class="example">
ND: (10) cpu() float32
[-1.475 ,  1.5194, -0.5241,  1.9041,  1.2663, -1.5734,  0.8951, -0.1401, -0.6016,  0.2967]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/set-requires-gradient a <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #7388d6;">[</span>gc <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>d <span style="color: #709870;">(</span>f a<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc d<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// d a<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/= <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/get-gradient a<span style="color: #6276ba;">)</span> <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// d a<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (10) cpu() float32
[512., 512., 512., 512., 512., 512., 512., 512., 512., 512.]

ND: (10) cpu() boolean
[ true,  true,  true,  true,  true,  true,  true,  true,  true,  true]

</pre>
</div>
</div>

<div id="outline-container-org048d748" class="outline-4">
<h4 id="org048d748"><span class="section-number-4">2.5.5.</span> Summary</h4>
<div class="outline-text-4" id="text-2-5-5">
<ul class="org-ul">
<li>Deep learning frameworks can automate the calculation of
derivatives.  To use it, we first attach gradients to those
variables with respect to which we desire partial derivatives. We
then record the computation of our target value, execute its
function for backpropagation, and access the resulting gradient.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org2f76be0" class="outline-3">
<h3 id="org2f76be0"><span class="section-number-3">2.6.</span> Probability</h3>
<div class="outline-text-3" id="text-2-6">
<p>
In some form or another, machine learning is all about making
predictions. We might want to predict the <b>probability</b> of a patient
suffering a heart attack in the next year, given their clinical
history. In anomaly detection, we might want to assess how <b>likely</b> a
set of readings from an airplane&rsquo;s jet engine would be, were it
operating normally. In reinforcement learning, we want an agent to act
intelligently in an environment. This means we need to think about the
probability of getting a high reward under each of the available
actions. And when we build recommender systems we also need to think
about probability. For example, say <b>hypothetically</b> that we worked for
a large online bookseller. We might want to estimate the probability
that a particular user would buy a particular book. For this we need
to use the language of probability. Entire courses, majors, theses,
careers, and even departments, are devoted to probability. So
naturally, our goal in this section is not to teach the whole
subject. Instead we hope to get you off the ground, to teach you just
enough that you can start building your first deep learning models,
and to give you enough of a flavor for the subject that you can begin
to explore it on your own if you wish.
</p>

<p>
We have already invoked probabilities in previous sections without
articulating what precisely they are or giving a concrete example. Let
us get more serious now by considering the first case: distinguishing
cats and dogs based on photographs. This might sound simple but it is
actually a formidable challenge. To start with, the difficulty of the
problem may depend on the resolution of the image.
</p>
</div>

<div id="outline-container-org67ae8bc" class="outline-4">
<h4 id="org67ae8bc"><span class="section-number-4">2.6.1.</span> Basic Probability Theory</h4>
<div class="outline-text-4" id="text-2-6-1">
<p>
Say that we cast a die and want to know what the chance is of seeing a
\(1\) rather than another digit. If the die is fair, all the six
outcomes \(\{1, \ldots, 6\}\) are equally likely to occur, and thus we
would see a \(1\) in one out of six cases. Formally we state that
\(1\) occurs with probability \(\frac{1}{6}\).
</p>

<p>
For a real die that we receive from a factory, we might not know those
proportions and we would need to check whether it is tainted. The only
way to investigate the die is by casting it many times and recording
the outcomes. For each cast of the die, we will observe a value in
\(\{1, \ldots, 6\}\). Given these outcomes, we want to investigate the
probability of observing each outcome.
</p>

<p>
One natural approach for each value is to take the individual count for
that value and to divide it by the total number of tosses. This gives us
an <b>estimate</b> of the probability of a given <b>event</b>. The <b>law of large
numbers</b> tell us that as the number of tosses grows this estimate will
draw closer and closer to the true underlying probability. Before going
into the details of what is going here, let us try it out.
</p>

<p>
To start, let us import the necessary packages.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.probability</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training <span style="color: #110099;">:as</span> t<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.chart <span style="color: #110099;">:as</span> chart<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.plot <span style="color: #110099;">:as</span> plot<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Next, we will want to be able to cast the die. In statistics we call
this process of drawing examples from probability distributions
<b>sampling</b>. The distribution that assigns probabilities to a number of
discrete choices is called the <b>multinomial distribution</b>. We will give
a more formal definition of <b>distribution</b> later, but at a high level,
think of it as just an assignment of probabilities to events.
</p>

<p>
To draw a single sample, we simply pass in a vector of probabilities.
The output is another vector of the same length: its value at index
\(i\) is the number of times the sampling outcome corresponds to
\(i\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/new-base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">fair-probs</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">repeat</span> 6 <span style="color: #709870;">(</span>/ 1.0 6<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #3F7F5F;">;</span><span style="color: #3F7F5F;">=&gt; [1/6 1/6 1/6 1/6 1/6 1/6]</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 1 fair-probs<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (6) cpu() int64
[ 0,  0,  1,  0,  0,  0]
</pre>


<p>
If you run the sampler a bunch of times, you will find that you get
out random values each time. As with estimating the fairness of a die,
we often want to generate many samples from the same distribution. It
would be unbearably slow to do this with a Clojure <code>loop</code>, so the
function we are using supports drawing multiple samples at once,
returning an array of independent samples in any shape we might
desire.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 10 fair-probs<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (6) cpu() int64
[ 3,  1,  0,  0,  3,  3]
</pre>


<p>
Now that we know how to sample rolls of a die, we can simulate 1000
rolls. We can then go through and count, after each of the 1000 rolls,
how many times each number was rolled. Specifically, we calculate the
relative frequency as the estimate of the true probability.
</p>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">counts</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 1000 fair-probs<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// counts 1000<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (6) cpu() float32
[0.183, 0.156, 0.182, 0.169, 0.153, 0.157]
</pre>


<p>
Because we generated the data from a fair die, we know that each
outcome has true probability \(\frac{1}{6}\), roughly \(0.167\), so
the above output estimates look good.
</p>

<p>
We can also visualize how these probabilities converge over time
towards the true probability. Let us conduct 500 groups of experiments
where each group draws 10 samples.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">counts</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-multinomial ndm 10 fair-probs <span style="color: #909183;">[</span>500<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">cum-counts</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/cumsum counts 0<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">estimates</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// cum-counts <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum cum-counts 1 <span style="color: #110099;">true</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 500<span style="color: #909183;">)</span>
      dies <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 6<span style="color: #909183;">)</span>
      names <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">mapv</span> #<span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> <span style="color: #2A00FF;">"P(die="</span> <span style="color: #000000;">%</span> <span style="color: #2A00FF;">")"</span><span style="color: #709870;">)</span> dies<span style="color: #909183;">)</span>
      ys <span style="color: #909183;">(</span>mapv #<span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/transpose estimates<span style="color: #6276ba;">)</span> <span style="color: #000000;">%</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span> dies<span style="color: #909183;">)</span>
      series <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">concat</span> <span style="color: #709870;">[</span><span style="color: #907373;">{</span><span style="color: #110099;">:name</span> <span style="color: #2A00FF;">"0.167"</span>
                       <span style="color: #110099;">:xs</span> x
                       <span style="color: #110099;">:ys</span> <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">repeat</span> 500 0.167<span style="color: #6276ba;">)</span><span style="color: #907373;">}</span><span style="color: #709870;">]</span>
                     <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">fn</span> <span style="color: #6276ba;">[</span>n y<span style="color: #6276ba;">]</span> <span style="color: #6276ba;">{</span><span style="color: #110099;">:name</span> n <span style="color: #110099;">:xs</span> x <span style="color: #110099;">:ys</span> y<span style="color: #6276ba;">}</span><span style="color: #907373;">)</span>
                          names ys<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
      c <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">chart</span>/line <span style="color: #709870;">{</span><span style="color: #110099;">:series</span> series<span style="color: #709870;">}</span><span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">plot</span>/store! c <span style="color: #110099;">nil</span> <span style="color: #2A00FF;">"notes/figures/probability_dies.svg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="org6862458" class="figure">
<p><img src="figures/probability_dies.svg" alt="probability_dies.svg" class="org-svg" />
</p>
</div>


<p>
Each solid curve corresponds to one of the six values of the die and
gives our estimated probability that the die turns up that value as
assessed after each group of experiments. The blue line gives
the true underlying probability. As we get more data by conducting
more experiments, the \(6\) solid curves converge towards the true
probability.
</p>
</div>
</div>

<div id="outline-container-org8c4aad2" class="outline-4">
<h4 id="org8c4aad2"><span class="section-number-4">2.6.2.</span> Axioms of Probability Theory</h4>
<div class="outline-text-4" id="text-2-6-2">
<p>
When dealing with the rolls of a die, we call the set \(\mathcal{S} =
\{1, 2, 3, 4, 5, 6\}\) the <b>sample space</b> or <b>outcome space</b>, where each
element is an <b>outcome</b>. An <b>event</b> is a set of outcomes from a given
sample space. For instance, &ldquo;seeing a \(5\)&rdquo; (\(\{5\}\)) and &ldquo;seeing
an odd number&rdquo; (\(\{1, 3, 5\}\)) are both valid events of rolling a
die. Note that if the outcome of a random experiment is in event
\(\mathcal{A}\), then event \(\mathcal{A}\) has occurred. That is to
say, if \(3\) dots faced up after rolling a die, since \(3 \in \{1, 3,
5\}\), we can say that the event &ldquo;seeing an odd number&rdquo; has occurred.
</p>

<p>
Formally, <b>probability</b> can be thought of as a function that maps a set
to a real value. The probability of an event \(\mathcal{A}\) in the
given sample space \(\mathcal{S}\), denoted as \(P(\mathcal{A})\),
satisfies the following properties:
</p>

<ul class="org-ul">
<li>For any event \(\mathcal{A}\), its probability is never negative,
i.e., \(P(\mathcal{A}) \geq 0\);</li>
<li>Probability of the entire sample space is \(1\), i.e.,
\(P(\mathcal{S}) = 1\);</li>
<li>For any countable sequence of events \(\mathcal{A}_1, \mathcal{A}_2,
  \ldots\) that are <b>mutually exclusive</b> (\(\mathcal{A}_i \cap
  \mathcal{A}_j = \emptyset\) for all \(i \neq j\)), the probability
that any happens is equal to the sum of their individual
probabilities, i.e., \(P(\bigcup_{i=1}^{\infty} \mathcal{A}_i) =
  \sum_{i=1}^{\infty} P(\mathcal{A}_i)\).</li>
</ul>

<p>
These are also the axioms of probability theory, proposed by
Kolmogorov in 1933. Thanks to this axiom system, we can avoid any
philosophical dispute on randomness; instead, we can reason rigorously
with a mathematical language. For instance, by letting event
\(\mathcal{A}_1\) be the entire sample space and \(\mathcal{A}_i =
\emptyset\) for all \(i > 1\), we can prove that \(P(\emptyset) = 0\),
i.e., the probability of an impossible event is \(0\).
</p>
</div>
</div>

<div id="outline-container-org59f4cfd" class="outline-4">
<h4 id="org59f4cfd"><span class="section-number-4">2.6.3.</span> Random Variables</h4>
<div class="outline-text-4" id="text-2-6-3">
<p>
In our random experiment of casting a die, we introduced the notion of
a <b>random variable</b>. A random variable can be pretty much any quantity
and is not deterministic. It could take one value among a set of
possibilities in a random experiment. Consider a random variable \(X\)
whose value is in the sample space \(\mathcal{S} = \{1, 2, 3, 4, 5,
6\}\) of rolling a die. We can denote the event &ldquo;seeing a \(5\)&rdquo; as
\(\{X = 5\}\) or \(X = 5\), and its probability as \(P(\{X = 5\})\) or
\(P(X = 5)\). By \(P(X = a)\), we make a distinction between the
random variable \(X\) and the values (e.g., \(a\)) that \(X\) can
take. However, such pedantry results in a cumbersome notation. For a
compact notation, on one hand, we can just denote \(P(X)\) as the
<b>distribution</b> over the random variable \(X\): the distribution tells us
the probability that \(X\) takes any value. On the other hand, we can
simply write \(P(a)\) to denote the probability that a random variable
takes the value \(a\). Since an event in probability theory is a set
of outcomes from the sample space, we can specify a range of values
for a random variable to take. For example, \(P(1 \leq X \leq 3)\)
denotes the probability of the event \(\{1 \leq X \leq 3\}\), which
means \(\{X = 1, 2, \text{or}, 3\}\). Equivalently, \(P(1 \leq X \leq
3)\) represents the probability that the random variable \(X\) can
take a value from \(\{1, 2, 3\}\).
</p>

<p>
Note that there is a subtle difference between <b>discrete</b> random
variables, like the sides of a die, and <b>continuous</b> ones, like the
weight and the height of a person. There is little point in asking
whether two people have exactly the same height. If we take precise
enough measurements you will find that no two people on the planet have
the exact same height. In fact, if we take a fine enough measurement,
you will not have the same height when you wake up and when you go to
sleep. So there is no purpose in asking about the probability that
someone is 1.80139278291028719210196740527486202 meters tall. Given the
world population of humans the probability is virtually 0. It makes more
sense in this case to ask whether someones height falls into a given
interval, say between 1.79 and 1.81 meters. In these cases we quantify
the likelihood that we see a value as a <b>density</b>. The height of exactly
1.80 meters has no probability, but nonzero density. In the interval
between any two different heights we have nonzero probability. In the
rest of this section, we consider probability in discrete space. For
probability over continuous random variables, you may refer to
Section 18.6.
</p>
</div>
</div>

<div id="outline-container-org87cf653" class="outline-4">
<h4 id="org87cf653"><span class="section-number-4">2.6.4.</span> Dealing with Multiple Random Variables</h4>
<div class="outline-text-4" id="text-2-6-4">
<p>
Very often, we will want to consider more than one random variable at
a time. For instance, we may want to model the relationship between
diseases and symptoms. Given a disease and a symptom, say &ldquo;flu&rdquo; and
&ldquo;cough&rdquo;, either may or may not occur in a patient with some
probability.  While we hope that the probability of both would be
close to zero, we may want to estimate these probabilities and their
relationships to each other so that we may apply our inferences to
effect better medical care.
</p>

<p>
As a more complicated example, images contain millions of pixels, thus
millions of random variables. And in many cases images will come with
a label, identifying objects in the image. We can also think of the
label as a random variable. We can even think of all the metadata as
random variables such as location, time, aperture, focal length, ISO,
focus distance, and camera type. All of these are random variables
that occur jointly. When we deal with multiple random variables, there
are several quantities of interest.
</p>
</div>
</div>

<div id="outline-container-orgf6f8a7d" class="outline-4">
<h4 id="orgf6f8a7d"><span class="section-number-4">2.6.5.</span> Joint Probability</h4>
<div class="outline-text-4" id="text-2-6-5">
<p>
The first is called the <b>joint probability</b> \(P(A = a, B=b)\). Given any
values \(a\) and \(b\), the joint probability lets us answer, what is
the probability that \(A=a\) and \(B=b\) simultaneously? Note that for
any values \(a\) and \(b\), \(P(A=a, B=b) \leq P(A=a)\). This has to
be the case, since for \(A=a\) and \(B=b\) to happen, \(A=a\) has to
happen <b>and</b> \(B=b\) also has to happen (and vice versa). Thus, \(A=a\)
and \(B=b\) cannot be more likely than \(A=a\) or \(B=b\)
individually.
</p>
</div>
</div>

<div id="outline-container-org363fa62" class="outline-4">
<h4 id="org363fa62"><span class="section-number-4">2.6.6.</span> Conditional Probability</h4>
<div class="outline-text-4" id="text-2-6-6">
<p>
This brings us to an interesting ratio: \(0 \leq \frac{P(A=a,
B=b)}{P(A=a)} \leq 1\). We call this ratio a <b>conditional probability</b>
and denote it by \(P(B=b \mid A=a)\): it is the probability of
\(B=b\), provided that \(A=a\) has occurred.
</p>
</div>
</div>

<div id="outline-container-orgd3731db" class="outline-4">
<h4 id="orgd3731db"><span class="section-number-4">2.6.7.</span> Bayes&rsquo; theorem</h4>
<div class="outline-text-4" id="text-2-6-7">
<p>
Using the definition of conditional probabilities, we can derive one
of the most useful and celebrated equations in statistics: <b>Bayes&rsquo;
theorem</b>. It goes as follows. By construction, we have the
<b>multiplication rule</b> that \(P(A, B) = P(B \mid A) P(A)\). By symmetry,
this also holds for \(P(A, B) = P(A \mid B) P(B)\). Assume that \(P(B)
> 0\). Solving for one of the conditional variables we get
</p>

\begin{equation}
\label{org0ad706a}
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.
\end{equation}

<p>
Note that here we use the more compact notation where \(P(A, B)\)
is a <b>joint distribution</b> and \(P(A \mid B)\) is a <b>conditional
distribution</b>. Such distributions can be evaluated for particular
values \(A = a, B=b\).
</p>
</div>
</div>

<div id="outline-container-org98612ae" class="outline-4">
<h4 id="org98612ae"><span class="section-number-4">2.6.8.</span> Marginalization</h4>
<div class="outline-text-4" id="text-2-6-8">
<p>
Bayes&rsquo; theorem is very useful if we want to infer one thing from the
other, say cause and effect, but we only know the properties in the
reverse direction, as we will see later in this section. One important
operation that we need, to make this work, is <b>marginalization</b>. It is
the operation of determining \(P(B)\) from \(P(A, B)\). We can see
that the probability of \(B\) amounts to accounting for all possible
choices of \(A\) and aggregating the joint probabilities over all of
them:
</p>

\begin{equation}
\label{org9485ae5}
P(B) = \sum_{A} P(A, B),
\end{equation}

<p>
which is also known as the <b>sum rule</b>. The probability or distribution
as a result of marginalization is called a <b>marginal probability</b> or a
<b>marginal distribution</b>.
</p>
</div>
</div>

<div id="outline-container-org2965e5a" class="outline-4">
<h4 id="org2965e5a"><span class="section-number-4">2.6.9.</span> Independence</h4>
<div class="outline-text-4" id="text-2-6-9">
<p>
Another useful property to check for is <b>dependence</b> vs. <b>independence</b>.
Two random variables \(A\) and \(B\) being independent means that the
occurrence of one event of \(A\) does not reveal any information about
the occurrence of an event of \(B\). In this case \(P(B \mid A) =
P(B)\). Statisticians typically express this as \(A \perp B\). From
Bayes&rsquo; theorem, it follows immediately that also \(P(A \mid B) =
P(A)\). In all the other cases we call \(A\) and \(B\) dependent. For
instance, two successive rolls of a die are independent. In contrast,
the position of a light switch and the brightness in the room are not
(they are not perfectly deterministic, though, since we could always
have a broken light bulb, power failure, or a broken switch).
</p>

<p>
Since \(P(A \mid B) = \frac{P(A, B)}{P(B)} = P(A)\) is equivalent to
\(P(A, B) = P(A)P(B)\), two random variables are independent if and
only if their joint distribution is the product of their individual
distributions. Likewise, two random variables \(A\) and \(B\) are
<b>conditionally independent</b> given another random variable \(C\) if and
only if \(P(A, B \mid C) = P(A \mid C)P(B \mid C)\). This is expressed
as \(A \perp B \mid C\).
</p>
</div>
</div>

<div id="outline-container-org99a4142" class="outline-4">
<h4 id="org99a4142"><span class="section-number-4">2.6.10.</span> Application</h4>
<div class="outline-text-4" id="text-2-6-10">
<p>
Let us put our skills to the test. Assume that a doctor administers an
HIV test to a patient. This test is fairly accurate and it fails only
with 1% probability if the patient is healthy but reporting him as
diseased. Moreover, it never fails to detect HIV if the patient
actually has it. We use \(D_1\) to indicate the diagnosis (\(1\) if
positive and \(0\) if negative) and \(H\) to denote the HIV status
(\(1\) if positive and \(0\) if negative).  Table <a href="#orgad7c693">1</a>
lists such conditional probabilities.
</p>

<table id="orgad7c693" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Conditional probability of \(P(D_1 \mid H)\).</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Conditional probability</th>
<th scope="col" class="org-right">\(H=1\)</th>
<th scope="col" class="org-right">\(H=0\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(P(D_1 = 1 \mid H)\)</td>
<td class="org-right">1</td>
<td class="org-right">0.01</td>
</tr>

<tr>
<td class="org-left">\(P(D_1 = 0 \mid H)\)</td>
<td class="org-right">0</td>
<td class="org-right">0.99</td>
</tr>
</tbody>
</table>

<p>
Note that the column sums are all 1 (but the row sums are not), since
the conditional probability needs to sum up to 1, just like the
probability. Let us work out the probability of the patient having HIV
if the test comes back positive, i.e., \(P(H = 1 \mid D_1 = 1)\).
Obviously this is going to depend on how common the disease is, since
it affects the number of false alarms. Assume that the population is
quite healthy, e.g., \(P(H=1) = 0.0015\). To apply Bayes&rsquo; theorem, we
need to apply marginalization and the multiplication rule to determine
</p>

\begin{equation}
  \begin{aligned}
    &P(D_1 = 1) \\
    =& P(D_1=1, H=0) + P(D_1=1, H=1)  \\
    =& P(D_1=1 \mid H=0) P(H=0) + P(D_1=1 \mid H=1) P(H=1) \\
    =& 0.011485.
  \end{aligned}
\end{equation}

<p>
Thus, we get
</p>

\begin{equation}
\begin{aligned}
&P(H = 1 \mid D_1 = 1)\\ =& \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)} \\ =& 0.1306
\end{aligned}
\end{equation}

<p>
In other words, there is only a 13.06% chance that the patient
actually has HIV, despite using a very accurate test. As we can see,
probability can be counter intuitive.
</p>

<p>
What should a patient do upon receiving such terrifying news? Likely,
the patient would ask the physician to administer another test to get
clarity. The second test has different characteristics and it is not
as good as the first one, as shown in Table <a href="#org97bf8a9">2</a>.
</p>

<table id="org97bf8a9" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> Conditional probability of \(P(D_2 \mid H)\).</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Conditional probability</th>
<th scope="col" class="org-right">\(H=1\)</th>
<th scope="col" class="org-right">\(H=0\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(P(D_2 = 1 \mid H)\)</td>
<td class="org-right">0.98</td>
<td class="org-right">0.03</td>
</tr>

<tr>
<td class="org-left">\(P(D_2 = 0 \mid H)\)</td>
<td class="org-right">0.02</td>
<td class="org-right">0.97</td>
</tr>
</tbody>
</table>

<p>
Unfortunately, the second test comes back positive, too. Let us work
out the requisite probabilities to invoke Bayes&rsquo; theorem by assuming
the conditional independence:
</p>

\begin{equation}
\begin{aligned}
&P(D_1 = 1, D_2 = 1 \mid H = 0) \\
=& P(D_1 = 1 \mid H = 0) P(D_2 = 1 \mid H = 0)  \\
=& 0.0003,
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
&P(D_1 = 1, D_2 = 1 \mid H = 1) \\
=& P(D_1 = 1 \mid H = 1) P(D_2 = 1 \mid H = 1)  \\
=& 0.98.
\end{aligned}
\end{equation}

<p>
Now we can apply marginalization and the multiplication rule:
</p>

\begin{equation}
\begin{aligned}
&P(D_1 = 1, D_2 = 1) \\
=& P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\
=& P(D_1 = 1, D_2 = 1 \mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \mid H = 1)P(H=1)\\
=& 0.00176955.
\end{aligned}
\end{equation}

<p>
In the end, the probability of the patient having HIV given both
positive tests is
</p>

\begin{equation}
\begin{aligned}
&P(H = 1 \mid D_1 = 1, D_2 = 1)\\
=& \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \\
=& 0.8307.
\end{aligned}
\end{equation}

<p>
That is, the second test allowed us to gain much higher confidence
that not all is well. Despite the second test being considerably less
accurate than the first one, it still significantly improved our
estimate.
</p>
</div>
</div>

<div id="outline-container-orgd74199c" class="outline-4">
<h4 id="orgd74199c"><span class="section-number-4">2.6.11.</span> Expectation and Variance</h4>
<div class="outline-text-4" id="text-2-6-11">
<p>
To summarize key characteristics of probability distributions, we need
some measures. The <b>expectation</b> (or average) of the random variable
\(X\) is denoted as
</p>

\begin{equation}
E[X] = \sum_{x} x P(X = x).
\end{equation}

<p>
When the input of a function \(f(x)\) is a random variable drawn from
the distribution \(P\) with different values \(x\), the expectation of
\(f(x)\) is computed as
</p>

\begin{equation}
E_{x \sim P}[f(x)] = \sum_x f(x) P(x).
\end{equation}

<p>
In many cases we want to measure by how much the random variable \(X\)
deviates from its expectation. This can be quantified by the variance
</p>


\begin{equation}
  \mathrm{Var}[X] = E\left[(X - E[X])^2\right] =
  E[X^2] - E[X]^2.
\end{equation}

<p>
Its square root is called the <b>standard deviation</b>. The variance of a
function of a random variable measures by how much the function
deviates from the expectation of the function, as different values
\(x\) of the random variable are sampled from its distribution:
</p>

\begin{equation}
\mathrm{Var}[f(x)] = E\left[\left(f(x) - E[f(x)]\right)^2\right].
\end{equation}
</div>
</div>

<div id="outline-container-org039e327" class="outline-4">
<h4 id="org039e327"><span class="section-number-4">2.6.12.</span> Summary</h4>
<div class="outline-text-4" id="text-2-6-12">
<ul class="org-ul">
<li>We can sample from probability distributions.</li>
<li>We can analyze multiple random variables using joint distribution,
conditional distribution, Bayes&rsquo; theorem, marginalization, and
independence assumptions.</li>
<li>Expectation and variance offer useful measures to summarize key
characteristics of probability distributions.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgeddee54" class="outline-2">
<h2 id="orgeddee54"><span class="section-number-2">3.</span> Linear Neural Networks</h2>
<div class="outline-text-2" id="text-3">
<p>
Before we get into the details of deep neural networks, we need to
cover the basics of neural network training. In this chapter, we will
cover the entire training process, including defining simple neural
network architectures, handling data, specifying a loss function, and
training the model. In order to make things easier to grasp, we begin
with the simplest concepts. Fortunately, classic statistical learning
techniques such as linear and softmax regression can be cast as <b>linear</b>
neural networks. Starting from these classic algorithms, we will
introduce you to the basics, providing the basis for more complex
techniques in the rest of the book.
</p>
</div>
<div id="outline-container-lin_reg" class="outline-3">
<h3 id="lin_reg"><span class="section-number-3">3.1.</span> Linear Regression</h3>
<div class="outline-text-3" id="text-lin_reg">
<p>
<b>Regression</b> refers to a set of methods for modeling the relationship
between one or more independent variables and a dependent variable. In
the natural sciences and social sciences, the purpose of regression is
most often to <b>characterize</b> the relationship between the inputs and
outputs. Machine learning, on the other hand, is most often concerned
with <b>prediction</b>.
</p>

<p>
Regression problems pop up whenever we want to predict a numerical
value. Common examples include predicting prices (of homes, stocks,
etc.), predicting length of stay (for patients in the hospital), demand
forecasting (for retail sales), among countless others. Not every
prediction problem is a classic regression problem. In subsequent
sections, we will introduce <b>classification problems</b>, where the goal is
to predict membership among a set of categories.
</p>
</div>

<div id="outline-container-orge96e1a0" class="outline-4">
<h4 id="orge96e1a0"><span class="section-number-4">3.1.1.</span> Basic Elements of Linear Regression</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
<b>Linear regression</b> may be both the simplest and most popular among the
standard tools to regression. Dating back to the dawn of the 19th
century, linear regression flows from a few simple assumptions. First,
we assume that the relationship between the independent variables
\(\mathbf{x}\) and the dependent variable \(y\) is linear, i.e., that
\(y\) can be expressed as a weighted sum of the elements in
\(\mathbf{x}\), given some noise on the observations. Second, we
assume that any noise is well-behaved (following a Gaussian
distribution).
</p>

<p>
To motivate the approach, let us start with a running example. Suppose
that we wish to estimate the prices of houses (in dollars) based on
their area (in square feet) and age (in years). To actually develop a
model for predicting house prices, we would need to get our hands on a
dataset consisting of sales for which we know the sale price, area, and
age for each home. In the terminology of machine learning, the dataset
is called a <b>training dataset</b> or <b>training set</b>, and each row (here the
data corresponding to one sale) is called an <b>example</b> (or <b>data point</b>,
<b>data instance</b>, <b>sample</b>). The thing we are trying to predict (price)
is called a <b>label</b> (or <b>target</b>). The independent variables (age and
area) upon which the predictions are based are called <b>features</b> (or
<b>covariates</b>).
</p>

<p>
Typically, we will use \(n\) to denote the number of examples in our
dataset. We index the data examples by \(i\), denoting each input as
\(\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\top\) and the
corresponding label as \(y^{(i)}\).
</p>
</div>

<div id="outline-container-org7dd6dc1" class="outline-5">
<h5 id="org7dd6dc1"><span class="section-number-5">3.1.1.1.</span> Linear Model</h5>
<div class="outline-text-5" id="text-3-1-1-1">
<p>
The linearity assumption just says that the target (price) can be
expressed as a weighted sum of the features (area and age):
</p>

\begin{equation}
\label{org21b6b79}
\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.
\end{equation}

<p>
In equation \eqref{org21b6b79}, \(w_{\mathrm{area}}\) and
\(w_{\mathrm{age}}\) are called <b>weights</b>, and \(b\) is called a <b>bias</b>
(also called an <b>offset</b> or <b>intercept</b>). The weights determine the
influence of each feature on our prediction and the bias just says
what value the predicted price should take when all of the features
take value 0. Even if we will never see any homes with zero area, or
that are precisely zero years old, we still need the bias or else we
will limit the expressivity of our model. Strictly speaking, equation
\eqref{org21b6b79} is an <b>affine transformation</b> of input features, which is
characterized by a <b>linear transformation</b> of features via weighted sum,
combined with a <b>translation</b> via the added bias.
</p>

<p>
Given a dataset, our goal is to choose the weights \(\mathbf{w}\)
and the bias \(b\) such that on average, the predictions made
according to our model best fit the true prices observed in the data.
Models whose output prediction is determined by the affine
transformation of input features are <b>linear models</b>, where the affine
transformation is specified by the chosen weights and bias.
</p>

<p>
In disciplines where it is common to focus on datasets with just a few
features, explicitly expressing models long-form like this is common. In
machine learning, we usually work with high-dimensional datasets, so it
is more convenient to employ linear algebra notation. When our inputs
consist of \(d\) features, we express our prediction \(\hat{y}\)
(in general the &ldquo;hat&rdquo; symbol denotes estimates) as
</p>

\begin{equation}
\hat{y} = w_1  x_1 + ... + w_d  x_d + b.
\end{equation}

<p>
Collecting all features into a vector \(\mathbf{x} \in \mathbb{R}^d\)
and all weights into a vector \(\mathbf{w} \in \mathbb{R}^d\), we can
express our model compactly using a dot product:
</p>

\begin{equation}
\label{org89c0d05}
\hat{y} = \mathbf{w}^\top \mathbf{x} + b.
\end{equation}

<p>
In equation \eqref{org89c0d05}, the vector \(\mathbf{x}\) corresponds to
features of a single data example. We will often find it convenient to
refer to features of our entire dataset of \(n\) examples via the
<b>design matrix</b> \(\mathbf{X} \in \mathbb{R}^{n \times d}\). Here,
\(\mathbf{X}\) contains one row for every example and one column for
every feature.
</p>

<p>
For a collection of features \(\mathbf{X}\), the predictions
\(\hat{\mathbf{y}} \in \mathbb{R}^n\) can be expressed via the
matrix-vector product:
</p>

\begin{equation}
{\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,
\end{equation}

<p>
where broadcasting (see <a href="2.1-data-manipulation.html#9dcbe412-db7e-485a-bb3c-d7181f2f7f05">2.1-data-manipulation.html#9dcbe412-db7e-485a-bb3c-d7181f2f7f05</a>) is
applied during the summation. Given features of a training dataset
\(\mathbf{X}\) and corresponding (known) labels \(\mathbf{y}\), the
goal of linear regression is to find the weight vector \(\mathbf{w}\)
and the bias term \(b\) that given features of a new data example
sampled from the same distribution as \(\mathbf{X}\), the new
example&rsquo;s label will (in expectation) be predicted with the lowest
error.
</p>

<p>
Even if we believe that the best model for predicting \(y\) given
\(\mathbf{x}\) is linear, we would not expect to find a real-world
dataset of \(n\) examples where \(y^{(i)}\) exactly equals
\(\mathbf{w}^\top \mathbf{x}^{(i)}+b\) for all
\(1 \leq i \leq n\). For example, whatever instruments we use to
observe the features \(\mathbf{X}\) and labels \(\mathbf{y}\)
might suffer small amount of measurement error. Thus, even when we are
confident that the underlying relationship is linear, we will
incorporate a noise term to account for such errors.
</p>

<p>
Before we can go about searching for the best <b>parameters</b> (or <b>model
parameters</b>) \(\mathbf{w}\) and \(b\), we will need two more
things: (i) a quality measure for some given model; and (ii) a procedure
for updating the model to improve its quality.
</p>
</div>
</div>

<div id="outline-container-org78446b1" class="outline-5">
<h5 id="org78446b1"><span class="section-number-5">3.1.1.2.</span> Loss Function</h5>
<div class="outline-text-5" id="text-3-1-1-2">
<p>
Before we start thinking about how to <b>fit</b> data with our model, we need
to determine a measure of <b>fitness</b>. The <b>loss function</b> quantifies the
distance between the <b>real</b> and <b>predicted</b> value of the target. The loss
will usually be a non-negative number where smaller values are better
and perfect predictions incur a loss of 0. The most popular loss
function in regression problems is the squared error. When our
prediction for an example \(i\) is \(\hat{y}^{(i)}\) and the
corresponding true label is \(y^{(i)}\), the squared error is given
by:
</p>

\begin{equation}
l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.
\end{equation}

<p>
The constant \(\frac{1}{2}\) makes no real difference but will prove
notationally convenient, canceling out when we take the derivative of
the loss. Since the training dataset is given to us, and thus out of
our control, the empirical error is only a function of the model
parameters.  To make things more concrete, consider the example below
where we plot a regression problem for a one-dimensional case as shown
in Fig <a href="#org2240470">1</a>.
</p>


<div id="org2240470" class="figure">
<p><img src="http://d2l.ai/_images/fit-linreg.svg" alt="fit-linreg.svg" class="org-svg" />
</p>
<p><span class="figure-number">Figure 1: </span>Fit data with a linear model.</p>
</div>

<p>
Note that large differences between estimates \(\hat{y}^{(i)}\) and
observations \(y^{(i)}\) lead to even larger contributions to the
loss, due to the quadratic dependence. To measure the quality of a
model on the entire dataset of \(n\) examples, we simply average (or
equivalently, sum) the losses on the training set.
</p>

\begin{equation}
L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.
\end{equation}

<p>
When training the model, we want to find parameters (\(\mathbf{w}^*,
b^*\)) that minimize the total loss across all training examples:
</p>

\begin{equation}
\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).
\end{equation}
</div>
</div>

<div id="outline-container-org4785318" class="outline-5">
<h5 id="org4785318"><span class="section-number-5">3.1.1.3.</span> Analytic Solution</h5>
<div class="outline-text-5" id="text-3-1-1-3">
<p>
Linear regression happens to be an unusually simple optimization
problem. Unlike most other models that we will encounter in this book,
linear regression can be solved analytically by applying a simple
formula. To start, we can subsume the bias \(b\) into the parameter
\(\mathbf{w}\) by appending a column to the design matrix consisting
of all ones. Then our prediction problem is to minimize
\(\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2\). There is just one
critical point on the loss surface and it corresponds to the minimum
of the loss over the entire domain. Taking the derivative of the loss
with respect to \(\mathbf{w}\) and setting it equal to zero yields the
analytic (closed-form) solution:
</p>

\begin{equation}
\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.
\end{equation}

<p>
While simple problems like linear regression may admit analytic
solutions, you should not get used to such good fortune. Although
analytic solutions allow for nice mathematical analysis, the
requirement of an analytic solution is so restrictive that it would
exclude all of deep learning.
</p>
</div>
</div>

<div id="outline-container-org5005ba1" class="outline-5">
<h5 id="org5005ba1"><span class="section-number-5">3.1.1.4.</span> Minibatch Stochastic Gradient Descent</h5>
<div class="outline-text-5" id="text-3-1-1-4">
<p>
Even in cases where we cannot solve the models analytically, it turns
out that we can still train models effectively in practice. Moreover,
for many tasks, those difficult-to-optimize models turn out to be so
much better that figuring out how to train them ends up being well worth
the trouble.
</p>

<p>
The key technique for optimizing nearly any deep learning model, and
which we will call upon throughout this book, consists of iteratively
reducing the error by updating the parameters in the direction that
incrementally lowers the loss function. This algorithm is called
<b>gradient descent</b>.
</p>

<p>
The most naive application of gradient descent consists of taking the
derivative of the loss function, which is an average of the losses
computed on every single example in the dataset. In practice, this can
be extremely slow: we must pass over the entire dataset before making a
single update. Thus, we will often settle for sampling a random
minibatch of examples every time we need to compute the update, a
variant called <b>minibatch stochastic gradient descent</b>.
</p>

<p>
In each iteration, we first randomly sample a minibatch
\(\mathcal{B}\) consisting of a fixed number of training examples.  We
then compute the derivative (gradient) of the average loss on the
minibatch with regard to the model parameters. Finally, we multiply
the gradient by a predetermined positive value \(\eta\) and subtract
the resulting term from the current parameter values.
</p>

<p>
We can express the update mathematically as follows (\(\partial\)
denotes the partial derivative):
</p>

\begin{equation}
(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).
\end{equation}

<p>
To summarize, steps of the algorithm are the following: (i) we
initialize the values of the model parameters, typically at random;
(ii) we iteratively sample random minibatches from the data, updating
the parameters in the direction of the negative gradient. For
quadratic losses and affine transformations, we can write this out
explicitly as follows:
</p>

\begin{equation}
\label{org3306916}
\begin{aligned}
\mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\
b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
\end{aligned}
\end{equation}

<p>
Note that \(\mathbf{w}\) and \(\mathbf{x}\) are vectors in equation
\eqref{org3306916}. Here, the more elegant vector notation makes
the math much more readable than expressing things in terms of
coefficients, say \(w_1, w_2, \ldots, w_d\). The set cardinality
\(|\mathcal{B}|\) represents the number of examples in each minibatch
(the <b>batch size</b>) and \(\eta\) denotes the <b>learning rate</b>. We emphasize
that the values of the batch size and learning rate are manually
pre-specified and not typically learned through model training. These
parameters that are tunable but not updated in the training loop are
called <b>hyperparameters</b>. <b>Hyperparameter tuning</b> is the process by which
hyperparameters are chosen, and typically requires that we adjust them
based on the results of the training loop as assessed on a separate
<b>validation dataset</b> (or <b>validation set</b>).
</p>

<p>
After training for some predetermined number of iterations (or until
some other stopping criteria are met), we record the estimated model
parameters, denoted \(\hat{\mathbf{w}}, \hat{b}\). Note that even if
our function is truly linear and noiseless, these parameters will not
be the exact minimizers of the loss because, although the algorithm
converges slowly towards the minimizers it cannot achieve it exactly
in a finite number of steps.
</p>

<p>
Linear regression happens to be a learning problem where there is only
one minimum over the entire domain. However, for more complicated
models, like deep networks, the loss surfaces contain many minima.
Fortunately, for reasons that are not yet fully understood, deep
learning practitioners seldom struggle to find parameters that
minimize the loss <b>on training sets</b>. The more formidable task is to
find parameters that will achieve low loss on data that we have not
seen before, a challenge called <b>generalization</b>. We return to these
topics throughout the book.
</p>
</div>
</div>

<div id="outline-container-org1fcf043" class="outline-5">
<h5 id="org1fcf043"><span class="section-number-5">3.1.1.5.</span> Making Predictions with the Learned Model</h5>
<div class="outline-text-5" id="text-3-1-1-5">
<p>
Given the learned linear regression model \(\hat{\mathbf{w}}^\top
\mathbf{x} + \hat{b}\), we can now estimate the price of a new house
(not contained in the training data) given its area \(x_1\) and age
\(x_2\). Estimating targets given features is commonly called
<b>prediction</b> or <b>inference</b>.
</p>

<p>
We will try to stick with <b>prediction</b> because calling this step
<b>inference</b>, despite emerging as standard jargon in deep learning, is
somewhat of a misnomer. In statistics, <b>inference</b> more often denotes
estimating parameters based on a dataset. This misuse of terminology
is a common source of confusion when deep learning practitioners talk
to statisticians.
</p>
</div>
</div>
</div>

<div id="outline-container-orgebbd0ba" class="outline-4">
<h4 id="orgebbd0ba"><span class="section-number-4">3.1.2.</span> Vectorization for Speed</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
When training our models, we typically want to process whole
minibatches of examples simultaneously. Doing this efficiently
requires that we vectorize the calculations and leverage fast linear
algebra libraries rather than writing costly loops in Clojure.
</p>

<p>
We will use <a href="https://github.com/dm3/stopwatch">dm3/stopwatch</a> to measure the time duration.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.linear_regression</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>stopwatch.core <span style="color: #110099;">:as</span> stopwatch<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.chart <span style="color: #110099;">:as</span> chart<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-chart.plot <span style="color: #110099;">:as</span> plot<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
To illustrate why this matters so much, we can consider two methods
for adding vectors. To start we instantiate two 10000-dimensional
vectors containing all ones. In one method we will loop over the
vectors with a Clojure <code>doseq</code>. In the other method we will rely on
a single call to <code>+</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">n</span> 10000<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">a</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/ones ndm <span style="color: #909183;">[</span>n<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">b</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/ones ndm <span style="color: #909183;">[</span>n<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">c</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/zeros ndm <span style="color: #909183;">[</span>n<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Now we can benchmark the workloads. First, we add them, one coordinate
at a time, using a <code>doseq</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>elapsed <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">stopwatch</span>/start<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span>i <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> n<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">set</span> c <span style="color: #709870;">[</span>i<span style="color: #709870;">]</span> <span style="color: #709870;">(</span>+ <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element a <span style="color: #6276ba;">[</span>i<span style="color: #6276ba;">]</span><span style="color: #907373;">)</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element b <span style="color: #6276ba;">[</span>i<span style="color: #6276ba;">]</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Elapsed: "</span> <span style="color: #909183;">(</span>/ <span style="color: #709870;">(</span>elapsed<span style="color: #709870;">)</span> 1e9<span style="color: #909183;">)</span> <span style="color: #2A00FF;">"sec"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Elapsed:  4.044424873 sec
</pre>



<p>
Alternatively, we rely on the <code>clj-djl.ndarray/+</code> operator to compute
the elementwise sum.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>elapsed <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">stopwatch</span>/start<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ a b<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Elapsed: "</span> <span style="color: #909183;">(</span>/ <span style="color: #709870;">(</span>elapsed<span style="color: #709870;">)</span> 1e9<span style="color: #909183;">)</span> <span style="color: #2A00FF;">"sec"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Elapsed:  1.64916E-4 sec
</pre>


<p>
You probably noticed that the second method is dramatically faster
than the first. Vectorizing code often yields order-of-magnitude
speedups.  Moreover, we push more of the mathematics to the library
and need not write as many calculations ourselves, reducing the
potential for errors.
</p>
</div>
</div>

<div id="outline-container-org30ac4aa" class="outline-4">
<h4 id="org30ac4aa"><span class="section-number-4">3.1.3.</span> The Normal Distribution and Squared Loss</h4>
<div class="outline-text-4" id="text-3-1-3">
<p>
While you can already get your hands dirty using only the information
above, in the following we can more formally motivate the squared loss
objective via assumptions about the distribution of noise.
</p>

<p>
Linear regression was invented by Gauss in 1795, who also discovered
the <b>normal distribution</b> (also called the <b>Gaussian</b>). It turns out that
the connection between the normal distribution and linear regression
runs deeper than common parentage. To refresh your memory, the
probability density of a normal distribution with mean \(\mu\) and
variance \(\sigma^2\) (standard deviation \(\sigma\)) is given as
</p>

\begin{equation}
p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).
\end{equation}

<p>
Below we define a Clojure function to compute the normal distribution.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">normal</span> <span style="color: #7388d6;">[</span>x mu sigma<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>p <span style="color: #709870;">(</span>/ 1.0 <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/sqrt <span style="color: #6276ba;">(</span>* 2 <span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/PI <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow sigma 2<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/exp <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/pow <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- x mu<span style="color: #858580;">)</span> 2<span style="color: #6276ba;">)</span>
                        <span style="color: #6276ba;">(</span>/ -0.5 <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">Math</span>/pow sigma 2<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span>
          p<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">x</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/arange ndm -7. 7. 0.01<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">params</span> <span style="color: #7388d6;">[</span><span style="color: #909183;">[</span>0 1<span style="color: #909183;">]</span> <span style="color: #909183;">[</span>0 2<span style="color: #909183;">]</span> <span style="color: #909183;">[</span>3 1<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>titles <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> <span style="color: #2A00FF;">"mean "</span> <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #000000;">%</span><span style="color: #907373;">)</span> <span style="color: #2A00FF;">", std "</span> <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> <span style="color: #000000;">%</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span> params<span style="color: #909183;">)</span>
      yss <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> #<span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #907373;">(</span>normal x <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #000000;">%</span><span style="color: #6276ba;">)</span> <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> <span style="color: #000000;">%</span><span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span> params<span style="color: #909183;">)</span>
      xs <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec x<span style="color: #909183;">)</span>
      series <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">fn</span> <span style="color: #907373;">[</span>t ys-<span style="color: #907373;">]</span> <span style="color: #907373;">{</span><span style="color: #110099;">:name</span> t <span style="color: #110099;">:xs</span> xs <span style="color: #110099;">:ys</span> ys-<span style="color: #907373;">}</span><span style="color: #709870;">)</span> titles yss<span style="color: #909183;">)</span>
      c <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">chart</span>/line <span style="color: #709870;">{</span><span style="color: #110099;">:series</span> series<span style="color: #709870;">}</span><span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">plot</span>/store! c <span style="color: #110099;">nil</span> <span style="color: #2A00FF;">"notes/figures/normal-distribution.svg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="org0e6cd52" class="figure">
<p><img src="figures/normal-distribution.svg" alt="normal-distribution.svg" class="org-svg" />
</p>
</div>

<p>
As we can see, changing the mean corresponds to a shift along the
\(x\)-axis, and increasing the variance spreads the distribution out,
lowering its peak.
</p>

<p>
One way to motivate linear regression with the mean squared error loss
function (or simply squared loss) is to formally assume that
observations arise from noisy observations, where the noise is normally
distributed as follows:
</p>

\begin{equation}
y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, \sigma^2).
\end{equation}

<p>
Thus, we can now write out the <b>likelihood</b> of seeing a particular \(y\)
for a given \(\mathbf{x}\) via
</p>

\begin{equation}
P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).
\end{equation}

<p>
Now, according to the principle of maximum likelihood, the best values
of parameters \(\mathbf{w}\) and \(b\) are those that maximize the
<b>likelihood</b> of the entire dataset:
</p>

\begin{equation}
P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).
\end{equation}


<p>
Estimators chosen according to the principle of maximum likelihood are
called <b>maximum likelihood estimators</b>. While, maximizing the product of
many exponential functions, might look difficult, we can simplify
things significantly, without changing the objective, by maximizing
the log of the likelihood instead. For historical reasons,
optimizations are more often expressed as minimization rather than
maximization. So, without changing anything we can minimize the
<b>negative log-likelihood</b> \(-\log P(\mathbf y \mid \mathbf X)\). Working
out the mathematics gives us:
</p>

\begin{equation}
-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.
\end{equation}

<p>
Now we just need one more assumption that \(\sigma\) is some fixed
constant. Thus we can ignore the first term because it does not depend
on \(\mathbf{w}\) or \(b\). Now the second term is identical to the
squared error loss introduced earlier, except for the multiplicative
constant \(\frac{1}{\sigma^2}\). Fortunately, the solution does not
depend on \(\sigma\). It follows that minimizing the mean squared
error is equivalent to maximum likelihood estimation of a linear model
under the assumption of additive Gaussian noise.
</p>
</div>
</div>


<div id="outline-container-org822f80f" class="outline-4">
<h4 id="org822f80f"><span class="section-number-4">3.1.4.</span> From Linear Regression to Deep Networks</h4>
<div class="outline-text-4" id="text-3-1-4">
<p>
So far we only talked about linear models. While neural networks cover
a much richer family of models, we can begin thinking of the linear
model as a neural network by expressing it in the language of neural
networks.  To begin, let us start by rewriting things in a &ldquo;layer&rdquo;
notation.
</p>
</div>

<div id="outline-container-orgcdb607b" class="outline-5">
<h5 id="orgcdb607b"><span class="section-number-5">3.1.4.1.</span> Neural Network Diagram</h5>
<div class="outline-text-5" id="text-3-1-4-1">
<p>
Deep learning practitioners like to draw diagrams to visualize what is
happening in their models. In fig <a href="#org99856e8">2</a>, we depict our
linear regression model as a neural network. Note that these diagrams
highlight the connectivity pattern such as how each input is connected
to the output, but not the values taken by the weights or biases.
</p>


<div id="org99856e8" class="figure">
<p><img src="figures/singleneuron.svg" alt="singleneuron.svg" class="org-svg" />
</p>
<p><span class="figure-number">Figure 2: </span>Linear regression is a single-layer neural network.</p>
</div>

<p>
For the neural network shown in fig <a href="#org99856e8">2</a>, the inputs are
\(x_1, \ldots, x_d\), so the <b>number of inputs</b> (or <b>feature
dimensionality</b>) in the input layer is \(d\). The output of the network
in fig <a href="#org99856e8">2</a> is \(o_1\), so the <b>number of outputs</b> in the
output layer is 1. Note that the input values are all <b>given</b> and there
is just a single <b>computed</b> neuron. Focusing on where computation takes
place, conventionally we do not consider the input layer when counting
layers. That is to say, the <b>number of layers</b> for the neural network in
fig <a href="#org99856e8">2</a> is 1. We can think of linear regression models
As neural networks consisting of just a single artificial neuron, or
as single-layer neural networks.
</p>

<p>
Since for linear regression, every input is connected to every output
(in this case there is only one output), we can regard this
transformation (the output layer in fig <a href="#org99856e8">2</a> as a
<b>fully-connected layer</b> or <b>dense layer</b>. We will talk a lot more about
networks composed of such layers in the next chapter.
</p>
</div>
</div>

<div id="outline-container-orga7ae26c" class="outline-5">
<h5 id="orga7ae26c"><span class="section-number-5">3.1.4.2.</span> Biology</h5>
<div class="outline-text-5" id="text-3-1-4-2">
<p>
Since linear regression (invented in 1795) predates computational
neuroscience, it might seem anachronistic to describe linear
regression as a neural network. To see why linear models were a
natural place to begin when the cyberneticists/neurophysiologists
Warren McCulloch and Walter Pitts began to develop models of
artificial neurons, consider the cartoonish picture of a biological
neuron in fig <a href="#org47c7f51">3</a>, consisting of <b>dendrites</b> (input terminals),
the <b>nucleus</b> (CPU), the <b>axon</b> (output wire), and the <b>axon terminals</b>
(output terminals), enabling connections to other neurons via
<b>synapses</b>.
</p>


<div id="org47c7f51" class="figure">
<p><img src="figures/neuron.svg" alt="neuron.svg" class="org-svg" />
</p>
<p><span class="figure-number">Figure 3: </span>The real neuron.</p>
</div>


<p>
Information \(x_i\) arriving from other neurons (or environmental
sensors such as the retina) is received in the dendrites. In particular,
that information is weighted by <b>synaptic weights</b> \(w_i\)
determining the effect of the inputs (e.g., activation or inhibition via
the product \(x_i w_i\)). The weighted inputs arriving from multiple
sources are aggregated in the nucleus as a weighted sum
\(y = \sum_i x_i w_i + b\), and this information is then sent for
further processing in the axon \(y\), typically after some nonlinear
processing via \(\sigma(y)\). From there it either reaches its
destination (e.g., a muscle) or is fed into another neuron via its
dendrites.
</p>

<p>
Certainly, the high-level idea that many such units could be cobbled
together with the right connectivity and right learning algorithm, to
produce far more interesting and complex behavior than any one neuron
alone could express owes to our study of real biological neural systems.
</p>

<p>
At the same time, most research in deep learning today draws little
direct inspiration in neuroscience. We invoke Stuart Russell and Peter
Norvig who, in their classic AI text book <b>Artificial Intelligence: A
Modern Approach</b> [Russell &amp; Norvig, 2016], pointed out that although
airplanes might have been <b>inspired</b> by birds, ornithology has not been
the primary driver of aeronautics innovation for some
centuries. Likewise, inspiration in deep learning these days comes in
equal or greater measure from mathematics, statistics, and computer
science.
</p>
</div>
</div>
</div>

<div id="outline-container-orga45d194" class="outline-4">
<h4 id="orga45d194"><span class="section-number-4">3.1.5.</span> Summary</h4>
<div class="outline-text-4" id="text-3-1-5">
<ul class="org-ul">
<li>Key ingredients in a machine learning model are training data, a loss
function, an optimization algorithm, and quite obviously, the model
itself.</li>
<li>Vectorizing makes everything better (mostly math) and faster (mostly
code).</li>
<li>Minimizing an objective function and performing maximum likelihood
estimation can mean the same thing.</li>
<li>Linear regression models are neural networks, too.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-lin-reg-scratch" class="outline-3">
<h3 id="lin-reg-scratch"><span class="section-number-3">3.2.</span> Linear Regression Implementation from Scratch</h3>
<div class="outline-text-3" id="text-lin-reg-scratch">
<p>
Now that you understand the key ideas behind linear regression, we can
begin to work through a hands-on implementation in code. In this
section, we will implement the entire method from scratch, including
the data pipeline, the model, the loss function, and the minibatch
stochastic gradient descent optimizer. While modern deep learning
frameworks can automate nearly all of this work, implementing things
from scratch is the only way to make sure that you really know what
you are doing. Moreover, when it comes time to customize models,
defining our own layers or loss functions, understanding how things
work under the hood will prove handy. In this section, we will rely
only on tensors and auto differentiation. Afterwards, we will
introduce a more concise implementation, taking advantage of bells and
whistles of deep learning frameworks.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.linreg</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.device <span style="color: #110099;">:as</span> device<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.engine <span style="color: #110099;">:as</span> engine<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training <span style="color: #110099;">:as</span> t<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.dataset <span style="color: #110099;">:as</span> ds<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>

<div id="outline-container-orgcf29b3d" class="outline-4">
<h4 id="orgcf29b3d"><span class="section-number-4">3.2.1.</span> Generating the Dataset</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
To keep things simple, we will construct an artificial dataset
according to a linear model with additive noise. Our task will be to
recover this model&rsquo;s parameters using the finite set of examples
contained in our dataset. We will keep the data low-dimensional so we
can visualize it easily. In the following code snippet, we generate a
dataset containing 1000 examples, each consisting of 2 features
sampled from a standard normal distribution. Thus our synthetic
dataset will be a matrix \(\mathbf{X}\in \mathbb{R}^{1000 \times 2}\).
</p>

<p>
The true parameters generating our dataset will be \(\mathbf{w} = [2,
-3.4]^\top\) and \(b = 4.2\), and our synthetic labels will be
assigned according to the following linear model with the noise term
\(\epsilon\):
</p>

\begin{equation}
\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.
\end{equation}


<p>
You could think of \(\epsilon\) as capturing potential measurement
errors on the features and labels. We will assume that the standard
assumptions hold and thus that \(\epsilon\) obeys a normal
distribution with mean of 0. To make our problem easy, we will set its
standard deviation to 0.01. The following code generates our synthetic
dataset.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">synthetic-data</span> <span style="color: #7388d6;">[</span>ndm w b num<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>X <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm <span style="color: #907373;">[</span>num <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size w<span style="color: #6276ba;">)</span><span style="color: #907373;">]</span><span style="color: #709870;">)</span>
        y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot X w<span style="color: #907373;">)</span> b<span style="color: #709870;">)</span>
        noise <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm 0 0.01 <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape y<span style="color: #907373;">)</span> <span style="color: #110099;">:float32</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">[</span>X <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ y noise<span style="color: #709870;">)</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/new-base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">true-w</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">float-array</span> <span style="color: #709870;">[</span>2 -3.4<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">true-b</span> 4.2<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">dp</span> <span style="color: #7388d6;">(</span>synthetic-data ndm true-w true-b 1000<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">features</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> dp 0<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">labels</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> dp 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Note that each row in features consists of a 2-dimensional data
example and that each row in labels consists of a 1-dimensional label
value (a scalar).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>str <span style="color: #2A00FF;">"features(0): "</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> features <span style="color: #709870;">[</span>0<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
     <span style="color: #2A00FF;">"</span><span style="color: #000000; background-color: #f8f8f8; font-weight: bold;">\n</span><span style="color: #2A00FF;">labels(0): "</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element labels <span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
features(0): [1.1630785 2.2122061]
labels(0): -1.0015316
</pre>


<p>
By generating a scatter plot using the second feature and <code>labels</code>, we
can clearly observe the linear correlation between the two.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> features <span style="color: #2A00FF;">":, 1"</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
      y <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec labels<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">d2l</span>/plot-scatter
   <span style="color: #2A00FF;">"notes/figures/synthetic_data.svg"</span>
   <span style="color: #2A00FF;">"data"</span>
   x
   y<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div id="orgd81b99d" class="figure">
<p><img src="figures/synthetic_data.svg" alt="synthetic_data.svg" class="org-svg" />
</p>
</div>
</div>
</div>


<div id="outline-container-orgf5ec15e" class="outline-4">
<h4 id="orgf5ec15e"><span class="section-number-4">3.2.2.</span> Reading the Dataset</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
Recall that training models consists of making multiple passes over the
dataset, grabbing one minibatch of examples at a time, and using them to
update our model. Since this process is so fundamental to training
machine learning algorithms, it is worth defining a utility function to
shuffle the dataset and access it in minibatches.
</p>

<p>
In the following code, we define the <code>data-iter</code> function to demonstrate
one possible implementation of this functionality. The function takes
a batch size, a matrix of features, and a vector of labels, yielding
minibatches of the size <code>batch-size</code>. Each minibatch consists of a tuple
of features and labels.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">data-iter</span> <span style="color: #7388d6;">[</span>batch-size features labels<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>num-examples <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size features<span style="color: #709870;">)</span>
        indices <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> num-examples<span style="color: #709870;">)</span>
        indices <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">shuffle</span> indices<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">for</span> <span style="color: #709870;">[</span>i <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> 0 num-examples batch-size<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">[</span><span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;&gt;</span> <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> i <span style="color: #858580;">(</span><span style="color: #7F0055; font-weight: bold;">min</span> <span style="color: #80a880;">(</span>+ i batch-size<span style="color: #80a880;">)</span> num-examples<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span>map #<span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> features <span style="color: #80a880;">[</span><span style="color: #000000;">%</span><span style="color: #80a880;">]</span><span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/stack<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
       <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;&gt;</span> <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> i <span style="color: #858580;">(</span><span style="color: #7F0055; font-weight: bold;">min</span> <span style="color: #80a880;">(</span>+ i batch-size<span style="color: #80a880;">)</span> num-examples<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span>map #<span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> labels <span style="color: #80a880;">[</span><span style="color: #000000;">%</span><span style="color: #80a880;">]</span><span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
           <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/stack<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
In general, note that we want to use reasonably sized minibatches to
take advantage of the GPU hardware, which excels at parallelizing
operations. Because each example can be fed through our models in
parallel and the gradient of the loss function for each example can also
be taken in parallel, GPUs allow us to process hundreds of examples in
scarcely more time than it might take to process just a single example.
</p>

<p>
To build some intuition, let us read and print the first small batch
of data examples. The shape of the features in each minibatch tells us
both the minibatch size and the number of input features. Likewise,
our minibatch of labels will have a shape given by <code>batch-size</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #909183;">(</span>data-iter 10 features labels<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example" id="orga396c8f">
ND: (10, 2) cpu() float32
[[ 1.1631,  2.2122],
 [ 0.4838,  0.774 ],
 [ 0.2996,  1.0434],
 [ 0.153 ,  1.1839],
 [-1.1688,  1.8917],
 [ 1.5581, -1.2347],
 [-0.5459, -1.771 ],
 [-2.3556, -0.4514],
 [ 0.5414,  0.5794],
 [ 2.6785, -1.8561],
]
</pre>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #909183;">(</span>data-iter 10 features labels<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (10) cpu() float32
[-1.0015,  2.5281,  1.2197,  0.4807, -4.5658, 11.5155,  9.1389,  1.0271,  3.3058, 15.8548]
</pre>


<p>
As we run the iteration, we obtain distinct minibatches successively
until the entire dataset has been exhausted (try this). While the
iteration implemented above is good for didactic purposes, it is
inefficient in ways that might get us in trouble on real problems. For
example, it requires that we load all the data in memory and that we
perform lots of random memory access. The built-in iterators
implemented in a deep learning framework are considerably more
efficient and they can deal with both data stored in files and data
fed via data streams.
</p>
</div>
</div>

<div id="outline-container-orgb2c811b" class="outline-4">
<h4 id="orgb2c811b"><span class="section-number-4">3.2.3.</span> Initializing Model Parameters</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
Before we can begin optimizing our model&rsquo;s parameters by minibatch
stochastic gradient descent, we need to have some parameters in the
first place. In the following code, we initialize weights by sampling
random numbers from a normal distribution with mean 0 and a standard
deviation of 0.01, and setting the bias to 0.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">w</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm 0 0.01 <span style="color: #909183;">[</span>2 1<span style="color: #909183;">]</span> <span style="color: #110099;">:float32</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">b</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/zeros ndm <span style="color: #909183;">[</span>1<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/set-requires-gradient w <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/set-requires-gradient b <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec w<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec b<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
[0.0013263007 0.0072455113]
[0.0]
</pre>


<p>
After initializing our parameters, our next task is to update them until
they fit our data sufficiently well. Each update requires taking the
gradient of our loss function with respect to the parameters. Given this
gradient, we can update each parameter in the direction that may reduce
the loss.
</p>

<p>
Since nobody wants to compute gradients explicitly (this is tedious
and error prone), we use automatic differentiation, as introduced in
Section
<a href="2.5-automatic-differentiation.html#automatic_differentiation">2.5-automatic-differentiation.html#automatic_differentiation</a>,
to compute the gradient.
</p>
</div>
</div>

<div id="outline-container-org575f338" class="outline-4">
<h4 id="org575f338"><span class="section-number-4">3.2.4.</span> Defining the Model</h4>
<div class="outline-text-4" id="text-3-2-4">
<p>
Next, we must define our model, relating its inputs and parameters to
its outputs. Recall that to calculate the output of the linear model,
we simply take the matrix-vector dot product of the input features
\(\mathbf{X}\) and the model weights \(\mathbf{w}\), and add the
offset \(b\) to each example. Note that below \(\mathbf{Xw}\) is a
vector and \(b\) is a scalar. Recall the broadcasting mechanism as
described in Section
<a href="2.1-data-manipulation.html#9dcbe412-db7e-485a-bb3c-d7181f2f7f05">2.1-data-manipulation.html#9dcbe412-db7e-485a-bb3c-d7181f2f7f05</a>. When
we add a vector and a scalar, the scalar is added to each component of
the vector.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">linreg</span>
  <span style="color: #2A00FF;">"The linear regression model."</span>
  <span style="color: #7388d6;">[</span>X w b<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot X w<span style="color: #909183;">)</span> b<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org319ddef" class="outline-4">
<h4 id="org319ddef"><span class="section-number-4">3.2.5.</span> Defining the Loss Function</h4>
<div class="outline-text-4" id="text-3-2-5">
<p>
Since updating our model requires taking the gradient of our loss
function, we ought to define the loss function first. Here we will use
the squared loss function as described in Section
<a href="3.1-linear-regression.html#lin_reg">3.1-linear-regression.html#lin_reg</a>. In the implementation,
we need to transform the true value <code>y</code> into the predicted value&rsquo;s shape
<code>y-hat</code>. The result returned by the following function will also have
the same shape as <code>y-hat</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">squared-loss</span> <span style="color: #7388d6;">[</span>y-hat y<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- y-hat <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape y <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape y-hat<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span>
              <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- y-hat <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape y <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape y-hat<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
        2<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org89bfe83" class="outline-4">
<h4 id="org89bfe83"><span class="section-number-4">3.2.6.</span> Defining the Optimization Algorithm</h4>
<div class="outline-text-4" id="text-3-2-6">
<p>
As we discussed in Section
<a href="3.1-linear-regression.html#lin_reg">3.1-linear-regression.html#lin_reg</a>, linear regression has a
closed-form solution. However, this is not a book about linear
regression: it is a book about deep learning. Since none of the other
models that this book introduces can be solved analytically, we will
take this opportunity to introduce your first working example of
minibatch stochastic gradient descent.
</p>

<p>
At each step, using one minibatch randomly drawn from our dataset, we
will estimate the gradient of the loss with respect to our parameters.
Next, we will update our parameters in the direction that may reduce
the loss. The following code applies the minibatch stochastic gradient
descent update, given a set of parameters, a learning rate, and a
batch size. The size of the update step is determined by the learning
rate <code>lr</code>. Because our loss is calculated as a sum over the minibatch of
examples, we normalize our step size by the batch size (<code>batch-size</code>),
so that the magnitude of a typical step size does not depend heavily
on our choice of the batch size.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">sgd</span>
  <span style="color: #2A00FF;">"Minibatch stochastic gradient descent."</span>
  <span style="color: #7388d6;">[</span>params lr batch-size<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span>param params<span style="color: #909183;">]</span>
    <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">param = param - param.gradient * lr / batchSize</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/-! param <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-gradient param<span style="color: #6276ba;">)</span> lr<span style="color: #907373;">)</span> batch-size<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgb161f26" class="outline-4">
<h4 id="orgb161f26"><span class="section-number-4">3.2.7.</span> Training</h4>
<div class="outline-text-4" id="text-3-2-7">
<p>
Now that we have all of the parts in place, we are ready to implement
the main training loop. It is crucial that you understand this code
because you will see nearly identical training loops over and over
again throughout your career in deep learning.
</p>

<p>
In each iteration, we will grab a minibatch of training examples, and
pass them through our model to obtain a set of predictions. After
calculating the loss, we initiate the backwards pass through the
network, storing the gradients with respect to each
parameter. Finally, we will call the optimization algorithm <code>sgd</code> to
update the model parameters.
</p>

<p>
In summary, we will execute the following loop:
</p>
<ul class="org-ul">
<li>Initialize parameters \((\mathbf{w}, b)\)</li>
<li>Repeat until done
<ul class="org-ul">
<li>Compute gradient \(\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)}
    \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}
    l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)\)</li>
<li>Update parameters \((\mathbf{w}, b) \leftarrow (\mathbf{w}, b) -
    \eta \mathbf{g}\)</li>
</ul></li>
</ul>

<p>
In each <b>epoch</b>, we will iterate through the entire dataset (using the
<code>data-iter</code> function) once passing through every example in the training
dataset (assuming that the number of examples is divisible by the
batch size). The number of epochs <code>num-epochs</code> and the learning rate <code>lr</code>
are both hyperparameters, which we set here to 3 and 0.03,
respectively. Unfortunately, setting hyperparameters is tricky and
requires some adjustment by trial and error. We elide these details
for now but revise them later in Section 11.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">lr</span> 0.03<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">num-epochs</span> 3<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">batch-size</span> 10<span style="color: #707183;">)</span>
<span style="color: #707183;">[</span>lr num-epochs batch-size<span style="color: #707183;">]</span>
</pre>
</div>

<pre class="example">
[0.03 3 10]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">datasets</span> <span style="color: #7388d6;">(</span>data-iter batch-size features labels<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.linreg/datasets
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #7388d6;">[</span>epoch <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> num-epochs<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span><span style="color: #709870;">[</span>X y<span style="color: #709870;">]</span> datasets<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #709870;">[</span>gc <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/gradient-collector<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #907373;">[</span>l <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #858580;">(</span>linreg X w b<span style="color: #858580;">)</span> <span style="color: #858580;">(</span>squared-loss y<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span><span style="color: #907373;">]</span>
        <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc l<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>sgd <span style="color: #709870;">[</span>w b<span style="color: #709870;">]</span> lr batch-size<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"epoch "</span> epoch
           <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #709870;">(</span>squared-loss <span style="color: #907373;">(</span>linreg features w b<span style="color: #907373;">)</span> labels<span style="color: #709870;">)</span>
               <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/mean<span style="color: #709870;">)</span>
               <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
epoch  0 3.3477103E-4
epoch  1 5.24727E-5
epoch  2 5.1000967E-5
</pre>


<p>
In this case, because we synthesized the dataset ourselves, we know
precisely what the true parameters are. Thus, we can evaluate our
success in training by comparing the true parameters with those that
we learned through our training loop. Indeed they turn out to be very
close to each other.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec w<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec true-w<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">w-error</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- true-w <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape w <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-shape true-w<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Error in estimating w:"</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">vec</span> w-error<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Error in estimating w:"</span> <span style="color: #7388d6;">(</span>- true-b <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element b<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
[1.999567 -3.400089]
[2.0 -3.4]
Error in estimating w: [4.3296814E-4 8.893013E-5]
Error in estimating w: 5.704879760743964E-4
</pre>


<p>
Note that we should not take it for granted that we are able to
recover the parameters perfectly. However, in machine learning, we are
typically less concerned with recovering true underlying parameters,
and more concerned with parameters that lead to highly accurate
prediction. Fortunately, even on difficult optimization problems,
stochastic gradient descent can often find remarkably good solutions,
owing partly to the fact that, for deep networks, there exist many
configurations of the parameters that lead to highly accurate
prediction.
</p>
</div>
</div>


<div id="outline-container-orgb56c1a6" class="outline-4">
<h4 id="orgb56c1a6"><span class="section-number-4">3.2.8.</span> Summary</h4>
<div class="outline-text-4" id="text-3-2-8">
<ul class="org-ul">
<li>We saw how a deep network can be implemented and optimized from
scratch, using just tensors and auto differentiation, without any
need for defining layers or fancy optimizers.</li>
<li>This section only scratches the surface of what is possible. In the
following sections, we will describe additional models based on the
concepts that we have just introduced and learn how to implement
them more concisely.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3f21f99" class="outline-3">
<h3 id="org3f21f99"><span class="section-number-3">3.3.</span> Concise Implementation of Linear Regression</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Broad and intense interest in deep learning for the past several years
has inspired companies, academics, and hobbyists to develop a variety of
mature open source frameworks for automating the repetitive work of
implementing gradient-based learning algorithms. In
:numref:`sec<sub>linear</sub><sub>scratch</sub>`, we relied only on (i) tensors for data
storage and linear algebra; and (ii) auto differentiation for
calculating gradients. In practice, because data iterators, loss
functions, optimizers, and neural network layers are so common, modern
libraries implement these components for us as well.
</p>

<p>
In this section, we will show you how to implement the linear
regression model from Section
<a href="3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch">3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch</a>
concisely by using high-level APIs of deep learning frameworks.
</p>
</div>

<div id="outline-container-org91ddedd" class="outline-4">
<h4 id="org91ddedd"><span class="section-number-4">3.3.1.</span> Generating the Dataset</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
To start, we will generate the same dataset as in Section
<a href="3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch">3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch</a>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.linreg-easy</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.device <span style="color: #110099;">:as</span> device<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.engine <span style="color: #110099;">:as</span> engine<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.dataset <span style="color: #110099;">:as</span> ds<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.model <span style="color: #110099;">:as</span> model<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.nn <span style="color: #110099;">:as</span> nn<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.loss <span style="color: #110099;">:as</span> loss<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.tracker <span style="color: #110099;">:as</span> tracker<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.optimizer <span style="color: #110099;">:as</span> optimizer<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.parameter <span style="color: #110099;">:as</span> parameter<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.initializer <span style="color: #110099;">:as</span> initializer<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training <span style="color: #110099;">:as</span> t<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.listener <span style="color: #110099;">:as</span> listener<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:import</span> <span style="color: #909183;">[</span>ai.djl.ndarray.types DataType<span style="color: #909183;">]</span>
           <span style="color: #909183;">[</span>java.nio.file Paths<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">true-w</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">float-array</span> <span style="color: #709870;">[</span>2 -3.4<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">true-b</span> 4.2<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">dp</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">d2l</span>/synthetic-data ndm true-w true-b 1000<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">features</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> dp 0<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">labels</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> dp 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span>println <span style="color: #2A00FF;">"features(0): "</span><span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> features <span style="color: #709870;">[</span>0<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span>println <span style="color: #2A00FF;">"labels(0): "</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> labels <span style="color: #709870;">[</span>0<span style="color: #709870;">]</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
features(0):  [0.36353156 1.8406333]
labels(0):  [-1.3278697]
</pre>
</div>
</div>

<div id="outline-container-org433afa7" class="outline-4">
<h4 id="org433afa7"><span class="section-number-4">3.3.2.</span> Reading the Dataset</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
Rather than rolling our own iterator, we can call upon the existing
API in a framework to read data. We pass in <code>features</code> and <code>labels</code> as
arguments and specify <code>batch-size</code> when instantiating a data iterator
object. Besides, the boolean value <code>is-train</code> indicates whether or not
we want the data iterator object to shuffle the data on each epoch
(pass through the dataset).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">batch-size</span> 10<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">datasets</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/new-array-dataset-builder<span style="color: #909183;">)</span>
                  <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/set-data features<span style="color: #909183;">)</span>
                  <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/opt-labels labels<span style="color: #909183;">)</span>
                  <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">(defn set-sampling [batch-size shuffle] ...)</span>
                  <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/set-sampling batch-size <span style="color: #110099;">false</span><span style="color: #909183;">)</span>
                  <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/build<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Now we can use <code>get-data-iterator</code> in much the same way as we called the
<code>data-iter</code> function in Section
<a href="3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch">3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch</a>. To
verify that it is working, we can read and print the first minibatch
of examples. Comparing with Section
<a href="3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch">3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch</a>, here
we use Clojure function <code>first</code> to obtain the first item from the
iterator.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> datasets
    <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data-iterator ndm<span style="color: #7388d6;">)</span> <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">generate a data iterator</span>
    first <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">get the first batch</span>
    <span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">get the data</span>
    first<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example" id="org9a7ae33">
ND: (10, 2) cpu() float32
[[-2.2896,  1.0315],
 [-0.6617,  0.5531],
 [ 0.3967, -0.9902],
 [ 0.9992, -1.9574],
 [-0.9857, -0.1098],
 [-0.5344,  0.0834],
 [ 1.0844,  0.2221],
 [ 1.3125, -0.8627],
 [-0.581 ,  0.7608],
 [-1.4804,  0.2687],
]
</pre>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> datasets
    <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data-iterator ndm<span style="color: #7388d6;">)</span> <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">generate a data iterator</span>
    first <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">get the first batch</span>
    <span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-labels <span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">get the labels</span>
    first<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (10) cpu() float32
[ 5.7955,  3.7533, 11.8775,  6.7918,  3.2628,  2.2621,  1.5316, 10.5247, 15.5371, -0.0894]
</pre>
</div>
</div>
</div>



<div id="outline-container-org2976a67" class="outline-3">
<h3 id="org2976a67"><span class="section-number-3">3.4.</span> Defining the Model</h3>
<div class="outline-text-3" id="text-3-4">
<p>
When we implemented linear regression from scratch in Section
<a href="3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch">3.2-linear-reg-impl-from-scratch.html#lin-reg-scratch</a>, we
defined our model parameters explicitly and coded up the calculations
to produce output using basic linear algebra operations. You <b>should</b>
know how to do this. But once your models get more complex, and once
you have to do this nearly every day, you will be glad for the
assistance. The situation is similar to coding up your own blog from
scratch. Doing it once or twice is rewarding and instructive, but you
would be a lousy web developer if every time you needed a blog you
spent a month reinventing the wheel.
</p>

<p>
For standard operations, we can use a framework&rsquo;s predefined layers,
which allow us to focus especially on the layers used to construct the
model rather than having to focus on the implementation. We will first
define a model variable <code>net</code>, which will refer to an instance of the
<code>sequential-block</code> class. The <code>sequential-block</code> class defines a container
for several layers that will be chained together. Given input data, a
<code>sequential-block</code> instance passes it through the first layer, in turn
passing the output as the second layer&rsquo;s input and so forth. In the
following example, our model consists of only one layer, so we do not
really need <code>sequential-block</code>. But since nearly all of our future
models will involve multiple layers, we will use it anyway just to
familiarize you with the most standard workflow.
</p>

<p>
Recall the architecture of a single-layer network as shown in Fig
<a href="3.1-linear-regression.html#org30de283">3.1-linear-regression.html#org30de283</a>. The layer is
said to be <b>fully-connected</b> because each of its inputs is connected to
each of its outputs by means of a matrix-vector multiplication.
</p>

<p>
Now we define a model with name &ldquo;lin-reg&rdquo; and create a
<code>sequential-block</code> with a <code>linear-block</code> inside it. And finally, set the
<code>sequential-block</code> to the model.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">model</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">model</span>/new-instance <span style="color: #2A00FF;">"lin-reg"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">net</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nn</span>/sequential-block<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">linear-block</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nn</span>/linear-block <span style="color: #909183;">{</span><span style="color: #110099;">:bias</span> <span style="color: #110099;">true</span>
                                    <span style="color: #110099;">:units</span> 1<span style="color: #909183;">}</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nn</span>/add net linear-block<span style="color: #707183;">)</span>
</pre>
</div>
</div>

<div id="outline-container-orgf26d353" class="outline-4">
<h4 id="orgf26d353"><span class="section-number-4">3.4.1.</span> Initializing Model Parameters</h4>
<div class="outline-text-4" id="text-3-4-1">
<p>
Before using <code>net</code>, we need to initialize the model parameters, such as
the weights and bias in the linear regression model. Deep learning
frameworks often have a predefined way to initialize the parameters.
Here we specify that each weight parameter should be randomly sampled
from a normal distribution with mean 0 and standard deviation
0.01. The bias parameter will be initialized to zero.
</p>

<p>
We import the <code>initializer</code> namespace from <code>clj-djl</code>. This module provides
various methods for model parameter initialization. We only specify
how to initialize the weight by calling <code>(normal-initializer
0.01)</code>. Bias parameters are initialized to zero by default.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nn</span>/set-initializer net <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">initializer</span>/normal-initializer 0.01<span style="color: #7388d6;">)</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">parameter</span>/weight<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">model</span>/set-block model net<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Model (
	Name: lin-reg
	Data Type: float32
)
</pre>


<p>
The code above may look straightforward but you should note that
something strange is happening here. We are initializing parameters
for a network even though clj-djl does not yet know how many
dimensions the input will have! It might be 2 as in our example or it
might be 2000. clj-djl lets us get away with this because behind the
scene, the initialization is actually deferred. The real
initialization will take place only when we for the first time attempt
to pass data through the network. Just be careful to remember that
since the parameters have not been initialized yet, we cannot access
or manipulate them.
</p>
</div>
</div>

<div id="outline-container-orgf7bdd94" class="outline-4">
<h4 id="orgf7bdd94"><span class="section-number-4">3.4.2.</span> Defining the Loss Function</h4>
<div class="outline-text-4" id="text-3-4-2">
<p>
In clj-djl, the loss namespace defines various loss functions. In this
example, we will use the squared loss (l2-Loss).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">loss</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">loss</span>/l2-loss<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.linreg-easy/loss
</pre>
</div>
</div>


<div id="outline-container-org41fd1a5" class="outline-4">
<h4 id="org41fd1a5"><span class="section-number-4">3.4.3.</span> Defining the Optimization Algorithm</h4>
<div class="outline-text-4" id="text-3-4-3">
<p>
Minibatch stochastic gradient descent is a standard tool for
optimizing neural networks and thus clj-djl supports it alongside a
number of variations on this algorithm through its <code>trainer</code>. When we
instantiate <code>trainer</code>, we will specify the parameters to optimize over,
the optimization algorithm we wish to use (sgd), and a dictionary of
hyperparameters required by our optimization algorithm. Minibatch
stochastic gradient descent just requires that we set the value
learning rate, which is set to 0.03 here.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">lrt</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">tracker</span>/fixed 0.3<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">sgd</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">optimizer</span>/sgd <span style="color: #909183;">{</span><span style="color: #110099;">:tracker</span> lrt<span style="color: #909183;">}</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.linreg-easy/sgd
</pre>
</div>
</div>


<div id="outline-container-org44ce3c7" class="outline-4">
<h4 id="org44ce3c7"><span class="section-number-4">3.4.4.</span> Instantiate Configuration and Trainer</h4>
<div class="outline-text-4" id="text-3-4-4">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">trainer</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/trainer <span style="color: #909183;">{</span><span style="color: #110099;">:model</span> model
                         <span style="color: #110099;">:loss</span> loss
                         <span style="color: #110099;">:optimizer</span> sgd
                         <span style="color: #110099;">:listeners</span> <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">listener</span>/logging<span style="color: #709870;">)</span><span style="color: #909183;">}</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.linreg-easy/trainer
</pre>
</div>
</div>


<div id="outline-container-orgf672d83" class="outline-4">
<h4 id="orgf672d83"><span class="section-number-4">3.4.5.</span> Initializing Model Parameters</h4>
<div class="outline-text-4" id="text-3-4-5">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/initialize trainer <span style="color: #7388d6;">[</span><span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape batch-size 2<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ai.djl.training.Trainer@4b641b7c
</pre>
</div>
</div>

<div id="outline-container-orgb6f16e7" class="outline-4">
<h4 id="orgb6f16e7"><span class="section-number-4">3.4.6.</span> Metrics</h4>
<div class="outline-text-4" id="text-3-4-6">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">metrics</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/metrics<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/set-metrics trainer metrics<span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd102c1f" class="outline-4">
<h4 id="orgd102c1f"><span class="section-number-4">3.4.7.</span> Training</h4>
<div class="outline-text-4" id="text-3-4-7">
<p>
You might have noticed that expressing our model through high-level
APIs of a deep learning framework requires comparatively few lines of
code. We did not have to individually allocate parameters, define our
loss function, or implement minibatch stochastic gradient
descent. Once we start working with much more complex models,
advantages of high-level APIs will grow considerably. However, once we
have all the basic pieces in place, the training loop itself is
strikingly similar to what we did when implementing everything from
scratch.
</p>

<p>
To refresh your memory: for some number of epochs, we will make a
complete pass over the dataset (train-data), iteratively grabbing one
minibatch of inputs and the corresponding ground-truth labels. For
each minibatch, we go through the following ritual:
</p>

<ul class="org-ul">
<li>Generate predictions by calling <code>train-batch</code> and calculate the loss l
(the forward propagation).</li>
<li>Calculate gradients by running the backpropagation.</li>
<li>Update the model parameters by invoking our optimizer.</li>
</ul>

<p>
For good measure, we compute the loss after each epoch and print it to
monitor progress.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">epochs</span> 3<span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #7388d6;">[</span>epoch <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> epochs<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span>batch <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/iterate-dataset trainer datasets<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/train-batch trainer batch<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/step trainer<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/close-batch batch<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/notify-listeners trainer <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">fn</span> <span style="color: #709870;">[</span>listner<span style="color: #709870;">]</span> <span style="color: #709870;">(</span>.onEpoch listner trainer<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example" id="org2ecd939">

Training:      1% |=                                       | L2Loss: _
Training:      2% |=                                       | L2Loss: _
Training:      3% |==                                      | L2Loss: _
Training:      4% |==                                      | L2Loss: _
Training:      5% |===                                     | L2Loss: 6.82
Training:      6% |===                                     | L2Loss: 6.82
Training:      7% |===                                     | L2Loss: 6.82
Training:      8% |====                                    | L2Loss: 6.82
Training:      9% |====                                    | L2Loss: 6.82
Training:     10% |=====                                   | L2Loss: 3.54
Training:     11% |=====                                   | L2Loss: 3.54
Training:     12% |=====                                   | L2Loss: 3.54
Training:     13% |======                                  | L2Loss: 3.54
Training:     14% |======                                  | L2Loss: 3.54
Training:     15% |=======                                 | L2Loss: 2.36
Training:     16% |=======                                 | L2Loss: 2.36
Training:     17% |=======                                 | L2Loss: 2.36
Training:     18% |========                                | L2Loss: 2.36
Training:     19% |========                                | L2Loss: 2.36
Training:     20% |=========                               | L2Loss: 1.77
Training:     21% |=========                               | L2Loss: 1.77
Training:     22% |=========                               | L2Loss: 1.77
Training:     23% |==========                              | L2Loss: 1.77
Training:     24% |==========                              | L2Loss: 1.77
Training:     25% |===========                             | L2Loss: 1.42
Training:     26% |===========                             | L2Loss: 1.42
Training:     27% |===========                             | L2Loss: 1.42
Training:     28% |============                            | L2Loss: 1.42
Training:     29% |============                            | L2Loss: 1.42
Training:     30% |=============                           | L2Loss: 1.18
Training:     31% |=============                           | L2Loss: 1.18
Training:     32% |=============                           | L2Loss: 1.18
Training:     33% |==============                          | L2Loss: 1.18
Training:     34% |==============                          | L2Loss: 1.18
Training:     35% |===============                         | L2Loss: 1.01
Training:     36% |===============                         | L2Loss: 1.01
Training:     37% |===============                         | L2Loss: 1.01
Training:     38% |================                        | L2Loss: 1.01
Training:     39% |================                        | L2Loss: 1.01
Training:     40% |=================                       | L2Loss: 0.89
Training:     41% |=================                       | L2Loss: 0.89
Training:     42% |=================                       | L2Loss: 0.89
Training:     43% |==================                      | L2Loss: 0.89
Training:     44% |==================                      | L2Loss: 0.89
Training:     45% |===================                     | L2Loss: 0.79
Training:     46% |===================                     | L2Loss: 0.79
Training:     47% |===================                     | L2Loss: 0.79
Training:     48% |====================                    | L2Loss: 0.79
Training:     49% |====================                    | L2Loss: 0.79
Training:     50% |=====================                   | L2Loss: 0.71
Training:     51% |=====================                   | L2Loss: 0.71
Training:     52% |=====================                   | L2Loss: 0.71
Training:     53% |======================                  | L2Loss: 0.71
Training:     54% |======================                  | L2Loss: 0.71
Training:     55% |=======================                 | L2Loss: 0.64
Training:     56% |=======================                 | L2Loss: 0.64
Training:     57% |=======================                 | L2Loss: 0.64
Training:     58% |========================                | L2Loss: 0.64
Training:     59% |========================                | L2Loss: 0.64
Training:     60% |=========================               | L2Loss: 0.59
Training:     61% |=========================               | L2Loss: 0.59
Training:     62% |=========================               | L2Loss: 0.59
Training:     63% |==========================              | L2Loss: 0.59
Training:     64% |==========================              | L2Loss: 0.59
Training:     65% |===========================             | L2Loss: 0.55
Training:     66% |===========================             | L2Loss: 0.55
Training:     67% |===========================             | L2Loss: 0.55
Training:     68% |============================            | L2Loss: 0.55
Training:     69% |============================            | L2Loss: 0.55
Training:     70% |=============================           | L2Loss: 0.51
Training:     71% |=============================           | L2Loss: 0.51
Training:     72% |=============================           | L2Loss: 0.51
Training:     73% |==============================          | L2Loss: 0.51
Training:     74% |==============================          | L2Loss: 0.51
Training:     75% |===============================         | L2Loss: 0.47
Training:     76% |===============================         | L2Loss: 0.47
Training:     77% |===============================         | L2Loss: 0.47
Training:     78% |================================        | L2Loss: 0.47
Training:     79% |================================        | L2Loss: 0.47
Training:     80% |=================================       | L2Loss: 0.44
Training:     81% |=================================       | L2Loss: 0.44
Training:     82% |=================================       | L2Loss: 0.44
Training:     83% |==================================      | L2Loss: 0.44
Training:     84% |==================================      | L2Loss: 0.44
Training:     85% |===================================     | L2Loss: 0.42
Training:     86% |===================================     | L2Loss: 0.42
Training:     87% |===================================     | L2Loss: 0.42
Training:     88% |====================================    | L2Loss: 0.42
Training:     89% |====================================    | L2Loss: 0.42
Training:     90% |=====================================   | L2Loss: 0.39
Training:     91% |=====================================   | L2Loss: 0.39
Training:     92% |=====================================   | L2Loss: 0.39
Training:     93% |======================================  | L2Loss: 0.39
Training:     94% |======================================  | L2Loss: 0.39
Training:     95% |======================================= | L2Loss: 0.37
Training:     96% |======================================= | L2Loss: 0.37
Training:     97% |======================================= | L2Loss: 0.37
Training:     98% |========================================| L2Loss: 0.37
Training:     99% |========================================| L2Loss: 0.37
Training:    100% |========================================| L2Loss: 0.35

Training:      1% |=                                       | L2Loss: 0.35
Training:      2% |=                                       | L2Loss: 0.35
Training:      3% |==                                      | L2Loss: 0.35
Training:      4% |==                                      | L2Loss: 0.35
Training:      5% |===                                     | L2Loss: 7.43E-05
Training:      6% |===                                     | L2Loss: 7.43E-05
Training:      7% |===                                     | L2Loss: 7.43E-05
Training:      8% |====                                    | L2Loss: 7.43E-05
Training:      9% |====                                    | L2Loss: 7.43E-05
Training:     10% |=====                                   | L2Loss: 6.59E-05
Training:     11% |=====                                   | L2Loss: 6.59E-05
Training:     12% |=====                                   | L2Loss: 6.59E-05
Training:     13% |======                                  | L2Loss: 6.59E-05
Training:     14% |======                                  | L2Loss: 6.59E-05
Training:     15% |=======                                 | L2Loss: 5.80E-05
Training:     16% |=======                                 | L2Loss: 5.80E-05
Training:     17% |=======                                 | L2Loss: 5.80E-05
Training:     18% |========                                | L2Loss: 5.80E-05
Training:     19% |========                                | L2Loss: 5.80E-05
Training:     20% |=========                               | L2Loss: 5.92E-05
Training:     21% |=========                               | L2Loss: 5.92E-05
Training:     22% |=========                               | L2Loss: 5.92E-05
Training:     23% |==========                              | L2Loss: 5.92E-05
Training:     24% |==========                              | L2Loss: 5.92E-05
Training:     25% |===========                             | L2Loss: 5.60E-05
Training:     26% |===========                             | L2Loss: 5.60E-05
Training:     27% |===========                             | L2Loss: 5.60E-05
Training:     28% |============                            | L2Loss: 5.60E-05
Training:     29% |============                            | L2Loss: 5.60E-05
Training:     30% |=============                           | L2Loss: 5.70E-05
Training:     31% |=============                           | L2Loss: 5.70E-05
Training:     32% |=============                           | L2Loss: 5.70E-05
Training:     33% |==============                          | L2Loss: 5.70E-05
Training:     34% |==============                          | L2Loss: 5.70E-05
Training:     35% |===============                         | L2Loss: 5.73E-05
Training:     36% |===============                         | L2Loss: 5.73E-05
Training:     37% |===============                         | L2Loss: 5.73E-05
Training:     38% |================                        | L2Loss: 5.73E-05
Training:     39% |================                        | L2Loss: 5.73E-05
Training:     40% |=================                       | L2Loss: 5.78E-05
Training:     41% |=================                       | L2Loss: 5.78E-05
Training:     42% |=================                       | L2Loss: 5.78E-05
Training:     43% |==================                      | L2Loss: 5.78E-05
Training:     44% |==================                      | L2Loss: 5.78E-05
Training:     45% |===================                     | L2Loss: 5.58E-05
Training:     46% |===================                     | L2Loss: 5.58E-05
Training:     47% |===================                     | L2Loss: 5.58E-05
Training:     48% |====================                    | L2Loss: 5.58E-05
Training:     49% |====================                    | L2Loss: 5.58E-05
Training:     50% |=====================                   | L2Loss: 5.64E-05
Training:     51% |=====================                   | L2Loss: 5.64E-05
Training:     52% |=====================                   | L2Loss: 5.64E-05
Training:     53% |======================                  | L2Loss: 5.64E-05
Training:     54% |======================                  | L2Loss: 5.64E-05
Training:     55% |=======================                 | L2Loss: 5.74E-05
Training:     56% |=======================                 | L2Loss: 5.74E-05
Training:     57% |=======================                 | L2Loss: 5.74E-05
Training:     58% |========================                | L2Loss: 5.74E-05
Training:     59% |========================                | L2Loss: 5.74E-05
Training:     60% |=========================               | L2Loss: 5.72E-05
Training:     61% |=========================               | L2Loss: 5.72E-05
Training:     62% |=========================               | L2Loss: 5.72E-05
Training:     63% |==========================              | L2Loss: 5.72E-05
Training:     64% |==========================              | L2Loss: 5.72E-05
Training:     65% |===========================             | L2Loss: 5.75E-05
Training:     66% |===========================             | L2Loss: 5.75E-05
Training:     67% |===========================             | L2Loss: 5.75E-05
Training:     68% |============================            | L2Loss: 5.75E-05
Training:     69% |============================            | L2Loss: 5.75E-05
Training:     70% |=============================           | L2Loss: 5.77E-05
Training:     71% |=============================           | L2Loss: 5.77E-05
Training:     72% |=============================           | L2Loss: 5.77E-05
Training:     73% |==============================          | L2Loss: 5.77E-05
Training:     74% |==============================          | L2Loss: 5.77E-05
Training:     75% |===============================         | L2Loss: 5.85E-05
Training:     76% |===============================         | L2Loss: 5.85E-05
Training:     77% |===============================         | L2Loss: 5.85E-05
Training:     78% |================================        | L2Loss: 5.85E-05
Training:     79% |================================        | L2Loss: 5.85E-05
Training:     80% |=================================       | L2Loss: 5.78E-05
Training:     81% |=================================       | L2Loss: 5.78E-05
Training:     82% |=================================       | L2Loss: 5.78E-05
Training:     83% |==================================      | L2Loss: 5.78E-05
Training:     84% |==================================      | L2Loss: 5.78E-05
Training:     85% |===================================     | L2Loss: 5.64E-05
Training:     86% |===================================     | L2Loss: 5.64E-05
Training:     87% |===================================     | L2Loss: 5.64E-05
Training:     88% |====================================    | L2Loss: 5.64E-05
Training:     89% |====================================    | L2Loss: 5.64E-05
Training:     90% |=====================================   | L2Loss: 5.66E-05
Training:     91% |=====================================   | L2Loss: 5.66E-05
Training:     92% |=====================================   | L2Loss: 5.66E-05
Training:     93% |======================================  | L2Loss: 5.66E-05
Training:     94% |======================================  | L2Loss: 5.66E-05
Training:     95% |======================================= | L2Loss: 5.76E-05
Training:     96% |======================================= | L2Loss: 5.76E-05
Training:     97% |======================================= | L2Loss: 5.76E-05
Training:     98% |========================================| L2Loss: 5.76E-05
Training:     99% |========================================| L2Loss: 5.76E-05
Training:    100% |========================================| L2Loss: 5.63E-05

Training:      1% |=                                       | L2Loss: 5.63E-05
Training:      2% |=                                       | L2Loss: 5.63E-05
Training:      3% |==                                      | L2Loss: 5.63E-05
Training:      4% |==                                      | L2Loss: 5.63E-05
Training:      5% |===                                     | L2Loss: 7.43E-05
Training:      6% |===                                     | L2Loss: 7.43E-05
Training:      7% |===                                     | L2Loss: 7.43E-05
Training:      8% |====                                    | L2Loss: 7.43E-05
Training:      9% |====                                    | L2Loss: 7.43E-05
Training:     10% |=====                                   | L2Loss: 6.59E-05
Training:     11% |=====                                   | L2Loss: 6.59E-05
Training:     12% |=====                                   | L2Loss: 6.59E-05
Training:     13% |======                                  | L2Loss: 6.59E-05
Training:     14% |======                                  | L2Loss: 6.59E-05
Training:     15% |=======                                 | L2Loss: 5.80E-05
Training:     16% |=======                                 | L2Loss: 5.80E-05
Training:     17% |=======                                 | L2Loss: 5.80E-05
Training:     18% |========                                | L2Loss: 5.80E-05
Training:     19% |========                                | L2Loss: 5.80E-05
Training:     20% |=========                               | L2Loss: 5.92E-05
Training:     21% |=========                               | L2Loss: 5.92E-05
Training:     22% |=========                               | L2Loss: 5.92E-05
Training:     23% |==========                              | L2Loss: 5.92E-05
Training:     24% |==========                              | L2Loss: 5.92E-05
Training:     25% |===========                             | L2Loss: 5.60E-05
Training:     26% |===========                             | L2Loss: 5.60E-05
Training:     27% |===========                             | L2Loss: 5.60E-05
Training:     28% |============                            | L2Loss: 5.60E-05
Training:     29% |============                            | L2Loss: 5.60E-05
Training:     30% |=============                           | L2Loss: 5.70E-05
Training:     31% |=============                           | L2Loss: 5.70E-05
Training:     32% |=============                           | L2Loss: 5.70E-05
Training:     33% |==============                          | L2Loss: 5.70E-05
Training:     34% |==============                          | L2Loss: 5.70E-05
Training:     35% |===============                         | L2Loss: 5.73E-05
Training:     36% |===============                         | L2Loss: 5.73E-05
Training:     37% |===============                         | L2Loss: 5.73E-05
Training:     38% |================                        | L2Loss: 5.73E-05
Training:     39% |================                        | L2Loss: 5.73E-05
Training:     40% |=================                       | L2Loss: 5.78E-05
Training:     41% |=================                       | L2Loss: 5.78E-05
Training:     42% |=================                       | L2Loss: 5.78E-05
Training:     43% |==================                      | L2Loss: 5.78E-05
Training:     44% |==================                      | L2Loss: 5.78E-05
Training:     45% |===================                     | L2Loss: 5.58E-05
Training:     46% |===================                     | L2Loss: 5.58E-05
Training:     47% |===================                     | L2Loss: 5.58E-05
Training:     48% |====================                    | L2Loss: 5.58E-05
Training:     49% |====================                    | L2Loss: 5.58E-05
Training:     50% |=====================                   | L2Loss: 5.64E-05
Training:     51% |=====================                   | L2Loss: 5.64E-05
Training:     52% |=====================                   | L2Loss: 5.64E-05
Training:     53% |======================                  | L2Loss: 5.64E-05
Training:     54% |======================                  | L2Loss: 5.64E-05
Training:     55% |=======================                 | L2Loss: 5.74E-05
Training:     56% |=======================                 | L2Loss: 5.74E-05
Training:     57% |=======================                 | L2Loss: 5.74E-05
Training:     58% |========================                | L2Loss: 5.74E-05
Training:     59% |========================                | L2Loss: 5.74E-05
Training:     60% |=========================               | L2Loss: 5.72E-05
Training:     61% |=========================               | L2Loss: 5.72E-05
Training:     62% |=========================               | L2Loss: 5.72E-05
Training:     63% |==========================              | L2Loss: 5.72E-05
Training:     64% |==========================              | L2Loss: 5.72E-05
Training:     65% |===========================             | L2Loss: 5.75E-05
Training:     66% |===========================             | L2Loss: 5.75E-05
Training:     67% |===========================             | L2Loss: 5.75E-05
Training:     68% |============================            | L2Loss: 5.75E-05
Training:     69% |============================            | L2Loss: 5.75E-05
Training:     70% |=============================           | L2Loss: 5.77E-05
Training:     71% |=============================           | L2Loss: 5.77E-05
Training:     72% |=============================           | L2Loss: 5.77E-05
Training:     73% |==============================          | L2Loss: 5.77E-05
Training:     74% |==============================          | L2Loss: 5.77E-05
Training:     75% |===============================         | L2Loss: 5.85E-05
Training:     76% |===============================         | L2Loss: 5.85E-05
Training:     77% |===============================         | L2Loss: 5.85E-05
Training:     78% |================================        | L2Loss: 5.85E-05
Training:     79% |================================        | L2Loss: 5.85E-05
Training:     80% |=================================       | L2Loss: 5.78E-05
Training:     81% |=================================       | L2Loss: 5.78E-05
Training:     82% |=================================       | L2Loss: 5.78E-05
Training:     83% |==================================      | L2Loss: 5.78E-05
Training:     84% |==================================      | L2Loss: 5.78E-05
Training:     85% |===================================     | L2Loss: 5.64E-05
Training:     86% |===================================     | L2Loss: 5.64E-05
Training:     87% |===================================     | L2Loss: 5.64E-05
Training:     88% |====================================    | L2Loss: 5.64E-05
Training:     89% |====================================    | L2Loss: 5.64E-05
Training:     90% |=====================================   | L2Loss: 5.66E-05
Training:     91% |=====================================   | L2Loss: 5.66E-05
Training:     92% |=====================================   | L2Loss: 5.66E-05
Training:     93% |======================================  | L2Loss: 5.66E-05
Training:     94% |======================================  | L2Loss: 5.66E-05
Training:     95% |======================================= | L2Loss: 5.76E-05
Training:     96% |======================================= | L2Loss: 5.76E-05
Training:     97% |======================================= | L2Loss: 5.76E-05
Training:     98% |========================================| L2Loss: 5.76E-05
Training:     99% |========================================| L2Loss: 5.76E-05
Training:    100% |========================================| L2Loss: 5.63E-05
</pre>

<p>
Below, we compare the model parameters learned by training on finite
data and the actual parameters that generated our dataset. To access
parameters, we first access the layer that we need from net and then
access that layers weights and bias. As in our from-scratch
implementation, note that our estimated parameters are close to their
ground-truth counterparts.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">params</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> model <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">model</span>/get-block<span style="color: #909183;">)</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">model</span>/get-parameters<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">w</span> <span style="color: #7388d6;">(</span>.getArray <span style="color: #909183;">(</span>.valueAt params 0<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">b</span> <span style="color: #7388d6;">(</span>.getArray <span style="color: #909183;">(</span>.valueAt params 1<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">w-error</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/- true-w <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape w <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-shape true-w<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Error in estimating w:"</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">vec</span> w-error<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Error in estimating w:"</span> <span style="color: #7388d6;">(</span>- true-b <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element b<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Error in estimating w: [-0.0019903183 7.4744225E-4]
Error in estimating w: -4.289627075193536E-4
</pre>
</div>
</div>

<div id="outline-container-org0759565" class="outline-4">
<h4 id="org0759565"><span class="section-number-4">3.4.8.</span> Saving Your Model</h4>
<div class="outline-text-4" id="text-3-4-8">
<p>
You can also save the model for future prediction task.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">save-model</span> <span style="color: #7388d6;">[</span>model path epoch name<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>nio-path <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">java.nio.file.Paths</span>/<span style="color: #7F0055; font-weight: bold;">get</span> path <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">into-array</span> <span style="color: #6276ba;">[</span><span style="color: #2A00FF;">""</span><span style="color: #6276ba;">]</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">io</span>/make-parents path<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">model</span>/set-property model <span style="color: #2A00FF;">"Epoch"</span> epoch<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">model</span>/save model nio-path name<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span>save-model model <span style="color: #2A00FF;">"models/lin-reg"</span> <span style="color: #2A00FF;">"3"</span> <span style="color: #2A00FF;">"lin-reg"</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">str</span> model<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Model (
	Name: lin-reg
	Model location: /home/kimim/workspace/clj-d2l/models/lin-reg
	Data Type: float32
	Epoch: 3
)
</pre>
</div>
</div>

<div id="outline-container-org5c69300" class="outline-4">
<h4 id="org5c69300"><span class="section-number-4">3.4.9.</span> Summary</h4>
<div class="outline-text-4" id="text-3-4-9">
<ul class="org-ul">
<li>Using clj-djl, we can implement models much more concisely.</li>
<li>In clj-djl, the <code>dataset</code> namespace provides tools for data
processing, the <code>nn</code> namespace defines a large number of neural
network layers, and the <code>loss</code> namespace defines many common loss
functions.</li>
<li><code>initializer</code> namespace provides various methods for model parameter
initialization.</li>
<li>Dimensionality and storage are automatically inferred, but be
careful not to attempt to access parameters before they have been
initialized.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-softmax" class="outline-3">
<h3 id="sec-softmax"><span class="section-number-3">3.5.</span> Softmax Regression</h3>
<div class="outline-text-3" id="text-sec-softmax">
<p>
In Section 3.1, we introduced linear regression, working through
implementations from scratch in Section 3.2 and again using high-level
APIs of a deep learning framework in Section 3.3 to do the heavy
lifting.
</p>

<p>
Regression is the hammer we reach for when we want to answer <b>how much</b>?
or <b>how many</b>? questions. If you want to predict the number of dollars
(price) at which a house will be sold, or the number of wins a
baseball team might have, or the number of days that a patient will
remain hospitalized before being discharged, then you are probably
looking for a regression model.
</p>

<p>
In practice, we are more often interested in <b>classification</b>: asking
not &ldquo;how much&rdquo; but &ldquo;which one&rdquo;:
</p>

<ul class="org-ul">
<li>Does this email belong in the spam folder or the inbox?</li>
<li>Is this customer more likely to sign up or not to sign up for a subscription service?</li>
<li>Does this image depict a donkey, a dog, a cat, or a rooster?</li>
<li>Which movie is Aston most likely to watch next?</li>
</ul>

<p>
Colloquially, machine learning practitioners overload the word
classification to describe two subtly different problems: (i) those
where we are interested only in hard assignments of examples to
categories (classes); and (ii) those where we wish to make soft
assignments, i.e., to assess the probability that each category
applies. The distinction tends to get blurred, in part, because often,
even when we only care about hard assignments, we still use models
that make soft assignments.
</p>
</div>
</div>
<div id="outline-container-org9e9632f" class="outline-3">
<h3 id="org9e9632f"><span class="section-number-3">3.6.</span> The Image Classification Dataset</h3>
<div class="outline-text-3" id="text-3-6">
<p>
One of the widely used dataset for image classification is the MNIST
dataset [LeCun et al., 1998]. While it had a good run as a benchmark
dataset, even simple models by todays standards achieve
classification accuracy over 95%, making it unsuitable for
distinguishing between stronger models and weaker ones. Today, MNIST
serves as more of sanity checks than as a benchmark. To up the ante
just a bit, we will focus our discussion in the coming sections on the
qualitatively similar, but comparatively complex Fashion-MNIST dataset
[Xiao et al., 2017], which was released in 2017.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.image-classification</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clj-djl.training.dataset <span style="color: #110099;">:as</span> ds<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>stopwatch.core <span style="color: #110099;">:as</span> stopwatch<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:import</span> <span style="color: #909183;">[</span>ai.djl.basicdataset.cv.classification FashionMnist<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>

<div id="outline-container-orgfa03a78" class="outline-4">
<h4 id="orgfa03a78"><span class="section-number-4">3.6.1.</span> Getting the Dataset</h4>
<div class="outline-text-4" id="text-3-6-1">
<p>
We can download and read the Fashion-MNIST dataset into memory via the
build-in functions in the framework.
</p>

<div class="org-src-container">
<pre class="src src-emacs-lisp"><span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">downloading time may be quite long, extend the repl timeout time</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">setq</span> org-babel-clojure-sync-nrepl-timeout 1000<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
1000
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">batch-size</span> 256<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">random-shuffle</span> <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
<span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">defn d2l/load-data-fashion-mnist in clj-d2l.core</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">mnist-train</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">FashionMnist</span>/builder<span style="color: #909183;">)</span>
                     <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/opt-usage <span style="color: #110099;">:train</span><span style="color: #909183;">)</span>
                     <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/set-sampling batch-size random-shuffle<span style="color: #909183;">)</span>
                     <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/build<span style="color: #909183;">)</span>
                     <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/prepare<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">mnist-test</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">FashionMnist</span>/builder<span style="color: #909183;">)</span>
                    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/opt-usage <span style="color: #110099;">:test</span><span style="color: #909183;">)</span>
                    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/set-sampling batch-size random-shuffle<span style="color: #909183;">)</span>
                    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/build<span style="color: #909183;">)</span>
                    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/prepare<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Fashion-MNIST consists of images from 10 categories, each represented
by 6000 images in the training dataset and by 1000 in the test
dataset. A <b>test dataset</b> (or test set) is used for evaluating model
performance and not for training. Consequently the training set and
the test set contain 60000 and 10000 images, respectively.
</p>

<pre class="example">
#'clj-d2l.image-classification/mnist-test
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">[</span><span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size mnist-test<span style="color: #7388d6;">)</span>
 <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size mnist-train<span style="color: #7388d6;">)</span><span style="color: #707183;">]</span>
</pre>
</div>

<pre class="example">
[10000 60000]
</pre>


<p>
The height and width of each input image are both 28 pixels. Note that
the dataset consists of grayscale images, whose number of channels
is 1. For brevity, throughout this book we store the shape of any
image with height \(h\) width \(w\) pixels as \(h \times w\) or \((h,
w)\).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">train-ds</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data-iterator mnist-train ndm<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.image-classification/train-ds
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> train-ds<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span> <span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(1, 28, 28)
</pre>


<p>
The images in Fashion-MNIST are associated with the following
categories: t-shirt, trousers, pullover, dress, coat, sandal, shirt,
sneaker, bag, and ankle boot. The following function converts between
numeric label indices and their names in text.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">get-fashion-mnist-labels</span> <span style="color: #7388d6;">[</span>labels<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>num-label-map  <span style="color: #709870;">{</span>0 <span style="color: #2A00FF;">"t-shirt"</span> 1 <span style="color: #2A00FF;">"trouser"</span> 2 <span style="color: #2A00FF;">"pullover"</span>
                        3 <span style="color: #2A00FF;">"dress"</span> 4 <span style="color: #2A00FF;">"coat"</span> 5 <span style="color: #2A00FF;">"sandal"</span>
                        6 <span style="color: #2A00FF;">"shirt"</span> 7 <span style="color: #2A00FF;">"sneaker"</span> 8 <span style="color: #2A00FF;">"bag"</span>
                        9 <span style="color: #2A00FF;">"ankle boot"</span><span style="color: #709870;">}</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> num-label-map labels<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">print</span> <span style="color: #7388d6;">(</span>get-fashion-mnist-labels <span style="color: #909183;">[</span>1 2 2 4<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
(trouser pullover pullover coat)
</pre>


<p>
We can now create a function to visualize these examples.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">TODO</span>
<span style="color: #3F7F5F;">;; </span><span style="color: #3F7F5F;">how to convert ndarray to an image?</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgbf85acb" class="outline-4">
<h4 id="orgbf85acb"><span class="section-number-4">3.6.2.</span> Reading a Minibatch</h4>
<div class="outline-text-4" id="text-3-6-2">
<p>
To make our life easier when reading from the training and test sets,
we use the built-in data iterator rather than creating one from
scratch. Recall that at each iteration, a data iterator reads a
minibatch of data with size <code>batch-size</code> each time. We also randomly
shuffle the examples for the training data iterator.
</p>

<p>
Let us look at the time it takes to read the training data.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #7388d6;">[</span>elapsed <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">stopwatch</span>/start<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">for</span> <span style="color: #909183;">[</span>batch train-ds<span style="color: #909183;">]</span>
    <span style="color: #909183;">[</span><span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data batch<span style="color: #907373;">)</span><span style="color: #709870;">)</span>
     <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-labels batch<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Elapsed: "</span> <span style="color: #909183;">(</span>/ <span style="color: #709870;">(</span>elapsed<span style="color: #709870;">)</span> 1e9<span style="color: #909183;">)</span> <span style="color: #2A00FF;">"sec"</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Elapsed:  1.667E-4 sec
</pre>


<p>
We are now ready to work with the Fashion-MNIST dataset in the sections that follow.
</p>
</div>
</div>

<div id="outline-container-orgeff911e" class="outline-4">
<h4 id="orgeff911e"><span class="section-number-4">3.6.3.</span> Summary</h4>
<div class="outline-text-4" id="text-3-6-3">
<ul class="org-ul">
<li>Fashion-MNIST is an apparel classification dataset consisting of
images representing 10 categories. We will use this dataset in
subsequent sections and chapters to evaluate various classification
algorithms.</li>
<li>We store the shape of any image with height \(h\) width \(w\) pixels
as \(h \times w\) or (\(h\), \(w\)).</li>
<li>Data iterators are a key component for efficient performance. Rely
on well-implemented data iterators that exploit high-performance
computing to avoid slowing down your training loop.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org124da78" class="outline-3">
<h3 id="org124da78"><span class="section-number-3">3.7.</span> Implementation of Softmax Regression from Scratch</h3>
<div class="outline-text-3" id="text-3-7">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">ns</span> <span style="color: #000000; font-style: italic; text-decoration: underline;">clj-d2l.softmax-from-scratch</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:require</span> <span style="color: #909183;">[</span>clojure.java.io <span style="color: #110099;">:as</span> io<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.ndarray <span style="color: #110099;">:as</span> nd<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.dataset <span style="color: #110099;">:as</span> ds<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.model <span style="color: #110099;">:as</span> model<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.nn <span style="color: #110099;">:as</span> nn<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.loss <span style="color: #110099;">:as</span> loss<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.tracker <span style="color: #110099;">:as</span> tracker<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.optimizer <span style="color: #110099;">:as</span> optimizer<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training <span style="color: #110099;">:as</span> t<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.training.listener <span style="color: #110099;">:as</span> listener<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-djl.engine <span style="color: #110099;">:as</span> engine<span style="color: #909183;">]</span>
            <span style="color: #909183;">[</span>clj-d2l.core <span style="color: #110099;">:as</span> d2l<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #110099;">:import</span> <span style="color: #909183;">[</span>ai.djl.basicdataset.cv.classification FashionMnist<span style="color: #909183;">]</span>
           <span style="color: #909183;">[</span>java.nio.file Paths<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-emacs-lisp"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">setq</span> org-babel-clojure-sync-nrepl-timeout 1000<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
1000
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">batch-size</span> 256<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">fashion-mnist</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">d2l</span>/load-data-fashion-mnist batch-size<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">#&rsquo;clj-d2l.softmax-from-scratch/batch-size</td>
</tr>

<tr>
<td class="org-left">#&rsquo;clj-d2l.softmax-from-scratch/fashion-mnist</td>
</tr>
</tbody>
</table>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>.size <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> fashion-mnist<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
60000
</pre>
</div>

<div id="outline-container-orga5a573c" class="outline-4">
<h4 id="orga5a573c"><span class="section-number-4">3.7.1.</span> Initializing Model Parameters</h4>
<div class="outline-text-4" id="text-3-7-1">
<p>
As in our linear regression example, each example here will be
represented by a fixed-length vector. Each example in the raw dataset
is a \(28 \times 28\) image. In this section, we will flatten each
image, treating them as vectors of length 784. In the future, we will
talk about more sophisticated strategies for exploiting the spatial
structure in images, but for now we treat each pixel location as just
another feature.
</p>

<p>
Recall that in softmax regression, we have as many outputs as there are
classes. Because our dataset has 10 classes, our network will have an
output dimension of 10. Consequently, our weights will constitute a
\(784 \times 10\) matrix and the biases will constitute a
\(1 \times 10\) row vector. As with linear regression, we will
initialize our weights <code>W</code> with Gaussian noise and our biases to take
the initial value 0.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">num-inputs</span> 784<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">num-outputs</span> 10<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">ndm</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/base-manager<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">W</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm 0 0.01 <span style="color: #909183;">[</span>num-inputs num-outputs<span style="color: #909183;">]</span> <span style="color: #110099;">:float32</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">b</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/zeros ndm <span style="color: #909183;">[</span>num-outputs<span style="color: #909183;">]</span> <span style="color: #110099;">:float32</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org05c0eef" class="outline-4">
<h4 id="org05c0eef"><span class="section-number-4">3.7.2.</span> Defining the Softmax Operation</h4>
<div class="outline-text-4" id="text-3-7-2">
<p>
Before implementing the softmax regression model, let us briefly
review how the sum operator works along specific dimensions in a
tensor, as discussed in Section
<a href="2.3-linear-algebra.html#lin-alg-reduction">2.3-linear-algebra.html#lin-alg-reduction</a> and Section
<a href="2.3-linear-algebra.html#lin-alg-non-reduction">2.3-linear-algebra.html#lin-alg-non-reduction</a>. Given a matrix <code>X</code>
we can sum over all elements (by default) or only over elements in the
same axis, i.e., the same column (axis 0) or the same row (axis
1). Note that if <code>X</code> is a tensor with shape (2, 3) and we sum over the
columns, the result will be a vector with shape (3,). When invoking
the sum operator, we can specify to keep the number of axes in the
original tensor, rather than collapsing out the dimension that we
summed over. This will result in a two-dimensional tensor with shape
(1, 3).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">X</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span><span style="color: #709870;">[</span>1 2 3<span style="color: #709870;">]</span> <span style="color: #709870;">[</span>4 5 6<span style="color: #709870;">]</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum X <span style="color: #7388d6;">[</span>0<span style="color: #7388d6;">]</span> <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (1, 3) cpu() int64
[[ 5,  7,  9],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum X <span style="color: #7388d6;">[</span>1<span style="color: #7388d6;">]</span> <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (2, 1) cpu() int64
[[ 6],
 [15],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum X <span style="color: #7388d6;">[</span>0 1<span style="color: #7388d6;">]</span> <span style="color: #110099;">true</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (1, 1) cpu() int64
[[21],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum X <span style="color: #7388d6;">[</span>0 1<span style="color: #7388d6;">]</span> <span style="color: #110099;">false</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() int64
21
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum X<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: () cpu() int64
21
</pre>


<p>
We are now ready to implement the softmax operation. Recall that
softmax consists of three steps: (i) we exponentiate each term (using
<code>exp</code>); (ii) we sum over each row (we have one row per example in the
batch) to get the normalization constant for each example; (iii) we
divide each row by its normalization constant, ensuring that the
result sums to 1.  Before looking at the code, let us recall how this
looks expressed as an equation:
</p>

\begin{equation}
\label{org0e76c78}
\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.
\end{equation}

<p>
The denominator, or normalization constant, is also sometimes called
the <b>partition function</b> (and its logarithm is called the log-partition
function). The origins of that name are in <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">statistical physics</a> where a
related equation models the distribution over an ensemble of
particles.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">softmax</span> <span style="color: #7388d6;">[</span>ndarray<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>Xexp <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/exp ndarray<span style="color: #709870;">)</span>
        partition <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum Xexp <span style="color: #907373;">[</span>1<span style="color: #907373;">]</span> <span style="color: #110099;">true</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// Xexp partition<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.softmax-from-scratch/softmax
</pre>


<p>
As you can see, for any random input, we turn each element into a
non-negative number. Moreover, each row sums up to 1, as is required
for a probability.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">X</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/random-normal ndm <span style="color: #909183;">[</span>2 5<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span>softmax X<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (2, 5) cpu() float32
[[0.2492, 0.2315, 0.2786, 0.0503, 0.1905],
 [0.1874, 0.4392, 0.2314, 0.0868, 0.0552],
]
</pre>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum <span style="color: #7388d6;">(</span>softmax X<span style="color: #7388d6;">)</span> <span style="color: #7388d6;">[</span>1<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (2) cpu() float32
[1., 1.]
</pre>


<p>
Note that while this looks correct mathematically, we were a bit
sloppy in our implementation because we failed to take precautions
against numerical overflow or underflow due to large or very small
elements of the matrix.
</p>
</div>
</div>

<div id="outline-container-org295ba2b" class="outline-4">
<h4 id="org295ba2b"><span class="section-number-4">3.7.3.</span> Defining the Model</h4>
<div class="outline-text-4" id="text-3-7-3">
<p>
Now that we have defined the softmax operation, we can implement the
softmax regression model. The below code defines how the input is
mapped to the output through the network. Note that we flatten each
original image in the batch into a vector using the <code>reshape</code> function
before passing the data through our model.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">net</span> <span style="color: #7388d6;">[</span>ndarray<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>current-W W
        current-b b<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> ndarray
        <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape <span style="color: #907373;">[</span>-1 num-inputs<span style="color: #907373;">]</span><span style="color: #709870;">)</span>
        <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/dot current-W<span style="color: #709870;">)</span>
        <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/+ current-b<span style="color: #709870;">)</span>
        softmax<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org425aa8b" class="outline-4">
<h4 id="org425aa8b"><span class="section-number-4">3.7.4.</span> Defining the Loss Function</h4>
<div class="outline-text-4" id="text-3-7-4">
<p>
Next, we need to implement the cross-entropy loss function, as
introduced in Section
<a href="3.4-softmax-regression.html#sec-softmax">3.4-softmax-regression.html#sec-softmax</a>. This may be the
most common loss function in all of deep learning because, at the
moment, classification problems far outnumber regression problems.
</p>

<p>
Recall that cross-entropy takes the negative log-likelihood of the
predicted probability assigned to the true label. Rather than
iterating over the predictions with a for-loop (which tends to be
inefficient), we can pick all elements by a single operator. Below, we
create sample data <code>y-hat</code> with 2 examples of predicted probabilities
over 3 classes and their corresponding labels <code>y</code>. With <code>y</code> we know that
in the first example the first class is the correct prediction and in
the second example the third class is the ground-truth. Using <code>y</code> as the
indices of the probabilities in <code>y-hat</code>, we pick the probability of the
first class in the first example and the probability of the third
class in the second example.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">y</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span>0 2<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">y-hat</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/create ndm <span style="color: #909183;">[</span><span style="color: #709870;">[</span>0.1 0.3 0.6<span style="color: #709870;">][</span>0.3 0.2 0.5<span style="color: #709870;">]</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> y-hat <span style="color: #2A00FF;">":,{}"</span> y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (2, 1) cpu() float64
[[0.1],
 [0.5],
]
</pre>


<p>
Now we can implement the cross-entropy loss function efficiently with
just one line of code.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">cross-entropy</span> <span style="color: #7388d6;">[</span>y-hat y<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/<span style="color: #7F0055; font-weight: bold;">get</span> y-hat <span style="color: #2A00FF;">":, {}"</span> <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-type y <span style="color: #110099;">:int32</span> <span style="color: #110099;">false</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
      <span style="color: #909183;">(</span>.log<span style="color: #909183;">)</span>
      <span style="color: #909183;">(</span>.neg<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span>cross-entropy y-hat y<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
ND: (2, 1) cpu() float64
[[2.3026],
 [0.6931],
]
</pre>
</div>
</div>


<div id="outline-container-orgfd4ce81" class="outline-4">
<h4 id="orgfd4ce81"><span class="section-number-4">3.7.5.</span> Classification Accuracy</h4>
<div class="outline-text-4" id="text-3-7-5">
<p>
Given the predicted probability distribution <code>y-hat</code>, we typically
choose the class with the highest predicted probability whenever we
must output a hard prediction. Indeed, many applications require that
we make a choice. Gmail must categorize an email into &ldquo;Primary&rdquo;,
&ldquo;Social&rdquo;, &ldquo;Updates&rdquo;, or &ldquo;Forums&rdquo;. It might estimate probabilities
internally, but at the end of the day it has to choose one among the
classes.
</p>

<p>
When predictions are consistent with the label class <code>y</code>, they are
correct. The classification accuracy is the fraction of all
predictions that are correct. Although it can be difficult to optimize
accuracy directly (it is not differentiable), it is often the
performance measure that we care most about, and we will nearly always
report it when training classifiers.
</p>

<p>
To compute accuracy we do the following. First, if <code>y-hat</code> is a matrix,
we assume that the second dimension stores prediction scores for each
class. We use <code>argmax</code> to obtain the predicted class by the index for
the largest entry in each row. Then we compare the predicted class
with the ground-truth <code>y</code> elementwise. Since the equality operator <code>==</code> is
sensitive to data types, we convert <code>y-hat</code>&rsquo;s data type to match that of
<code>y</code>. The result is a tensor containing entries of 0 (false) and 1
(true). Taking the sum yields the number of correct predictions.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">accuracy</span> <span style="color: #7388d6;">[</span>y-hat y<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">if</span> <span style="color: #909183;">(</span>&gt; <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/shape y-hat<span style="color: #907373;">)</span><span style="color: #709870;">)</span> 1<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/argmax y-hat 1<span style="color: #709870;">)</span>
        <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/= <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-type y <span style="color: #110099;">:int64</span> <span style="color: #110099;">false</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span>
        <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum<span style="color: #709870;">)</span>
        <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
We will continue to use the variables <code>y-hat</code> and <code>y</code> defined before as
the predicted probability distributions and labels, respectively. We
can see that the first example&rsquo;s prediction class is 2 (the largest
element of the row is 0.6 with the index 2), which is inconsistent
with the actual label, 0. The second example&rsquo;s prediction class is 2
(the largest element of the row is 0.5 with the index of 2), which is
consistent with the actual label, 2. Therefore, the classification
accuracy rate for these two examples is 0.5.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>/ <span style="color: #7388d6;">(</span>accuracy y-hat y<span style="color: #7388d6;">)</span> <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size y<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
1/2
</pre>


<p>
Similarly, we can evaluate the accuracy for any model net on a dataset
that is accessed via the data iterator <code>data-iter</code>.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">fashion-mnist-train</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> fashion-mnist<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">evaluate-accuracy</span> <span style="color: #7388d6;">[</span>net data-iter<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>acc <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">atom</span> <span style="color: #907373;">[</span>0 0<span style="color: #907373;">]</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #709870;">[</span>batch data-iter<span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #907373;">[</span>X <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/head <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data batch<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
            y <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/head <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-labels batch<span style="color: #858580;">)</span><span style="color: #6276ba;">)</span><span style="color: #907373;">]</span>
        <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">swap!</span> acc update 0 + <span style="color: #6276ba;">(</span>accuracy <span style="color: #858580;">(</span>net X<span style="color: #858580;">)</span> y<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
        <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">swap!</span> acc update 1 + <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size y<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
        <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/close batch<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">reduce</span> / @acc<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
#'clj-d2l.softmax-from-scratch/evaluate-accuracy
</pre>


<p>
Here <code>accumulate</code> is a utility function to accumulate sums over multiple
variables. In the above <code>evaluate-accuracy</code> function, we create a <code>atom</code>
of vector with 2 variables for storing both the number of correct
predictions and the number of predictions, respectively. Both will be
accumulated over time as we iterate over the dataset.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">accumulate</span> <span style="color: #7388d6;">[</span>atom x y z<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">swap!</span> atom update 0 + x<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">swap!</span> atom update 1 + y<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">swap!</span> atom update 2 + z<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Because we initialized the net model with random weights, the accuracy
of this model should be close to random guessing, i.e., 0.1 for 10
classes.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>evaluate-accuracy net <span style="color: #7388d6;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data-iterator <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> fashion-mnist<span style="color: #909183;">)</span> ndm<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
119/2000
</pre>
</div>
</div>

<div id="outline-container-org15594fe" class="outline-4">
<h4 id="org15594fe"><span class="section-number-4">3.7.6.</span> Training</h4>
<div class="outline-text-4" id="text-3-7-6">
<p>
The training loop for softmax regression should look strikingly
familiar if you read through our implementation of linear regression
in Section 3.2. Here we refactor the implementation to make it
reusable. First, we define a function to train for one epoch. Note
that updater is a general function to update the model parameters,
which accepts the batch size as an argument. It can be either a
wrapper of the <code>d2l/sgd</code> function or a framework&rsquo;s built-in optimization
function.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">train-epoch-ch3</span> <span style="color: #7388d6;">[</span>net train-iter lr loss updater<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>acc <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">atom</span> <span style="color: #907373;">[</span>0 0 0<span style="color: #907373;">]</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #709870;">[</span>param <span style="color: #907373;">[</span>W b<span style="color: #907373;">]</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/set-requires-gradient param <span style="color: #110099;">true</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #709870;">[</span>batch <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/iter-seq train-iter<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #907373;">[</span>X <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> batch <span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data <span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/head <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/reshape <span style="color: #80a880;">[</span>-1 num-inputs<span style="color: #80a880;">]</span><span style="color: #858580;">)</span><span style="color: #6276ba;">)</span>
            y <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> batch <span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-labels <span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/head<span style="color: #6276ba;">)</span><span style="color: #907373;">]</span>
        <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">with-open</span> <span style="color: #6276ba;">[</span>gc <span style="color: #858580;">(</span><span style="color: #7F0055; font-weight: bold;">-&gt;</span> <span style="color: #80a880;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">engine</span>/get-instance<span style="color: #80a880;">)</span> <span style="color: #80a880;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">engine</span>/new-gradient-collector<span style="color: #80a880;">)</span><span style="color: #858580;">)</span><span style="color: #6276ba;">]</span>
          <span style="color: #6276ba;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #858580;">[</span>y-hat <span style="color: #80a880;">(</span>net X<span style="color: #80a880;">)</span>
                l <span style="color: #80a880;">(</span>loss y-hat y<span style="color: #80a880;">)</span><span style="color: #858580;">]</span>
            <span style="color: #858580;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">t</span>/backward gc l<span style="color: #858580;">)</span>
            <span style="color: #858580;">(</span>accumulate acc <span style="color: #80a880;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-element <span style="color: #887070;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/sum l<span style="color: #887070;">)</span><span style="color: #80a880;">)</span> <span style="color: #80a880;">(</span>accuracy y-hat y<span style="color: #80a880;">)</span> <span style="color: #80a880;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/size y<span style="color: #80a880;">)</span><span style="color: #858580;">)</span><span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span>updater <span style="color: #907373;">[</span>W b<span style="color: #907373;">]</span> lr batch-size<span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/close batch<span style="color: #709870;">)</span><span style="color: #909183;">)</span>
    <span style="color: #909183;">[</span><span style="color: #709870;">(</span>/ <span style="color: #907373;">(</span>@acc 0<span style="color: #907373;">)</span> <span style="color: #907373;">(</span>@acc 2<span style="color: #907373;">)</span><span style="color: #709870;">)</span> <span style="color: #709870;">(</span>/ <span style="color: #907373;">(</span>@acc 1<span style="color: #907373;">)</span> <span style="color: #907373;">(</span>@acc 2<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
The training function then runs multiple epochs and visualize the
training progress.
</p>

<p>
Again, we use the minibatch stochastic gradient descent to optimize
the loss function of the model. Note that the number of epochs
(numEpochs), and learning rate (lr) are both adjustable
hyper-parameters. By changing their values, we may be able to increase
the classification accuracy of the model. In practice we will want to
split our data three ways into training, validation, and test data,
using the validation data to choose the best values of our
hyper-parameters.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">sgd</span> <span style="color: #7388d6;">[</span>params lr batch-size<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span>param params<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/-! param <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>// <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/* <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/get-gradient param<span style="color: #6276ba;">)</span> lr<span style="color: #907373;">)</span> batch-size<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">train-ch3</span> <span style="color: #7388d6;">[</span>net train-ds test-ds lr loss num-epochs updater<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">doseq</span> <span style="color: #909183;">[</span>i <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">range</span> num-epochs<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #709870;">[</span>train-metrics <span style="color: #907373;">(</span>train-epoch-ch3 net <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data-iterator train-ds ndm<span style="color: #6276ba;">)</span> lr loss updater<span style="color: #907373;">)</span>
          accuracy <span style="color: #907373;">(</span>evaluate-accuracy net <span style="color: #6276ba;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data-iterator test-ds ndm<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
          train-accuracy <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> train-metrics 1<span style="color: #907373;">)</span>
          train-loss <span style="color: #907373;">(</span><span style="color: #7F0055; font-weight: bold;">get</span> train-metrics 0<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Epoch "</span> i <span style="color: #2A00FF;">": Test Accuracy: "</span> accuracy<span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Train Accuracy: "</span> train-accuracy<span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Train Loss: "</span>train-loss<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>


<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">num-epochs</span> 3<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">lr</span> 0.1<span style="color: #707183;">)</span>
<span style="color: #707183;">(</span>train-ch3 net <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> fashion-mnist<span style="color: #7388d6;">)</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> fashion-mnist<span style="color: #7388d6;">)</span> lr cross-entropy num-epochs sgd<span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Epoch  0 : Test Accuracy:  2067/2500
Train Accuracy:  10117/12000
Train Loss:  0.4646147914886475
Epoch  1 : Test Accuracy:  1041/1250
Train Accuracy:  25369/30000
Train Loss:  0.4579694811503092
Epoch  2 : Test Accuracy:  4169/5000
Train Accuracy:  25367/30000
Train Loss:  0.45321130771636964
</pre>
</div>
</div>

<div id="outline-container-orgf659fca" class="outline-4">
<h4 id="orgf659fca"><span class="section-number-4">3.7.7.</span> Prediction</h4>
<div class="outline-text-4" id="text-3-7-7">
<p>
Now that training is complete, our model is ready to classify some
images. Given a series of images, we will compare their actual labels
(first line of text output) and the model predictions (second line of
text output).
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">defn</span> <span style="color: #0000ff; font-weight: bold;">predict-ch3</span> <span style="color: #7388d6;">[</span>net dataset ndmanager<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">let</span> <span style="color: #909183;">[</span>batch <span style="color: #709870;">(</span><span style="color: #7F0055; font-weight: bold;">first</span> <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data-iterator dataset ndmanager<span style="color: #907373;">)</span><span style="color: #709870;">)</span>
        X <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/head <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-data batch<span style="color: #907373;">)</span><span style="color: #709870;">)</span>
        y-hat <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/argmax <span style="color: #907373;">(</span>net X<span style="color: #907373;">)</span> 1<span style="color: #709870;">)</span>
        y <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/head <span style="color: #907373;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">ds</span>/get-labels batch<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">[</span>y-hat y<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">def</span> <span style="color: #000000;">prediction</span> <span style="color: #7388d6;">(</span>predict-ch3 net <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">second</span> fashion-mnist<span style="color: #909183;">)</span> ndm<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Prediction:   "</span> <span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">take</span> 20 <span style="color: #909183;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #709870;">(</span>prediction 0<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span><span style="color: #7F0055; font-weight: bold;">println</span> <span style="color: #2A00FF;">"Actual label: "</span><span style="color: #7388d6;">(</span><span style="color: #7F0055; font-weight: bold;">take</span> 20 <span style="color: #909183;">(</span><span style="color: #7F0055; font-weight: bold;">map</span> int <span style="color: #709870;">(</span><span style="color: #000000; font-style: italic; text-decoration: underline;">nd</span>/to-vec <span style="color: #907373;">(</span>prediction 1<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
Prediction:    (3 4 3 0 7 1 5 3 9 7 8 0 5 3 4 8 1 0 4 4)
Actual label:  (3 4 3 0 7 1 5 3 9 7 8 0 5 3 2 8 1 6 2 4)
</pre>
</div>
</div>

<div id="outline-container-org3cb93ff" class="outline-4">
<h4 id="org3cb93ff"><span class="section-number-4">3.7.8.</span> Summary</h4>
<div class="outline-text-4" id="text-3-7-8">
<p>
With softmax regression, we can train models for multi-category
classification. The training loop is very similar to that in linear
regression: retrieve and read data, define models and loss functions,
then train models using optimization algorithms. As you will soon find
out, most common deep learning models have similar training
procedures.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Kimi Ma</p>
<p class="date">Created: 2022-05-17 Tue 08:13</p>
</div>
</body>
</html>
