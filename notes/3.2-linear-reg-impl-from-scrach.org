#+PROPERTY: header-args    :tangle src/clj_d2l/linreg.clj
* Linear Regression Implementation from Scratch



Now that you understand the key ideas behind linear regression, we can
begin to work through a hands-on implementation in code. In this
section, we will implement the entire method from scratch, including
the data pipeline, the model, the loss function, and the minibatch
stochastic gradient descent optimizer. While modern deep learning
frameworks can automate nearly all of this work, implementing things
from scratch is the only way to make sure that you really know what
you are doing. Moreover, when it comes time to customize models,
defining our own layers or loss functions, understanding how things
work under the hood will prove handy. In this section, we will rely
only on tensors and auto differentiation. Afterwards, we will
introduce a more concise implementation, taking advantage of bells and
whistles of deep learning frameworks.

#+begin_src clojure :results silent
(ns clj-d2l.linreg
  (:require [clj-djl.ndarray :as nd]
            [clj-djl.device :as device]
            [clj-djl.engine :as engine]
            [clj-djl.training :as training]
            [clj-djl.training.dataset :as ds]
            [clj-d2l.core :as d2l]))
#+end_src

** Generating the Dataset

To keep things simple, we will construct an artificial dataset
according to a linear model with additive noise. Our task will be to
recover this model's parameters using the finite set of examples
contained in our dataset. We will keep the data low-dimensional so we
can visualize it easily. In the following code snippet, we generate a
dataset containing 1000 examples, each consisting of 2 features
sampled from a standard normal distribution. Thus our synthetic
dataset will be a matrix \(\mathbf{X}\in \mathbb{R}^{1000 \times 2}\).

The true parameters generating our dataset will be \(\mathbf{w} = [2,
-3.4]^\top\) and \(b = 4.2\), and our synthetic labels will be
assigned according to the following linear model with the noise term
\(\epsilon\):

\begin{equation}
\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.
\end{equation}


You could think of \(\epsilon\) as capturing potential measurement
errors on the features and labels. We will assume that the standard
assumptions hold and thus that \(\epsilon\) obeys a normal
distribution with mean of 0. To make our problem easy, we will set its
standard deviation to 0.01. The following code generates our synthetic
dataset.

#+begin_src clojure :results silent :exports both :eval no-export
(defn synthetic-data [ndm w b num]
  (let [X (nd/random-normal ndm [num (nd/size w)])
        y (nd/+ (nd/dot X w) b)
        noise (nd/random-normal ndm 0 0.01 (nd/shape y) :float32)]
    [X (nd/+ y noise)]))

(def ndm (nd/new-base-manager))
(def true-w (nd/create ndm (float-array [2 -3.4])))
(def true-b 4.2)
(def dp (synthetic-data ndm true-w true-b 1000))
(def features (get dp 0))
(def labels (get dp 1))
#+end_src

Note that each row in features consists of a 2-dimensional data
example and that each row in labels consists of a 1-dimensional label
value (a scalar).

#+begin_src clojure :results pp :exports both :eval no-export
(str "features(0): " (nd/to-vec (nd/get features [0]))
     "\nlabels(0): " (nd/get-element labels [0]))
#+end_src

#+RESULTS:
: features(0): [1.5995362 0.08100491]
: labels(0): 7.122985

By generating a scatter plot using the second feature and ~labels~, we
can clearly observe the linear correlation between the two.

#+begin_src clojure :results file graphics :output-dir figures :file synthetic_data.svg :exports both :eval no-export
(let [x (nd/to-vec (nd/get features ":, 1"))
      y (nd/to-vec labels)]
  (d2l/plot-scatter
   "notes/figures/synthetic_data.svg"
   "data"
   x
   y))
#+end_src

#+RESULTS:
[[file:figures/synthetic_data.svg]]


** Reading the Dataset

#+begin_src clojure :results silent :exports both
(def batch-size 10)
(def dataset (-> (ds/new-array-dataset-builder)
                 (ds/set-data features)
                 (ds/opt-labels labels)
                 (ds/set-sampling batch-size false)
                 (ds/build)))
#+end_src

#+begin_src clojure :results output :exports both
(let [batch (.next (ds/get-data dataset ndm))
      X (-> (ds/get-batch-data batch)
            (nd/head))
      y (-> (ds/get-batch-labels batch)
            (nd/head))]
  (println (str X))
  (println (nd/to-vec (nd/+ (nd/dot X true-w) true-b)))
  (println (nd/to-vec y))
  (ds/close-batch batch))
#+end_src

#+RESULTS:
#+begin_example
ND: (10, 2) cpu() float32
[[ 0.2727,  0.0463],
 [ 0.8032,  1.3645],
 [ 0.0895,  0.6368],
 [-0.435 , -1.8171],
 [ 1.6563, -1.197 ],
 [ 0.5207, -1.0564],
 [ 0.1503,  1.0269],
 [ 0.5365,  0.6632],
 [-0.6452,  1.2165],
 [ 2.0424,  0.8562],
]

[4.5877643 1.1672001 2.213755 9.508151 11.582605 8.833304 1.0091412 3.0179806 -1.2264853 5.3737807]
[4.5826263 1.178563 2.2149844 9.506688 11.571146 8.845152 1.0214792 3.0197468 -1.2324206 5.3641534]
#+end_example

** Initializing Model Parameters

#+begin_src clojure :results output :exports both
(def w (nd/random-normal ndm 0 0.01 [2 1] DataType/FLOAT32 (device/default-device)))
(def b (nd/zeros ndm [1]))
(println (nd/to-vec w))
(println (nd/to-vec b))
#+end_src

#+RESULTS:
: [8.1814115E-4 -0.016517017]
: [0.0]

** Defining the Model

#+begin_src clojure :results silent :export both
(defn linreg [X w b]
  (nd/+ (nd/dot X w) b))
#+end_src

** Defining the Loss Function

#+begin_src clojure :results silent :export both
(defn squared-loss [y-hat y]
  (nd// (nd/* (nd/- y-hat (nd/reshape y (nd/get-shape y-hat)))
              (nd/- y-hat (nd/reshape y (nd/get-shape y-hat))))
        2))
#+end_src

** Defining the Optimization Algorithm

stochastic gradient descent (SGD):

#+begin_src clojure :results silent :export both
(defn sgd [params lr batch-size]
  (doseq [param params]
    ;; param = param - param.gradient * lr / batchSize
    (nd/-! param (nd// (nd/* (nd/get-gradient param) lr) batch-size))))
#+end_src

** Training

#+begin_src clojure :results output :exports both
(def lr 0.03)
(def epochs 3)

(dorun (map #(nd/attach-gradient %) [w b]))

(doseq [epoch (range epochs)]
  (doseq [batch (training/iter-seq (ds/get-data dataset ndm))]
    (let [X (-> (ds/get-batch-data batch)
                (nd/head))
          y (-> (ds/get-batch-labels batch)
                (nd/head))]
      (with-open [gc (-> (engine/get-instance) (engine/new-gradient-collector))]
        (let [l (-> (linreg X w b) (squared-loss y))]
          (.backward gc l)))
      (sgd [w b] lr batch-size)
      (ds/close-batch batch)))
  (let [train-loss (squared-loss (linreg features w b) labels)]
    (println "epoch" (inc epoch) ", loss " (nd/get-element (.mean train-loss)))))
#+end_src

#+RESULTS:
: epoch 1 , loss  0.037740294
: epoch 2 , loss  1.4411E-4
: epoch 3 , loss  4.955114E-5

#+begin_src clojure :results output :exports both
(println (nd/to-vec w))
(println (nd/to-vec true-w))
(def w-error (nd/to-vec (nd/- true-w (nd/reshape w (nd/get-shape true-w)))))
(println "Error in estimating w:" (vec w-error))
(println "Error in estimating w:" (- true-b (nd/get-element b)))
#+end_src

#+RESULTS:
: [2.0000813 -3.3988967]
: [2.0 -3.4]
: Error in estimating w: [-8.1300735E-5 -0.0011034012]
: Error in estimating w: 6.200790405275214E-4
